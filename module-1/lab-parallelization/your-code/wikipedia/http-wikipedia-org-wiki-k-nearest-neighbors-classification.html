b'<!DOCTYPE html>\n\n<html class="client-nojs" dir="ltr" lang="en">\n<head>\n<meta charset="utf8"/>\n<title>k-nearest neighbors algorithm - Wikipedia</title>\n<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"f2943a2b-4bf5-4b17-a4fe-d6b9c01630bc","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"K-nearest_neighbors_algorithm","wgTitle":"K-nearest neighbors algorithm","wgCurRevisionId":982934962,"wgRevisionId":982934962,"wgArticleId":1775388,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from March 2013","Articles with unsourced statements from December 2008","Wikipedia articles needing clarification from January 2019","Articles with unsourced statements from September 2019"\n,"Wikipedia articles needing clarification from July 2020","Classification algorithms","Search algorithms","Machine learning algorithms","Statistical classification","Nonparametric statistics"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"K-nearest_neighbors_algorithm","wgRelevantArticleId":1775388,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRedirectedFrom":"K-nearest_neighbors_classification","wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgInternalRedirectTargetUrl":\n"/wiki/K-nearest_neighbors_algorithm","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q1071612"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.page.gallery.styles":"ready","skins.vector.styles.legacy":"ready","mediawiki.toc.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["mediawiki.action.view.redirect","ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin"\n,"mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\\\","watchToken":"+\\\\","csrfToken":"+\\\\"});\n});});</script>\n<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.page.gallery.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>\n<meta content="" name="ResourceLoaderDynamicStyles"/>\n<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<meta content="MediaWiki 1.36.0-wmf.14" name="generator"/>\n<meta content="origin" name="referrer"/>\n<meta content="origin-when-crossorigin" name="referrer"/>\n<meta content="origin-when-cross-origin" name="referrer"/>\n<link href="//en.m.wikipedia.org/wiki/K-nearest_neighbors_algorithm" media="only screen and (max-width: 720px)" rel="alternate"/>\n<link href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>\n<link href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit" rel="edit" title="Edit this page"/>\n<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>\n<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>\n<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>\n<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>\n<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>\n<link href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="canonical"/>\n<link href="//login.wikimedia.org" rel="dns-prefetch"/>\n<link href="//meta.wikimedia.org" rel="dns-prefetch"/>\n</head>\n<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-K-nearest_neighbors_algorithm rootpage-K-nearest_neighbors_algorithm skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>\n<div class="noprint" id="mw-head-base"></div>\n<div class="mw-body" id="content" role="main">\n<a id="top"></a>\n<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>\n<div class="mw-indicators mw-body-content">\n</div>\n<h1 class="firstHeading" id="firstHeading" lang="en"><i>k</i>-nearest neighbors algorithm</h1>\n<div class="mw-body-content" id="bodyContent">\n<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>\n<div id="contentSub"><span class="mw-redirectedfrom">\xc2\xa0\xc2\xa0(Redirected from <a class="mw-redirect" href="/w/index.php?title=K-nearest_neighbors_classification&amp;redirect=no" title="K-nearest neighbors classification">K-nearest neighbors classification</a>)</span></div>\n<div id="contentSub2"></div>\n<div id="jump-to-nav"></div>\n<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>\n<a class="mw-jump-link" href="#searchInput">Jump to search</a>\n<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="hatnote navigation-not-searchable" role="note">Not to be confused with <a href="/wiki/K-means_clustering" title="K-means clustering">k-means clustering</a>.</div>\n<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em">Part of a series on</td></tr><tr><th style="padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br/>and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>\n<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>\n<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>\n<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>\n<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>\n<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>\n<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>\n<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>\n<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>\n<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>\n<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>\n<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>\xc2\xa0\xe2\x80\xa2 <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>\n<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>\n<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>\n<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>\n<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>\n<li><a class="mw-selflink selflink"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>\n<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>\n<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>\n<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>\n<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>\n<li><a class="mw-redirect" href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>\n<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>\n<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>\n<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>\n<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation\xe2\x80\x93maximization algorithm">Expectation\xe2\x80\x93maximization (EM)</a></li>\n<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>\n<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>\n<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>\n<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>\n<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>\n<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>\n<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>\n<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>\n<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>\n<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>\n<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>\n<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>\n<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>\n<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>\n<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>\n<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>\n<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>\n<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>\n<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>\n<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li></ul></li>\n<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>\n<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>\n<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>\n<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>\n<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>\n<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias\xe2\x80\x93variance dilemma">Bias\xe2\x80\x93variance dilemma</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>\n<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>\n<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>\n<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>\n<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>\n<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik\xe2\x80\x93Chervonenkis theory">VC theory</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>\n<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>\n<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>\n<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>\n<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>\n<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p>In <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, the <b><i>k</i>-nearest neighbors algorithm</b> (<b><i>k</i>-NN</b>) is a <a class="mw-redirect" href="/wiki/Non-parametric_statistics" title="Non-parametric statistics">non-parametric</a> method proposed by <a href="/wiki/Thomas_M._Cover" title="Thomas M. Cover">Thomas Cover</a> used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>.<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> In both cases, the input consists of the <i>k</i> closest training examples in the <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature space</a>. The output depends on whether <i>k</i>-NN is used for classification or regression:\n</p>\n<dl><dd><ul><li>In <i>k-NN classification</i>, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its <i>k</i> nearest neighbors (<i>k</i> is a positive <a href="/wiki/Integer" title="Integer">integer</a>, typically small). If <i>k</i>\xc2\xa0=\xc2\xa01, then the object is simply assigned to the class of that single nearest neighbor.</li></ul></dd></dl>\n<dl><dd><ul><li>In <i>k-NN regression</i>, the output is the property value for the object. This value is the average of the values of <i>k</i> nearest neighbors.</li></ul></dd></dl>\n<p><i>k</i>-NN is a type of <a href="/wiki/Instance-based_learning" title="Instance-based learning">instance-based learning</a>, or <a href="/wiki/Lazy_learning" title="Lazy learning">lazy learning</a>, where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, <a href="/wiki/Normalization_(statistics)" title="Normalization (statistics)">normalizing</a> the training data can improve its accuracy dramatically.<sup class="reference" id="cite_ref-:0_2-0"><a href="#cite_note-:0-2">[2]</a></sup><sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup><sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup>\n</p><p>Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/<i>d</i>, where <i>d</i> is the distance to the neighbor.<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>\n</p><p>The neighbors are taken from a set of objects for which the class (for <i>k</i>-NN classification) or the object property value (for <i>k</i>-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n</p><p>A peculiarity of the <i>k</i>-NN algorithm is that it is sensitive to the local structure of the data.\n</p>\n<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>\n<ul>\n<li class="toclevel-1 tocsection-1"><a href="#Statistical_setting"><span class="tocnumber">1</span> <span class="toctext">Statistical setting</span></a></li>\n<li class="toclevel-1 tocsection-2"><a href="#Algorithm"><span class="tocnumber">2</span> <span class="toctext">Algorithm</span></a></li>\n<li class="toclevel-1 tocsection-3"><a href="#Parameter_selection"><span class="tocnumber">3</span> <span class="toctext">Parameter selection</span></a></li>\n<li class="toclevel-1 tocsection-4"><a href="#The_1-nearest_neighbor_classifier"><span class="tocnumber">4</span> <span class="toctext">The <span>1</span>-nearest neighbor classifier</span></a></li>\n<li class="toclevel-1 tocsection-5"><a href="#The_weighted_nearest_neighbour_classifier"><span class="tocnumber">5</span> <span class="toctext">The weighted nearest neighbour classifier</span></a></li>\n<li class="toclevel-1 tocsection-6"><a href="#Properties"><span class="tocnumber">6</span> <span class="toctext">Properties</span></a></li>\n<li class="toclevel-1 tocsection-7"><a href="#Error_rates"><span class="tocnumber">7</span> <span class="toctext">Error rates</span></a></li>\n<li class="toclevel-1 tocsection-8"><a href="#Metric_learning"><span class="tocnumber">8</span> <span class="toctext">Metric learning</span></a></li>\n<li class="toclevel-1 tocsection-9"><a href="#Feature_extraction"><span class="tocnumber">9</span> <span class="toctext">Feature extraction</span></a></li>\n<li class="toclevel-1 tocsection-10"><a href="#Dimension_reduction"><span class="tocnumber">10</span> <span class="toctext">Dimension reduction</span></a></li>\n<li class="toclevel-1 tocsection-11"><a href="#Decision_boundary"><span class="tocnumber">11</span> <span class="toctext">Decision boundary</span></a></li>\n<li class="toclevel-1 tocsection-12"><a href="#Data_reduction"><span class="tocnumber">12</span> <span class="toctext">Data reduction</span></a>\n<ul>\n<li class="toclevel-2 tocsection-13"><a href="#Selection_of_class-outliers"><span class="tocnumber">12.1</span> <span class="toctext">Selection of class-outliers</span></a></li>\n<li class="toclevel-2 tocsection-14"><a href="#CNN_for_data_reduction"><span class="tocnumber">12.2</span> <span class="toctext">CNN for data reduction</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-15"><a href="#k-NN_regression"><span class="tocnumber">13</span> <span class="toctext"><i>k</i>-NN regression</span></a></li>\n<li class="toclevel-1 tocsection-16"><a href="#k-NN_outlier"><span class="tocnumber">14</span> <span class="toctext"><i>k</i>-NN outlier</span></a></li>\n<li class="toclevel-1 tocsection-17"><a href="#Validation_of_results"><span class="tocnumber">15</span> <span class="toctext">Validation of results</span></a></li>\n<li class="toclevel-1 tocsection-18"><a href="#See_also"><span class="tocnumber">16</span> <span class="toctext">See also</span></a></li>\n<li class="toclevel-1 tocsection-19"><a href="#References"><span class="tocnumber">17</span> <span class="toctext">References</span></a></li>\n<li class="toclevel-1 tocsection-20"><a href="#Further_reading"><span class="tocnumber">18</span> <span class="toctext">Further reading</span></a></li>\n</ul>\n</div>\n<h2><span class="mw-headline" id="Statistical_setting">Statistical setting</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=1" title="Edit section: Statistical setting">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>Suppose we have pairs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\\dots ,(X_{n},Y_{n})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>Y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>Y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>Y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\\dots ,(X_{n},Y_{n})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\\dots ,(X_{n},Y_{n})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f16142bcc75fbc4a7fece97df2d93185e831185" style="vertical-align: -0.838ex; width:31.22ex; height:2.843ex;"/></span> taking values in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbb {R} ^{d}\\times \\{1,2\\}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n<mo>\xc3\x97<!-- \xc3\x97 --></mo>\n<mo fence="false" stretchy="false">{</mo>\n<mn>1</mn>\n<mo>,</mo>\n<mn>2</mn>\n<mo fence="false" stretchy="false">}</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbb {R} ^{d}\\times \\{1,2\\}}</annotation>\n</semantics>\n</math></span><img alt="{\\mathbb  {R}}^{d}\\times \\{1,2\\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60671efc51865fcfac4e8939ab2acb643539302d" style="vertical-align: -0.838ex; width:11.294ex; height:3.176ex;"/></span>, where <span class="texhtml mvar" style="font-style:italic;">Y</span> is the class label of <span class="texhtml mvar" style="font-style:italic;">X</span>, so that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle X|Y=r\\sim P_{r}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mi>Y</mi>\n<mo>=</mo>\n<mi>r</mi>\n<mo>\xe2\x88\xbc<!-- \xe2\x88\xbc --></mo>\n<msub>\n<mi>P</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>r</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle X|Y=r\\sim P_{r}}</annotation>\n</semantics>\n</math></span><img alt="X|Y=r\\sim P_{r}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e1b6a01b179f721be03ed284c21e2d3cbb70d4a" style="vertical-align: -0.838ex; width:14.112ex; height:2.843ex;"/></span> for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle r=1,2}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>r</mi>\n<mo>=</mo>\n<mn>1</mn>\n<mo>,</mo>\n<mn>2</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle r=1,2}</annotation>\n</semantics>\n</math></span><img alt="r=1,2" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6dcdedf1b83451b9a51dc73b95f854eeaf83dc46" style="vertical-align: -0.671ex; width:7.506ex; height:2.509ex;"/></span> (and probability distributions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle P_{r}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>P</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>r</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle P_{r}}</annotation>\n</semantics>\n</math></span><img alt="P_{r}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3e7f814e80c6ff1469112bd4b7430e358e86c7d6" style="vertical-align: -0.671ex; width:2.466ex; height:2.509ex;"/></span>). Given some norm <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\|\\cdot \\|}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo fence="false" stretchy="false">\xe2\x80\x96<!-- \xe2\x80\x96 --></mo>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mo fence="false" stretchy="false">\xe2\x80\x96<!-- \xe2\x80\x96 --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\|\\cdot \\|}</annotation>\n</semantics>\n</math></span><img alt="\\|\\cdot \\|" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/113f0d8fe6108fc1c5e9802f7c3f634f5480b3d1" style="vertical-align: -0.838ex; width:4.004ex; height:2.843ex;"/></span> on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="\\mathbb {R} ^{d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a713426956296f1668fce772df3c60b9dde8a685" style="vertical-align: -0.338ex; width:2.77ex; height:2.676ex;"/></span> and a point <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x\\in \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>x</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x\\in \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="x\\in {\\mathbb  {R}}^{d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f351538c1465ec3881164b501f612b1f54cbfe7e" style="vertical-align: -0.338ex; width:6.94ex; height:2.676ex;"/></span>, let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (X_{(1)},Y_{(1)}),\\dots ,(X_{(n)},Y_{(n)})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">(</mo>\n<mn>1</mn>\n<mo stretchy="false">)</mo>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>Y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">(</mo>\n<mn>1</mn>\n<mo stretchy="false">)</mo>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">(</mo>\n<mi>n</mi>\n<mo stretchy="false">)</mo>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>Y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">(</mo>\n<mi>n</mi>\n<mo stretchy="false">)</mo>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (X_{(1)},Y_{(1)}),\\dots ,(X_{(n)},Y_{(n)})}</annotation>\n</semantics>\n</math></span><img alt="(X_{{(1)}},Y_{{(1)}}),\\dots ,(X_{{(n)}},Y_{{(n)}})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961b632956473933c75b8cbfc56f8a83ccd41524" style="vertical-align: -1.171ex; width:27.077ex; height:3.176ex;"/></span> be a reordering of the training data such that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\|X_{(1)}-x\\|\\leq \\dots \\leq \\|X_{(n)}-x\\|}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo fence="false" stretchy="false">\xe2\x80\x96<!-- \xe2\x80\x96 --></mo>\n<msub>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">(</mo>\n<mn>1</mn>\n<mo stretchy="false">)</mo>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>x</mi>\n<mo fence="false" stretchy="false">\xe2\x80\x96<!-- \xe2\x80\x96 --></mo>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mo>\xe2\x8b\xaf<!-- \xe2\x8b\xaf --></mo>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mo fence="false" stretchy="false">\xe2\x80\x96<!-- \xe2\x80\x96 --></mo>\n<msub>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">(</mo>\n<mi>n</mi>\n<mo stretchy="false">)</mo>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>x</mi>\n<mo fence="false" stretchy="false">\xe2\x80\x96<!-- \xe2\x80\x96 --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\|X_{(1)}-x\\|\\leq \\dots \\leq \\|X_{(n)}-x\\|}</annotation>\n</semantics>\n</math></span><img alt="\\|X_{{(1)}}-x\\|\\leq \\dots \\leq \\|X_{{(n)}}-x\\|" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/364eb25128db023e58f0dc8584f7e6de07e94907" style="vertical-align: -1.171ex; width:30.59ex; height:3.176ex;"/></span>\n</p>\n<h2><span class="mw-headline" id="Algorithm">Algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=2" title="Edit section: Algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:KnnClassification.svg"><img alt="" class="thumbimage" data-file-height="252" data-file-width="279" decoding="async" height="199" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/330px-KnnClassification.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/440px-KnnClassification.svg.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:KnnClassification.svg" title="Enlarge"></a></div>Example of <i>k</i>-NN classification. The test sample (green dot) should be classified either to blue squares or to red triangles.  If <i>k = 3</i> (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle.  If <i>k = 5</i> (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).</div></div></div>\n<p>The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the <a class="mw-redirect" href="/wiki/Feature_vector" title="Feature vector">feature vectors</a> and class labels of the training samples.\n</p><p>In the classification phase, <i>k</i> is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the <i>k</i> training samples nearest to that query point.\n</p><p>A commonly used distance metric for <a class="mw-redirect" href="/wiki/Continuous_variable" title="Continuous variable">continuous variables</a> is <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a>. For discrete variables, such as for text classification, another metric can be used, such as the <b>overlap metric</b> (or <a href="/wiki/Hamming_distance" title="Hamming distance">Hamming distance</a>). In the context of gene expression microarray data, for example, <i>k</i>-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric.<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup> Often, the classification accuracy of <i>k</i>-NN can be improved significantly if the distance metric is learned with specialized algorithms such as <a class="mw-redirect" href="/wiki/Large_Margin_Nearest_Neighbor" title="Large Margin Nearest Neighbor">Large Margin Nearest Neighbor</a> or <a href="/wiki/Neighbourhood_components_analysis" title="Neighbourhood components analysis">Neighbourhood components analysis</a>.\n</p><p>A drawback of the basic "majority voting" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the <i>k</i> nearest neighbors due to their large number.<sup class="reference" id="cite_ref-Coomans_Massart1982_7-0"><a href="#cite_note-Coomans_Massart1982-7">[7]</a></sup> One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its <i>k</i> nearest neighbors. The class (or value, in regression problems) of each of the <i>k</i> nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a <a href="/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a> (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. <i>K</i>-NN can then be applied to the SOM.\n</p>\n<h2><span class="mw-headline" id="Parameter_selection">Parameter selection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=3" title="Edit section: Parameter selection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The best choice of <i>k</i> depends upon the data; generally, larger values of <i>k</i> reduces effect of the noise on the classification,<sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup> but make boundaries between classes less distinct. A good <i>k</i> can be selected by various <a href="/wiki/Heuristic_(computer_science)" title="Heuristic (computer science)">heuristic</a> techniques (see <a href="/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">hyperparameter optimization</a>). The special case where the class is predicted to be the class of the closest training sample (i.e. when <i>k</i> = 1) is called the nearest neighbor algorithm.\n</p><p>The accuracy of the <i>k</i>-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into <a href="/wiki/Feature_selection" title="Feature selection">selecting</a> or <a href="/wiki/Feature_scaling" title="Feature scaling">scaling</a> features to improve classification. A particularly popular<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2013)">citation needed</span></a></i>]</sup> approach is the use of <a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">evolutionary algorithms</a> to optimize feature scaling.<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Another popular approach is to scale features by the <a href="/wiki/Mutual_information" title="Mutual information">mutual information</a> of the training data with the training classes.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2008)">citation needed</span></a></i>]</sup>\n</p><p>In binary (two class) classification problems, it is helpful to choose <i>k</i> to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal <i>k</i> in this setting is via bootstrap method.<sup class="reference" id="cite_ref-HPS2008_10-0"><a href="#cite_note-HPS2008-10">[10]</a></sup>\n</p>\n<h2><span class="mw-headline" id="The_1-nearest_neighbor_classifier">The <span class="texhtml">1</span>-nearest neighbor classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=4" title="Edit section: The 1-nearest neighbor classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point <span class="texhtml mvar" style="font-style:italic;">x</span> to the class of its closest neighbour in the feature space, that is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>C</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n<mi>n</mi>\n<mi>n</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<msub>\n<mi>Y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">(</mo>\n<mn>1</mn>\n<mo stretchy="false">)</mo>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}</annotation>\n</semantics>\n</math></span><img alt="C_{n}^{{1nn}}(x)=Y_{{(1)}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f815904edbff2ce82502172ec0dce3311d57f2bb" style="vertical-align: -1.171ex; width:14.746ex; height:3.343ex;"/></span>.\n</p><p>As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the <a href="/wiki/Bayes_error_rate" title="Bayes error rate">Bayes error rate</a> (the minimum achievable error rate given the distribution of the data).\n</p>\n<h2><span class="mw-headline" id="The_weighted_nearest_neighbour_classifier">The weighted nearest neighbour classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=5" title="Edit section: The weighted nearest neighbour classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The <span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbour classifier can be viewed as assigning the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbours a weight <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle 1/k}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mn>1</mn>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>k</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle 1/k}</annotation>\n</semantics>\n</math></span><img alt="1/k" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a7e9fedad8c70c6331b2640b56c23cef8c884e1f" style="vertical-align: -0.838ex; width:3.536ex; height:2.843ex;"/></span> and all others <span class="texhtml mvar" style="font-style:italic;">0</span> weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the <span class="texhtml mvar" style="font-style:italic;">i</span>th nearest neighbour is assigned a weight <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{ni}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{ni}}</annotation>\n</semantics>\n</math></span><img alt="w_{{ni}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6d76326293e410139d081d073068b9eb32a0777" style="vertical-align: -0.671ex; width:3.45ex; height:2.009ex;"/></span>, with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\sum _{i=1}^{n}w_{ni}=1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</munderover>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\sum _{i=1}^{n}w_{ni}=1}</annotation>\n</semantics>\n</math></span><img alt="\\sum _{{i=1}}^{n}w_{{ni}}=1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/167dcdfe9d31faea6f1e6c157cc23fe6e3b39fb7" style="vertical-align: -3.005ex; width:11.453ex; height:6.843ex;"/></span>. An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.<sup class="reference" id="cite_ref-Stone_11-0"><a href="#cite_note-Stone-11">[11]</a></sup>\n</p><p>Let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle C_{n}^{wnn}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>C</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>w</mi>\n<mi>n</mi>\n<mi>n</mi>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle C_{n}^{wnn}}</annotation>\n</semantics>\n</math></span><img alt="C_{n}^{{wnn}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e88b657ee88d912408396f8c9ef6af3483bfdf01" style="vertical-align: -0.671ex; width:5.179ex; height:2.509ex;"/></span> denote the weighted nearest classifier with weights <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\{w_{ni}\\}_{i=1}^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo fence="false" stretchy="false">{</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>i</mi>\n</mrow>\n</msub>\n<msubsup>\n<mo fence="false" stretchy="false">}</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\{w_{ni}\\}_{i=1}^{n}}</annotation>\n</semantics>\n</math></span><img alt="\\{w_{{ni}}\\}_{{i=1}}^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/efaf258e02ccae2b27c279885d6fb898adaf331d" style="vertical-align: -1.005ex; width:8.675ex; height:3.009ex;"/></span>. Subject to regularity conditions<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag needs further explanation. (January 2019)">further explanation needed</span></a></i>]</sup> on the class distributions the excess risk has the following asymptotic expansion<sup class="reference" id="cite_ref-Samworth12_12-0"><a href="#cite_note-Samworth12-12">[12]</a></sup>\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\mathcal {R}}_{\\mathcal {R}}(C_{n}^{wnn})-{\\mathcal {R}}_{\\mathcal {R}}(C^{Bayes})=\\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\\right)\\{1+o(1)\\},}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>\n</mrow>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>\n</mrow>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msubsup>\n<mi>C</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>w</mi>\n<mi>n</mi>\n<mi>n</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">)</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>\n</mrow>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>\n</mrow>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>C</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>B</mi>\n<mi>a</mi>\n<mi>y</mi>\n<mi>e</mi>\n<mi>s</mi>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mrow>\n<mo>(</mo>\n<mrow>\n<msub>\n<mi>B</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<msubsup>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msubsup>\n<mo>+</mo>\n<msub>\n<mi>B</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<msubsup>\n<mi>t</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msubsup>\n</mrow>\n<mo>)</mo>\n</mrow>\n<mo fence="false" stretchy="false">{</mo>\n<mn>1</mn>\n<mo>+</mo>\n<mi>o</mi>\n<mo stretchy="false">(</mo>\n<mn>1</mn>\n<mo stretchy="false">)</mo>\n<mo fence="false" stretchy="false">}</mo>\n<mo>,</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {R}}_{\\mathcal {R}}(C_{n}^{wnn})-{\\mathcal {R}}_{\\mathcal {R}}(C^{Bayes})=\\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\\right)\\{1+o(1)\\},}</annotation>\n</semantics>\n</math></span><img alt="{\\mathcal  {R}}_{{\\mathcal  {R}}}(C_{{n}}^{{wnn}})-{\\mathcal  {R}}_{{{\\mathcal  {R}}}}(C^{{Bayes}})=\\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\\right)\\{1+o(1)\\}," aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/02f064c830dbca4fcd695427bcc45c7aeb1b1196" style="vertical-align: -1.005ex; width:54.866ex; height:3.343ex;"/></span></dd></dl>\n<p>for constants <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle B_{1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>B</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle B_{1}}</annotation>\n</semantics>\n</math></span><img alt="B_{1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fa091eb428443c9c5c5fcf32a69d3665c89e00c" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle B_{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>B</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle B_{2}}</annotation>\n</semantics>\n</math></span><img alt="B_{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/199944d59dcc18842dfd1deab6000a1d1dadcbae" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;"/></span> where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{n}^{2}=\\sum _{i=1}^{n}w_{ni}^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msubsup>\n<mo>=</mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</munderover>\n<msubsup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{n}^{2}=\\sum _{i=1}^{n}w_{ni}^{2}}</annotation>\n</semantics>\n</math></span><img alt="s_{n}^{2}=\\sum _{{i=1}}^{n}w_{{ni}}^{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ed6dbd702b3141f1649ce10ccff3bac0acd55299" style="vertical-align: -3.005ex; width:12.599ex; height:6.843ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle t_{n}=n^{-2/d}\\sum _{i=1}^{n}w_{ni}\\{i^{1+2/d}-(i-1)^{1+2/d}\\}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>t</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msup>\n<mi>n</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>2</mn>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>d</mi>\n</mrow>\n</msup>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</munderover>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>i</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">{</mo>\n<msup>\n<mi>i</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n<mo>+</mo>\n<mn>2</mn>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>d</mi>\n</mrow>\n</msup>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mo stretchy="false">(</mo>\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n<mo>+</mo>\n<mn>2</mn>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>d</mi>\n</mrow>\n</msup>\n<mo fence="false" stretchy="false">}</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle t_{n}=n^{-2/d}\\sum _{i=1}^{n}w_{ni}\\{i^{1+2/d}-(i-1)^{1+2/d}\\}}</annotation>\n</semantics>\n</math></span><img alt="t_{n}=n^{{-2/d}}\\sum _{{i=1}}^{n}w_{{ni}}\\{i^{{1+2/d}}-(i-1)^{{1+2/d}}\\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd32f71ab3cd0784e73324108ecb05be734cd7de" style="vertical-align: -3.005ex; width:40.4ex; height:6.843ex;"/></span>.\n</p><p>The optimal weighting scheme <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\{w_{ni}^{*}\\}_{i=1}^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo fence="false" stretchy="false">{</mo>\n<msubsup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msubsup>\n<msubsup>\n<mo fence="false" stretchy="false">}</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\{w_{ni}^{*}\\}_{i=1}^{n}}</annotation>\n</semantics>\n</math></span><img alt="\\{w_{{ni}}^{*}\\}_{{i=1}}^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f97b387c9e937fac91f0644ac895c5c95d9a4921" style="vertical-align: -1.005ex; width:8.675ex; height:3.009ex;"/></span>, that balances the two terms in the display above, is given as follows: set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k^{*}=\\lfloor Bn^{\\frac {4}{d+4}}\\rfloor }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mo>=</mo>\n<mo fence="false" stretchy="false">\xe2\x8c\x8a<!-- \xe2\x8c\x8a --></mo>\n<mi>B</mi>\n<msup>\n<mi>n</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>4</mn>\n<mrow>\n<mi>d</mi>\n<mo>+</mo>\n<mn>4</mn>\n</mrow>\n</mfrac>\n</mrow>\n</msup>\n<mo fence="false" stretchy="false">\xe2\x8c\x8b<!-- \xe2\x8c\x8b --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k^{*}=\\lfloor Bn^{\\frac {4}{d+4}}\\rfloor }</annotation>\n</semantics>\n</math></span><img alt="k^{*}=\\lfloor Bn^{{{\\frac  4{d+4}}}}\\rfloor " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5cbecc881f1b8637b3d4d4527fd1671f5be252fa" style="vertical-align: -0.838ex; width:14.059ex; height:4.176ex;"/></span>, \n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{ni}^{*}={\\frac {1}{k^{*}}}\\left[1+{\\frac {d}{2}}-{\\frac {d}{2{k^{*}}^{2/d}}}\\{i^{1+2/d}-(i-1)^{1+2/d}\\}\\right]}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msubsup>\n<mo>=</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>1</mn>\n<msup>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n</mfrac>\n</mrow>\n<mrow>\n<mo>[</mo>\n<mrow>\n<mn>1</mn>\n<mo>+</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mi>d</mi>\n<mn>2</mn>\n</mfrac>\n</mrow>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mi>d</mi>\n<mrow>\n<mn>2</mn>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<msup>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>d</mi>\n</mrow>\n</msup>\n</mrow>\n</mfrac>\n</mrow>\n<mo fence="false" stretchy="false">{</mo>\n<msup>\n<mi>i</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n<mo>+</mo>\n<mn>2</mn>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>d</mi>\n</mrow>\n</msup>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mo stretchy="false">(</mo>\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n<mo>+</mo>\n<mn>2</mn>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>d</mi>\n</mrow>\n</msup>\n<mo fence="false" stretchy="false">}</mo>\n</mrow>\n<mo>]</mo>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{ni}^{*}={\\frac {1}{k^{*}}}\\left[1+{\\frac {d}{2}}-{\\frac {d}{2{k^{*}}^{2/d}}}\\{i^{1+2/d}-(i-1)^{1+2/d}\\}\\right]}</annotation>\n</semantics>\n</math></span><img alt="w_{{ni}}^{*}={\\frac  1{k^{*}}}\\left[1+{\\frac  d2}-{\\frac  d{2{k^{*}}^{{2/d}}}}\\{i^{{1+2/d}}-(i-1)^{{1+2/d}}\\}\\right]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fbfa4058134234385c31544db3e657c4b242ab34" style="vertical-align: -2.505ex; width:50.643ex; height:6.176ex;"/></span> for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i=1,2,\\dots ,k^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n<mo>,</mo>\n<mn>2</mn>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<msup>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i=1,2,\\dots ,k^{*}}</annotation>\n</semantics>\n</math></span><img alt="i=1,2,\\dots ,k^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ea7e974f7466c9dedfe409ea013bc719264872b" style="vertical-align: -0.671ex; width:14.703ex; height:2.676ex;"/></span> and</dd>\n<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{ni}^{*}=0}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msubsup>\n<mo>=</mo>\n<mn>0</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{ni}^{*}=0}</annotation>\n</semantics>\n</math></span><img alt="w_{{ni}}^{*}=0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6bd3d5b77d7fef0dabd4326ee85b04fa244fa988" style="vertical-align: -1.005ex; width:7.711ex; height:2.843ex;"/></span> for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i=k^{*}+1,\\dots ,n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n<mo>=</mo>\n<msup>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mo>+</mo>\n<mn>1</mn>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i=k^{*}+1,\\dots ,n}</annotation>\n</semantics>\n</math></span><img alt="i=k^{*}+1,\\dots ,n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7c2f2e5c12f3febcefe0aae189d44031daf8e79" style="vertical-align: -0.671ex; width:16.742ex; height:2.676ex;"/></span>.</dd></dl>\n<p>With optimal weights the dominant term in the asymptotic expansion of the excess risk is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\mathcal {O}}(n^{-{\\frac {4}{d+4}}})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n</mrow>\n</mrow>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>n</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>4</mn>\n<mrow>\n<mi>d</mi>\n<mo>+</mo>\n<mn>4</mn>\n</mrow>\n</mfrac>\n</mrow>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {O}}(n^{-{\\frac {4}{d+4}}})}</annotation>\n</semantics>\n</math></span><img alt="{\\mathcal  {O}}(n^{{-{\\frac  4{d+4}}}})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07280735a8852d609ffd3942647d7e3255697f05" style="vertical-align: -0.838ex; width:9.804ex; height:4.176ex;"/></span>. Similar results are true when using a <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bagged nearest neighbour classifier</a>.\n</p>\n<h2><span class="mw-headline" id="Properties">Properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=6" title="Edit section: Properties">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p><i>k</i>-NN is a special case of a <a href="/wiki/Variable_kernel_density_estimation" title="Variable kernel density estimation">variable-bandwidth, kernel density "balloon" estimator</a> with a uniform <a href="/wiki/Kernel_(statistics)" title="Kernel (statistics)">kernel</a>.<sup class="reference" id="cite_ref-Terrell_Scott1992_13-0"><a href="#cite_note-Terrell_Scott1992-13">[13]</a></sup>\n<sup class="reference" id="cite_ref-Mills2010_14-0"><a href="#cite_note-Mills2010-14">[14]</a></sup>\n</p><p>The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate <a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">nearest neighbor search</a> algorithm makes <i>k-</i>NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.\n</p><p><i>k-</i>NN has some strong <a href="/wiki/Consistency_(statistics)" title="Consistency (statistics)">consistency</a> results. As the amount of data approaches infinity, the two-class <i>k-</i>NN algorithm is guaranteed to yield an error rate no worse than twice the <a href="/wiki/Bayes_error_rate" title="Bayes error rate">Bayes error rate</a> (the minimum achievable error rate given the distribution of the data).<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> Various improvements to the <i>k</i>-NN speed are possible by using proximity graphs.<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>\n</p><p>For multi-class <i>k-</i>NN classification, <a href="/wiki/Thomas_M._Cover" title="Thomas M. Cover">Cover</a> and <a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart</a> (1967) prove an upper bound error rate of\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle R^{*}\\ \\leq \\ R_{k\\mathrm {NN} }\\ \\leq \\ R^{*}\\left(2-{\\frac {MR^{*}}{M-1}}\\right)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>R</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mtext>\xc2\xa0</mtext>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mtext>\xc2\xa0</mtext>\n<msub>\n<mi>R</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="normal">N</mi>\n<mi mathvariant="normal">N</mi>\n</mrow>\n</mrow>\n</msub>\n<mtext>\xc2\xa0</mtext>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mtext>\xc2\xa0</mtext>\n<msup>\n<mi>R</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mrow>\n<mo>(</mo>\n<mrow>\n<mn>2</mn>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mrow>\n<mi>M</mi>\n<msup>\n<mi>R</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n</mrow>\n<mrow>\n<mi>M</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</mfrac>\n</mrow>\n</mrow>\n<mo>)</mo>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle R^{*}\\ \\leq \\ R_{k\\mathrm {NN} }\\ \\leq \\ R^{*}\\left(2-{\\frac {MR^{*}}{M-1}}\\right)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle R^{*}\\ \\leq \\ R_{k\\mathrm {NN} }\\ \\leq \\ R^{*}\\left(2-{\\frac {MR^{*}}{M-1}}\\right)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3a20a21397df93ca13f8098a25731888f763efe7" style="vertical-align: -2.505ex; width:34.566ex; height:6.176ex;"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle R^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>R</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle R^{*}}</annotation>\n</semantics>\n</math></span><img alt="R^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a74b1bc0fa98794b6460254044f8e7b75e6d84f" style="vertical-align: -0.338ex; width:2.818ex; height:2.343ex;"/></span>is the Bayes error rate (which is the minimal error rate possible),  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle R_{kNN}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>R</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>k</mi>\n<mi>N</mi>\n<mi>N</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle R_{kNN}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle R_{kNN}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f3666637c81f02c1bfeb4e1969828745062aeba" style="vertical-align: -0.671ex; width:5.771ex; height:2.509ex;"/></span> is the <i>k-</i>NN error rate, and <span class="texhtml mvar" style="font-style:italic;">M</span> is the number of classes in the problem.  For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle M=2}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>M</mi>\n<mo>=</mo>\n<mn>2</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle M=2}</annotation>\n</semantics>\n</math></span><img alt="M=2" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2801ac77396e68de0b640087e1531a2329067e9f" style="vertical-align: -0.338ex; width:6.703ex; height:2.176ex;"/></span> and as the Bayesian error rate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle R^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>R</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle R^{*}}</annotation>\n</semantics>\n</math></span><img alt="R^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a74b1bc0fa98794b6460254044f8e7b75e6d84f" style="vertical-align: -0.338ex; width:2.818ex; height:2.343ex;"/></span> approaches zero, this limit reduces to "not more than twice the Bayesian error rate".\n</p>\n<h2><span class="mw-headline" id="Error_rates">Error rates</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=7" title="Edit section: Error rates">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>There are many results on the error rate of the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbour classifiers.<sup class="reference" id="cite_ref-PTPR_17-0"><a href="#cite_note-PTPR-17">[17]</a></sup>  The <span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbour classifier is strongly (that is for any joint distribution on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (X,Y)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<mi>X</mi>\n<mo>,</mo>\n<mi>Y</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (X,Y)}</annotation>\n</semantics>\n</math></span><img alt="(X,Y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/41f29b9537685f499713112d6802e811cbf51bba" style="vertical-align: -0.838ex; width:6.597ex; height:2.843ex;"/></span>) <a href="/wiki/Bayes_classifier" title="Bayes classifier">consistent</a> provided <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k:=k_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>k</mi>\n<mo>:=</mo>\n<msub>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k:=k_{n}}</annotation>\n</semantics>\n</math></span><img alt="k:=k_{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0fccbcda1dd871e437cca392a4294e6affdb5692" style="vertical-align: -0.671ex; width:7.386ex; height:2.509ex;"/></span> diverges and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k_{n}/n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k_{n}/n}</annotation>\n</semantics>\n</math></span><img alt="k_{n}/n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4e9a4dfc54979e5190d6baa3cc552824357edca1" style="vertical-align: -0.838ex; width:4.987ex; height:2.843ex;"/></span> converges to zero as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle n\\to \\infty }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>n</mi>\n<mo stretchy="false">\xe2\x86\x92<!-- \xe2\x86\x92 --></mo>\n<mi mathvariant="normal">\xe2\x88\x9e<!-- \xe2\x88\x9e --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle n\\to \\infty }</annotation>\n</semantics>\n</math></span><img alt="n\\to \\infty " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0d55d9b32f6fa8fab6a84ea444a6b5a24bb45e1" style="vertical-align: -0.338ex; width:7.333ex; height:1.843ex;"/></span>.\n</p><p>Let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle C_{n}^{knn}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>C</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>k</mi>\n<mi>n</mi>\n<mi>n</mi>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle C_{n}^{knn}}</annotation>\n</semantics>\n</math></span><img alt="C_{n}^{{knn}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3f2588b76da9c7429b56cec54535a18a8cae6386" style="vertical-align: -0.671ex; width:4.859ex; height:2.843ex;"/></span> denote the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbour classifier based on a training set of size <span class="texhtml mvar" style="font-style:italic;">n</span>. Under certain regularity conditions, the <a href="/wiki/Bayes_classifier" title="Bayes classifier">excess risk</a> yields the following asymptotic expansion<sup class="reference" id="cite_ref-Samworth12_12-1"><a href="#cite_note-Samworth12-12">[12]</a></sup>\n</p>\n<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\mathcal {R}}_{\\mathcal {R}}(C_{n}^{knn})-{\\mathcal {R}}_{\\mathcal {R}}(C^{Bayes})=\\left\\{B_{1}{\\frac {1}{k}}+B_{2}\\left({\\frac {k}{n}}\\right)^{4/d}\\right\\}\\{1+o(1)\\},}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>\n</mrow>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>\n</mrow>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msubsup>\n<mi>C</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>k</mi>\n<mi>n</mi>\n<mi>n</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">)</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>\n</mrow>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>\n</mrow>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>C</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>B</mi>\n<mi>a</mi>\n<mi>y</mi>\n<mi>e</mi>\n<mi>s</mi>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mrow>\n<mo>{</mo>\n<mrow>\n<msub>\n<mi>B</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>1</mn>\n<mi>k</mi>\n</mfrac>\n</mrow>\n<mo>+</mo>\n<msub>\n<mi>B</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<msup>\n<mrow>\n<mo>(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mi>k</mi>\n<mi>n</mi>\n</mfrac>\n</mrow>\n<mo>)</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>4</mn>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<mi>d</mi>\n</mrow>\n</msup>\n</mrow>\n<mo>}</mo>\n</mrow>\n<mo fence="false" stretchy="false">{</mo>\n<mn>1</mn>\n<mo>+</mo>\n<mi>o</mi>\n<mo stretchy="false">(</mo>\n<mn>1</mn>\n<mo stretchy="false">)</mo>\n<mo fence="false" stretchy="false">}</mo>\n<mo>,</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {R}}_{\\mathcal {R}}(C_{n}^{knn})-{\\mathcal {R}}_{\\mathcal {R}}(C^{Bayes})=\\left\\{B_{1}{\\frac {1}{k}}+B_{2}\\left({\\frac {k}{n}}\\right)^{4/d}\\right\\}\\{1+o(1)\\},}</annotation>\n</semantics>\n</math></span><img alt="{\\mathcal  {R}}_{{\\mathcal  {R}}}(C_{{n}}^{{knn}})-{\\mathcal  {R}}_{{{\\mathcal  {R}}}}(C^{{Bayes}})=\\left\\{B_{1}{\\frac  1k}+B_{2}\\left({\\frac  kn}\\right)^{{4/d}}\\right\\}\\{1+o(1)\\}," aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d37bdd6d06251f082b0fe52c3f7da579b11cc85" style="vertical-align: -3.171ex; width:62.23ex; height:7.509ex;"/></span></dd></dl></dd></dl>\n<p>for some constants <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle B_{1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>B</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle B_{1}}</annotation>\n</semantics>\n</math></span><img alt="B_{1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fa091eb428443c9c5c5fcf32a69d3665c89e00c" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle B_{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>B</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle B_{2}}</annotation>\n</semantics>\n</math></span><img alt="B_{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/199944d59dcc18842dfd1deab6000a1d1dadcbae" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;"/></span>.\n</p><p>The choice <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k^{*}=\\lfloor Bn^{\\frac {4}{d+4}}\\rfloor }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mo>=</mo>\n<mo fence="false" stretchy="false">\xe2\x8c\x8a<!-- \xe2\x8c\x8a --></mo>\n<mi>B</mi>\n<msup>\n<mi>n</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>4</mn>\n<mrow>\n<mi>d</mi>\n<mo>+</mo>\n<mn>4</mn>\n</mrow>\n</mfrac>\n</mrow>\n</msup>\n<mo fence="false" stretchy="false">\xe2\x8c\x8b<!-- \xe2\x8c\x8b --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k^{*}=\\lfloor Bn^{\\frac {4}{d+4}}\\rfloor }</annotation>\n</semantics>\n</math></span><img alt="k^{*}=\\lfloor Bn^{{{\\frac  4{d+4}}}}\\rfloor " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5cbecc881f1b8637b3d4d4527fd1671f5be252fa" style="vertical-align: -0.838ex; width:14.059ex; height:4.176ex;"/></span> offers a trade off between the two terms in the above display, for which the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k^{*}}</annotation>\n</semantics>\n</math></span><img alt="k^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc02328d1105a031ca024abcb86629ea4edb3cc8" style="vertical-align: -0.338ex; width:2.265ex; height:2.343ex;"/></span>-nearest neighbour error converges to the Bayes error at the optimal (<a href="/wiki/Minimax" title="Minimax">minimax</a>) rate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\mathcal {O}}(n^{-{\\frac {4}{d+4}}})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n</mrow>\n</mrow>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>n</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>4</mn>\n<mrow>\n<mi>d</mi>\n<mo>+</mo>\n<mn>4</mn>\n</mrow>\n</mfrac>\n</mrow>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {O}}(n^{-{\\frac {4}{d+4}}})}</annotation>\n</semantics>\n</math></span><img alt="{\\mathcal  {O}}(n^{{-{\\frac  4{d+4}}}})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07280735a8852d609ffd3942647d7e3255697f05" style="vertical-align: -0.838ex; width:9.804ex; height:4.176ex;"/></span>.\n</p>\n<h2><span class="mw-headline" id="Metric_learning">Metric learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=8" title="Edit section: Metric learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are <a href="/wiki/Neighbourhood_components_analysis" title="Neighbourhood components analysis">neighbourhood components analysis</a> and <a href="/wiki/Large_margin_nearest_neighbor" title="Large margin nearest neighbor">large margin nearest neighbor</a>. Supervised metric learning algorithms use the label information to learn a new <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric</a> or <a href="/wiki/Pseudometric_space" title="Pseudometric space">pseudo-metric</a>.\n</p>\n<h2><span class="mw-headline" id="Feature_extraction">Feature extraction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=9" title="Edit section: Feature extraction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called <a href="/wiki/Feature_extraction" title="Feature extraction">feature extraction</a>. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying <i>k</i>-NN algorithm on the transformed data in <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature space</a>.\n</p><p>An example of a typical <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> computation pipeline for <a href="/wiki/Facial_recognition_system" title="Facial recognition system">face recognition</a> using <i>k</i>-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with <a href="/wiki/OpenCV" title="OpenCV">OpenCV</a>):\n</p>\n<ol><li><a href="/wiki/Haar_wavelet" title="Haar wavelet">Haar</a> face detection</li>\n<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a> tracking analysis</li>\n<li><a class="mw-redirect" href="/wiki/Principal_Component_Analysis" title="Principal Component Analysis">PCA</a> or <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Fisher LDA</a> projection into feature space, followed by <i>k</i>-NN classification</li></ol>\n<h2><span class="mw-headline" id="Dimension_reduction">Dimension reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=10" title="Edit section: Dimension reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>For high-dimensional data (e.g., with number of dimensions more than 10) <a class="mw-redirect" href="/wiki/Dimension_reduction" title="Dimension reduction">dimension reduction</a> is usually performed prior to applying the <i>k</i>-NN algorithm in order to avoid the effects of the <a class="mw-redirect" href="/wiki/Curse_of_Dimensionality" title="Curse of Dimensionality">curse of dimensionality</a>.\n<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup>\n</p><p>The <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a> in the <i>k</i>-NN context basically means that <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).\n</p><p><a href="/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a> and dimension reduction can be combined in one step using <a class="mw-redirect" href="/wiki/Principal_Component_Analysis" title="Principal Component Analysis">principal component analysis</a> (PCA),  <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">linear discriminant analysis</a> (LDA), or <a href="/wiki/Canonical_correlation" title="Canonical correlation">canonical correlation analysis</a> (CCA) techniques as a pre-processing step, followed by clustering by <i>k</i>-NN on <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature vectors</a> in reduced-dimension space. In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> this process is also called low-dimensional <a href="/wiki/Embedding" title="Embedding">embedding</a>.<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup>\n</p><p>For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional <a href="/wiki/Time_series" title="Time series">time series</a>) running a fast <b>approximate</b> <i>k</i>-NN search using <a class="mw-redirect" href="/wiki/Locality_Sensitive_Hashing" title="Locality Sensitive Hashing">locality sensitive hashing</a>, "random projections",<sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> "sketches" <sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> or other high-dimensional similarity search techniques from the <a class="mw-redirect" href="/wiki/VLDB_conference" title="VLDB conference">VLDB</a> toolbox might be the only feasible option.\n</p>\n<h2><span class="mw-headline" id="Decision_boundary">Decision boundary</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=11" title="Edit section: Decision boundary">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>Nearest neighbor rules in effect implicitly compute the <a href="/wiki/Decision_boundary" title="Decision boundary">decision boundary</a>. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.<sup class="reference" id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>\n</p>\n<h2><span class="mw-headline" id="Data_reduction">Data reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=12" title="Edit section: Data reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p><a href="/wiki/Data_reduction" title="Data reduction">Data reduction</a> is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the <i>prototypes</i> and can be found as follows:\n</p>\n<ol><li>Select the <i>class-outliers</i>, that is, training data that are classified incorrectly by <i>k</i>-NN (for a given <i>k</i>)</li>\n<li>Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the <i>absorbed points</i> that can be correctly classified by <i>k</i>-NN using prototypes. The absorbed points can then be removed from the training set.</li></ol>\n<h3><span class="mw-headline" id="Selection_of_class-outliers">Selection of class-outliers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=13" title="Edit section: Selection of class-outliers">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:\n</p>\n<ul><li>random error</li>\n<li>insufficient training examples of this class (an isolated example appears instead of a cluster)</li>\n<li>missing important features (the classes are separated in other dimensions which we do not know)</li>\n<li>too many training examples of other classes (unbalanced classes) that create a "hostile" background for the given small class</li></ul>\n<p>Class outliers with <i>k</i>-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, <i>k&gt;r&gt;0</i>, a training example is called a <i>(k,r)</i>NN class-outlier if its <i>k</i> nearest neighbors include more than <i>r</i> examples of other classes.\n</p>\n<h3><span class="mw-headline" id="CNN_for_data_reduction">CNN for data reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=14" title="Edit section: CNN for data reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Condensed nearest neighbor (CNN, the <i><a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart</a> algorithm</i>) is an algorithm designed to reduce the data set for <i>k</i>-NN classification.<sup class="reference" id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> It selects the set of prototypes <i>U</i> from the training data, such that 1NN with <i>U</i> can classify the examples almost as accurately as 1NN does with the whole data set.\n</p>\n<div class="thumb tright"><div class="thumbinner" style="width:132px;"><a class="image" href="/wiki/File:BorderRAtio.PNG"><img alt="" class="thumbimage" data-file-height="127" data-file-width="159" decoding="async" height="104" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e6/BorderRAtio.PNG/130px-BorderRAtio.PNG" srcset="//upload.wikimedia.org/wikipedia/commons/e/e6/BorderRAtio.PNG 1.5x" width="130"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:BorderRAtio.PNG" title="Enlarge"></a></div>Calculation of the border ratio.</div></div></div>\n<div class="thumb tright"><div class="thumbinner" style="width:132px;"><a class="image" href="/wiki/File:PointsTypes.png"><img alt="" class="thumbimage" data-file-height="59" data-file-width="130" decoding="async" height="59" src="//upload.wikimedia.org/wikipedia/commons/e/e7/PointsTypes.png" width="130"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:PointsTypes.png" title="Enlarge"></a></div>Three types of points: prototypes, class-outliers, and absorbed points.</div></div></div>\n<p>Given a training set <i>X</i>, CNN works iteratively:\n</p>\n<ol><li>Scan all elements of <i>X</i>, looking for an element <i>x</i> whose nearest prototype from <i>U</i> has a different label than <i>x</i>.</li>\n<li>Remove <i>x</i> from <i>X</i> and add it to <i>U</i></li>\n<li>Repeat the scan until no more prototypes are added to <i>U</i>.</li></ol>\n<p>Use <i>U</i> instead of <i>X</i> for classification. The examples that are not prototypes are called "absorbed" points.\n</p><p>It is efficient to scan the training examples in order of decreasing border ratio.<sup class="reference" id="cite_ref-MirkesKnn_24-0"><a href="#cite_note-MirkesKnn-24">[24]</a></sup> The border ratio of a training example <i>x</i> is defined as \n</p>\n<dl><dd><span class="texhtml"><i>a</i>(<i>x</i>) = <span class="sfrac nowrap tion" role="math" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"><span class="num" style="display:block; line-height:1em; margin:0 0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x\'-y</i></span>|</span>|</span><span class="slash visualhide">/</span><span class="den" style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>|</span>|</span></span></span></dd></dl>\n<p>where <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>|</span>|</span> is the distance to the closest example <i>y</i> having a different color than <i>x</i>, and <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x\'-y</i></span>|</span>|</span> is the distance from <i>y</i> to its closest example <i>x\' </i> with the same label as <i>x</i>.\n</p><p>The border ratio is in the interval [0,1] because <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x\'-y</i></span>|</span>|</span>never exceeds <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>|</span>|</span>. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes <i>U</i>. A point of a different label than <i>x</i> is called external to <i>x</i>. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is <i>x</i> and its label is red. External points are blue and green. The closest to <i>x</i> external point is <i>y</i>. The closest to <i>y</i> red point is <i>x\' </i>. The border ratio <span class="texhtml"><i>a</i>(<i>x</i>) = |<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x\'-y</i></span>|</span>| / |<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>|</span>|</span>is the attribute of the initial point <i>x</i>.\n</p><p>Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.<sup class="reference" id="cite_ref-MirkesKnn_24-1"><a href="#cite_note-MirkesKnn-24">[24]</a></sup>\n</p>\n<ul class="gallery mw-gallery-traditional">\n<li class="gallerycaption">CNN model reduction for k-NN classifiers</li>\n<li class="gallerybox" style="width: 235px"><div style="width: 235px">\n<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Data3classes.png"><img alt="" data-file-height="397" data-file-width="602" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/182px-Data3classes.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/273px-Data3classes.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/364px-Data3classes.png 2x" width="182"/></a></div></div>\n<div class="gallerytext">\n<p>Fig. 1. The dataset.\n</p>\n</div>\n</div></li>\n<li class="gallerybox" style="width: 235px"><div style="width: 235px">\n<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Map1NN.png"><img alt="" data-file-height="397" data-file-width="603" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/183px-Map1NN.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/274px-Map1NN.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/365px-Map1NN.png 2x" width="183"/></a></div></div>\n<div class="gallerytext">\n<p>Fig. 2. The 1NN classification map.\n</p>\n</div>\n</div></li>\n<li class="gallerybox" style="width: 235px"><div style="width: 235px">\n<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Map5NN.png"><img alt="" data-file-height="396" data-file-width="602" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/183px-Map5NN.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/274px-Map5NN.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/365px-Map5NN.png 2x" width="183"/></a></div></div>\n<div class="gallerytext">\n<p>Fig. 3. The 5NN classification map.\n</p>\n</div>\n</div></li>\n<li class="gallerybox" style="width: 235px"><div style="width: 235px">\n<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:ReducedDataSet.png"><img alt="" data-file-height="402" data-file-width="608" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/182px-ReducedDataSet.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/272px-ReducedDataSet.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/363px-ReducedDataSet.png 2x" width="182"/></a></div></div>\n<div class="gallerytext">\n<p>Fig. 4. The CNN reduced dataset.\n</p>\n</div>\n</div></li>\n<li class="gallerybox" style="width: 235px"><div style="width: 235px">\n<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Map1NNReducedDataSet.png"><img alt="" data-file-height="401" data-file-width="611" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/183px-Map1NNReducedDataSet.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/275px-Map1NNReducedDataSet.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/366px-Map1NNReducedDataSet.png 2x" width="183"/></a></div></div>\n<div class="gallerytext">\n<p>Fig. 5. The 1NN classification map based on the CNN extracted prototypes.\n</p>\n</div>\n</div></li>\n</ul>\n<h2><span class="mw-headline" id="k-NN_regression"><i>k</i>-NN regression</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=15" title="Edit section: k-NN regression">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>In <i>k</i>-NN regression, the <i>k</i>-NN algorithm<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2019)">citation needed</span></a></i>]</sup> is used for estimating continuous variables. One such algorithm uses a weighted average of the <i>k</i> nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:\n</p>\n<ol><li>Compute the Euclidean or <a href="/wiki/Mahalanobis_distance" title="Mahalanobis distance">Mahalanobis distance</a> from the query example to the labeled examples.</li>\n<li>Order the labeled examples by increasing distance.</li>\n<li>Find a heuristically optimal number <i>k</i> of nearest neighbors, based on <a class="mw-redirect" href="/wiki/RMSE" title="RMSE">RMSE</a>. This is done using cross validation.</li>\n<li>Calculate an inverse distance weighted average with the <i>k</i>-nearest multivariate neighbors.</li></ol>\n<h2><span class="mw-headline" id="k-NN_outlier"><i>k</i>-NN outlier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=16" title="Edit section: k-NN outlier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The distance to the <i>k</i>th nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>. The larger the distance to the <i>k</i>-NN, the lower the local density, the more likely the query point is an outlier.<sup class="reference" id="cite_ref-25"><a href="#cite_note-25">[25]</a></sup> Although quite simple, this outlier model, along with another classic data mining method, <a href="/wiki/Local_outlier_factor" title="Local outlier factor">local outlier factor</a>, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.<sup class="reference" id="cite_ref-CamposZimek2016_26-0"><a href="#cite_note-CamposZimek2016-26">[26]</a></sup>\n</p>\n<h2><span class="mw-headline" id="Validation_of_results">Validation of results</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=17" title="Edit section: Validation of results">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>A <a href="/wiki/Confusion_matrix" title="Confusion matrix">confusion matrix</a> or "matching matrix" is often used as a tool to validate the accuracy of <i>k</i>-NN classification. More robust statistical methods such as <a href="/wiki/Likelihood-ratio_test" title="Likelihood-ratio test">likelihood-ratio test</a> can also be applied.<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="Please clarify the preceding statement or statements with a good explanation from a reliable source. (July 2020)">how?</span></a></i>]</sup>\n</p>\n<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=18" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<style data-mw-deduplicate="TemplateStyles:r936637989">.mw-parser-output .portal{border:solid #aaa 1px;padding:0}.mw-parser-output .portal.tleft{margin:0.5em 1em 0.5em 0}.mw-parser-output .portal.tright{margin:0.5em 0 0.5em 1em}.mw-parser-output .portal>ul{display:table;box-sizing:border-box;padding:0.1em;max-width:175px;background:#f9f9f9;font-size:85%;line-height:110%;font-style:italic;font-weight:bold}.mw-parser-output .portal>ul>li{display:table-row}.mw-parser-output .portal>ul>li>span:first-child{display:table-cell;padding:0.2em;vertical-align:middle;text-align:center}.mw-parser-output .portal>ul>li>span:last-child{display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle}</style><div aria-label="Portals" class="noprint portal plainlist tright" role="navigation">\n<ul>\n<li><span><a class="image" href="/wiki/File:Nuvola_apps_edu_mathematics_blue-p.svg"><img alt="icon" class="noviewer" data-file-height="128" data-file-width="128" decoding="async" height="28" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/28px-Nuvola_apps_edu_mathematics_blue-p.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/42px-Nuvola_apps_edu_mathematics_blue-p.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/56px-Nuvola_apps_edu_mathematics_blue-p.svg.png 2x" width="28"/></a></span><span><a href="/wiki/Portal:Mathematics" title="Portal:Mathematics">Mathematics portal</a></span></li></ul></div>\n<ul><li><a href="/wiki/Nearest_centroid_classifier" title="Nearest centroid classifier">Nearest centroid classifier</a></li>\n<li><a href="/wiki/Closest_pair_of_points_problem" title="Closest pair of points problem">Closest pair of points problem</a></li></ul>\n<div style="clear:right;"></div>\n<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=19" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">\n<ol class="references">\n<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFAltman1992"><a href="/wiki/Naomi_Altman" title="Naomi Altman">Altman, Naomi S.</a> (1992). <a class="external text" href="https://ecommons.cornell.edu/bitstream/1813/31637/1/BU-1065-MA.pdf" rel="nofollow">"An introduction to kernel and nearest-neighbor nonparametric regression"</a> <span class="cs1-format">(PDF)</span>. <i>The American Statistician</i>. <b>46</b> (3): 175\xe2\x80\x93185. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1080%2F00031305.1992.10475879" rel="nofollow">10.1080/00031305.1992.10475879</a>. <a class="mw-redirect" href="/wiki/Hdl_(identifier)" title="Hdl (identifier)">hdl</a>:<a class="external text" href="//hdl.handle.net/1813%2F31637" rel="nofollow">1813/31637</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Statistician&amp;rft.atitle=An+introduction+to+kernel+and+nearest-neighbor+nonparametric+regression&amp;rft.volume=46&amp;rft.issue=3&amp;rft.pages=175-185&amp;rft.date=1992&amp;rft_id=info%3Ahdl%2F1813%2F31637&amp;rft_id=info%3Adoi%2F10.1080%2F00031305.1992.10475879&amp;rft.aulast=Altman&amp;rft.aufirst=Naomi+S.&amp;rft_id=https%3A%2F%2Fecommons.cornell.edu%2Fbitstream%2F1813%2F31637%2F1%2FBU-1065-MA.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><style data-mw-deduplicate="TemplateStyles:r982806391">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>\n</li>\n<li id="cite_note-:0-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-:0_2-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFPiryonesi_S._MadehEl-Diraby_Tamer_E.2020">Piryonesi S. Madeh; El-Diraby Tamer E. (2020-06-01). "Role of Data Analytics in Infrastructure Asset Management: Overcoming Data Size and Quality Problems". <i>Journal of Transportation Engineering, Part B: Pavements</i>. <b>146</b> (2): 04020022. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1061%2FJPEODX.0000175" rel="nofollow">10.1061/JPEODX.0000175</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Transportation+Engineering%2C+Part+B%3A+Pavements&amp;rft.atitle=Role+of+Data+Analytics+in+Infrastructure+Asset+Management%3A+Overcoming+Data+Size+and+Quality+Problems&amp;rft.volume=146&amp;rft.issue=2&amp;rft.pages=04020022&amp;rft.date=2020-06-01&amp;rft_id=info%3Adoi%2F10.1061%2FJPEODX.0000175&amp;rft.au=Piryonesi+S.+Madeh&amp;rft.au=El-Diraby+Tamer+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFHastie,_Trevor.2001">Hastie, Trevor. (2001). <i>The elements of statistical learning\xc2\xa0: data mining, inference, and prediction\xc2\xa0: with 200 full-color illustrations</i>. Tibshirani, Robert., Friedman, J. H. (Jerome H.). New York: Springer. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/0-387-95284-5" title="Special:BookSources/0-387-95284-5"><bdi>0-387-95284-5</bdi></a>. <a class="mw-redirect" href="/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>\xc2\xa0<a class="external text" href="//www.worldcat.org/oclc/46809224" rel="nofollow">46809224</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+elements+of+statistical+learning+%3A+data+mining%2C+inference%2C+and+prediction+%3A+with+200+full-color+illustrations&amp;rft.place=New+York&amp;rft.pub=Springer&amp;rft.date=2001&amp;rft_id=info%3Aoclcnum%2F46809224&amp;rft.isbn=0-387-95284-5&amp;rft.au=Hastie%2C+Trevor.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFArianHariri,_AmMehridehnavi,_AFassihi,_A2020">Arian, R; Hariri, Am; Mehridehnavi, A; Fassihi, A; Ghasemi, F (April 27, 2020). "Protein kinase inhibitors\' classification using K-Nearest neighbor algorithm". <i>Computational Biology and Chemistry</i>. <b>86</b>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.compbiolchem.2020.107269" rel="nofollow">10.1016/j.compbiolchem.2020.107269</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computational+Biology+and+Chemistry&amp;rft.atitle=Protein+kinase+inhibitors%E2%80%99+classification+using+K-Nearest+neighbor+algorithm&amp;rft.volume=86&amp;rft.date=2020-04-27&amp;rft_id=info%3Adoi%2F10.1016%2Fj.compbiolchem.2020.107269&amp;rft.aulast=Arian&amp;rft.aufirst=R&amp;rft.au=Hariri%2C+Am&amp;rft.au=Mehridehnavi%2C+A&amp;rft.au=Fassihi%2C+A&amp;rft.au=Ghasemi%2C+F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">This scheme is a generalization of linear interpolation.</span>\n</li>\n<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFJaskowiakCampello">Jaskowiak, Pablo A.; Campello, Ricardo J. G. B. "Comparing Correlation Coefficients as Dissimilarity Measures for Cancer Classification in Gene Expression Data". <i>Brazilian Symposium on Bioinformatics (BSB 2011)</i>: 1\xe2\x80\x938. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a>\xc2\xa0<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.208.993" rel="nofollow">10.1.1.208.993</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Brazilian+Symposium+on+Bioinformatics+%28BSB+2011%29&amp;rft.atitle=Comparing+Correlation+Coefficients+as+Dissimilarity+Measures+for+Cancer+Classification+in+Gene+Expression+Data&amp;rft.pages=1-8&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.208.993&amp;rft.aulast=Jaskowiak&amp;rft.aufirst=Pablo+A.&amp;rft.au=Campello%2C+Ricardo+J.+G.+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-Coomans_Massart1982-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-Coomans_Massart1982_7-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFCoomansMassart1982">Coomans, Danny; Massart, Desire L. (1982). "Alternative k-nearest neighbour rules in supervised pattern recognition\xc2\xa0: Part 1. k-Nearest neighbour classification by using alternative voting rules". <i><a href="/wiki/Analytica_Chimica_Acta" title="Analytica Chimica Acta">Analytica Chimica Acta</a></i>. <b>136</b>: 15\xe2\x80\x9327. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2FS0003-2670%2801%2995359-0" rel="nofollow">10.1016/S0003-2670(01)95359-0</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Analytica+Chimica+Acta&amp;rft.atitle=Alternative+k-nearest+neighbour+rules+in+supervised+pattern+recognition+%3A+Part+1.+k-Nearest+neighbour+classification+by+using+alternative+voting+rules&amp;rft.volume=136&amp;rft.pages=15-27&amp;rft.date=1982&amp;rft_id=info%3Adoi%2F10.1016%2FS0003-2670%2801%2995359-0&amp;rft.aulast=Coomans&amp;rft.aufirst=Danny&amp;rft.au=Massart%2C+Desire+L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Everitt, Brian S.; Landau, Sabine; Leese, Morven; and Stahl, Daniel (2011) "Miscellaneous Clustering Methods", in <i>Cluster Analysis</i>, 5th Edition, John Wiley &amp; Sons, Ltd., Chichester, UK</span>\n</li>\n<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFNigschBendervan_BuurenTissen2006">Nigsch, Florian; Bender, Andreas; van Buuren, Bernd; Tissen, Jos; Nigsch, Eduard; Mitchell, John B. O. (2006). "Melting point prediction employing k-nearest neighbor algorithms and genetic parameter optimization". <i>Journal of Chemical Information and Modeling</i>. <b>46</b> (6): 2412\xe2\x80\x932422. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1021%2Fci060149f" rel="nofollow">10.1021/ci060149f</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>\xc2\xa0<a class="external text" href="//pubmed.ncbi.nlm.nih.gov/17125183" rel="nofollow">17125183</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Chemical+Information+and+Modeling&amp;rft.atitle=Melting+point+prediction+employing+k-nearest+neighbor+algorithms+and+genetic+parameter+optimization&amp;rft.volume=46&amp;rft.issue=6&amp;rft.pages=2412-2422&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1021%2Fci060149f&amp;rft_id=info%3Apmid%2F17125183&amp;rft.aulast=Nigsch&amp;rft.aufirst=Florian&amp;rft.au=Bender%2C+Andreas&amp;rft.au=van+Buuren%2C+Bernd&amp;rft.au=Tissen%2C+Jos&amp;rft.au=Nigsch%2C+Eduard&amp;rft.au=Mitchell%2C+John+B.+O.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-HPS2008-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-HPS2008_10-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFHallParkSamworth2008">Hall, Peter; Park, Byeong U.; Samworth, Richard J. (2008). "Choice of neighbor order in nearest-neighbor classification". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>36</b> (5): 2135\xe2\x80\x932152. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/0810.5276" rel="nofollow">0810.5276</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2008arXiv0810.5276H" rel="nofollow">2008arXiv0810.5276H</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1214%2F07-AOS537" rel="nofollow">10.1214/07-AOS537</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>\xc2\xa0<a class="external text" href="https://api.semanticscholar.org/CorpusID:14059866" rel="nofollow">14059866</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Choice+of+neighbor+order+in+nearest-neighbor+classification&amp;rft.volume=36&amp;rft.issue=5&amp;rft.pages=2135-2152&amp;rft.date=2008&amp;rft_id=info%3Aarxiv%2F0810.5276&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A14059866&amp;rft_id=info%3Adoi%2F10.1214%2F07-AOS537&amp;rft_id=info%3Abibcode%2F2008arXiv0810.5276H&amp;rft.aulast=Hall&amp;rft.aufirst=Peter&amp;rft.au=Park%2C+Byeong+U.&amp;rft.au=Samworth%2C+Richard+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-Stone-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-Stone_11-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFStone1977">Stone, Charles J. (1977). <a class="external text" href="https://doi.org/10.1214%2Faos%2F1176343886" rel="nofollow">"Consistent nonparametric regression"</a>. <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>5</b> (4): 595\xe2\x80\x93620. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1214%2Faos%2F1176343886" rel="nofollow">10.1214/aos/1176343886</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Consistent+nonparametric+regression&amp;rft.volume=5&amp;rft.issue=4&amp;rft.pages=595-620&amp;rft.date=1977&amp;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176343886&amp;rft.aulast=Stone&amp;rft.aufirst=Charles+J.&amp;rft_id=%2F%2Fdoi.org%2F10.1214%252Faos%252F1176343886&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-Samworth12-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-Samworth12_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Samworth12_12-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFSamworth2012">Samworth, Richard J. (2012). "Optimal weighted nearest neighbour classifiers". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>40</b> (5): 2733\xe2\x80\x932763. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1101.5783" rel="nofollow">1101.5783</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1214%2F12-AOS1049" rel="nofollow">10.1214/12-AOS1049</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>\xc2\xa0<a class="external text" href="https://api.semanticscholar.org/CorpusID:88511688" rel="nofollow">88511688</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Optimal+weighted+nearest+neighbour+classifiers&amp;rft.volume=40&amp;rft.issue=5&amp;rft.pages=2733-2763&amp;rft.date=2012&amp;rft_id=info%3Aarxiv%2F1101.5783&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A88511688&amp;rft_id=info%3Adoi%2F10.1214%2F12-AOS1049&amp;rft.aulast=Samworth&amp;rft.aufirst=Richard+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-Terrell_Scott1992-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-Terrell_Scott1992_13-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFTerrellScott1992">Terrell, George R.; Scott, David W. (1992). <a class="external text" href="https://doi.org/10.1214%2Faos%2F1176348768" rel="nofollow">"Variable kernel density estimation"</a>. <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>20</b> (3): 1236\xe2\x80\x931265. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1214%2Faos%2F1176348768" rel="nofollow">10.1214/aos/1176348768</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Variable+kernel+density+estimation&amp;rft.volume=20&amp;rft.issue=3&amp;rft.pages=1236-1265&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176348768&amp;rft.aulast=Terrell&amp;rft.aufirst=George+R.&amp;rft.au=Scott%2C+David+W.&amp;rft_id=%2F%2Fdoi.org%2F10.1214%252Faos%252F1176348768&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-Mills2010-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-Mills2010_14-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFMills2012">Mills, Peter (2012-08-09). <a class="external text" href="https://archive.org/details/arxiv-1202.2194" rel="nofollow">"Efficient statistical classification of satellite measurements"</a>. <i>International Journal of Remote Sensing</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Remote+Sensing&amp;rft.atitle=Efficient+statistical+classification+of+satellite+measurements&amp;rft.date=2012-08-09&amp;rft.aulast=Mills&amp;rft.aufirst=Peter&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Farxiv-1202.2194&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFCoverHart1967"><a href="/wiki/Thomas_M._Cover" title="Thomas M. Cover">Cover, Thomas M.</a>; <a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart, Peter E.</a> (1967). <a class="external text" href="http://ssg.mit.edu/cal/abs/2000_spring/np_dens/classification/cover67.pdf" rel="nofollow">"Nearest neighbor pattern classification"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Information Theory</i>. <b>13</b> (1): 21\xe2\x80\x9327. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a>\xc2\xa0<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.2616" rel="nofollow">10.1.1.68.2616</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FTIT.1967.1053964" rel="nofollow">10.1109/TIT.1967.1053964</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=Nearest+neighbor+pattern+classification&amp;rft.volume=13&amp;rft.issue=1&amp;rft.pages=21-27&amp;rft.date=1967&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.68.2616&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1967.1053964&amp;rft.aulast=Cover&amp;rft.aufirst=Thomas+M.&amp;rft.au=Hart%2C+Peter+E.&amp;rft_id=http%3A%2F%2Fssg.mit.edu%2Fcal%2Fabs%2F2000_spring%2Fnp_dens%2Fclassification%2Fcover67.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFToussaint2005">Toussaint, Godfried T. (April 2005). "Geometric proximity graphs for improving nearest neighbor methods in instance-based learning and data mining". <i>International Journal of Computational Geometry and Applications</i>. <b>15</b> (2): 101\xe2\x80\x93150. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1142%2FS0218195905001622" rel="nofollow">10.1142/S0218195905001622</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Computational+Geometry+and+Applications&amp;rft.atitle=Geometric+proximity+graphs+for+improving+nearest+neighbor+methods+in+instance-based+learning+and+data+mining&amp;rft.volume=15&amp;rft.issue=2&amp;rft.pages=101-150&amp;rft.date=2005-04&amp;rft_id=info%3Adoi%2F10.1142%2FS0218195905001622&amp;rft.aulast=Toussaint&amp;rft.aufirst=Godfried+T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-PTPR-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-PTPR_17-0">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFDevroyeGyorfiLugosi1996">Devroye, Luc; Gyorfi, Laszlo; Lugosi, Gabor (1996). <i>A probabilistic theory of pattern recognition</i>. Springer. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0-3879-4618-4" title="Special:BookSources/978-0-3879-4618-4"><bdi>978-0-3879-4618-4</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+probabilistic+theory+of+pattern+recognition&amp;rft.pub=Springer&amp;rft.date=1996&amp;rft.isbn=978-0-3879-4618-4&amp;rft.aulast=Devroye&amp;rft.aufirst=Luc&amp;rft.au=Gyorfi%2C+Laszlo&amp;rft.au=Lugosi%2C+Gabor&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBeyer">Beyer, Kevin;  et al. <a class="external text" href="https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1" rel="nofollow">"When is "nearest neighbor" meaningful?"</a> <span class="cs1-format">(PDF)</span>. <i>Database Theory\xe2\x80\x94ICDT\'99</i>. <b>1999</b>: 217\xe2\x80\x93235.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Database+Theory%E2%80%94ICDT%2799&amp;rft.atitle=When+is+%22nearest+neighbor%22+meaningful%3F&amp;rft.volume=1999&amp;rft.pages=217-235&amp;rft.aulast=Beyer&amp;rft.aufirst=Kevin&amp;rft_id=https%3A%2F%2Fminds.wisconsin.edu%2Fbitstream%2Fhandle%2F1793%2F60174%2FTR1377.pdf%3Fsequence%3D1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">Shaw, Blake; and Jebara, Tony; "Structure preserving embedding", in <i>Proceedings of the 26th Annual International Conference on Machine Learning</i>, ACM, 2009</span>\n</li>\n<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text">Bingham, Ella; and Mannila, Heikki; <a class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.5135&amp;rep=rep1&amp;type=pdf" rel="nofollow">"Random projection in dimensionality reduction: applications to image and text data"</a>, in <i>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</i>, ACM, 2001</span>\n</li>\n<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text">Ryan, Donna (editor); <i>High Performance Discovery in Time Series</i>, Berlin: Springer, 2004, <link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/><a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/0-387-00857-8" title="Special:BookSources/0-387-00857-8">0-387-00857-8</a></span>\n</li>\n<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBremnerDemaineEricksonIacono2005">Bremner, David; <a href="/wiki/Erik_Demaine" title="Erik Demaine">Demaine, Erik</a>; Erickson, Jeff; <a href="/wiki/John_Iacono" title="John Iacono">Iacono, John</a>; <a href="/wiki/Stefan_Langerman" title="Stefan Langerman">Langerman, Stefan</a>; <a href="/wiki/Pat_Morin" title="Pat Morin">Morin, Pat</a>; <a href="/wiki/Godfried_Toussaint" title="Godfried Toussaint">Toussaint, Godfried T.</a> (2005). <a class="external text" href="https://doi.org/10.1007%2Fs00454-004-1152-0" rel="nofollow">"Output-sensitive algorithms for computing nearest-neighbor decision boundaries"</a>. <i>Discrete and Computational Geometry</i>. <b>33</b> (4): 593\xe2\x80\x93604. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1007%2Fs00454-004-1152-0" rel="nofollow">10.1007/s00454-004-1152-0</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Discrete+and+Computational+Geometry&amp;rft.atitle=Output-sensitive+algorithms+for+computing+nearest-neighbor+decision+boundaries&amp;rft.volume=33&amp;rft.issue=4&amp;rft.pages=593-604&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1007%2Fs00454-004-1152-0&amp;rft.aulast=Bremner&amp;rft.aufirst=David&amp;rft.au=Demaine%2C+Erik&amp;rft.au=Erickson%2C+Jeff&amp;rft.au=Iacono%2C+John&amp;rft.au=Langerman%2C+Stefan&amp;rft.au=Morin%2C+Pat&amp;rft.au=Toussaint%2C+Godfried+T.&amp;rft_id=%2F%2Fdoi.org%2F10.1007%252Fs00454-004-1152-0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFHart1968"><a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart, Peter E.</a> (1968). "The Condensed Nearest Neighbor Rule". <i>IEEE Transactions on Information Theory</i>. <b>18</b>: 515\xe2\x80\x93516. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FTIT.1968.1054155" rel="nofollow">10.1109/TIT.1968.1054155</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=The+Condensed+Nearest+Neighbor+Rule&amp;rft.volume=18&amp;rft.pages=515-516&amp;rft.date=1968&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1968.1054155&amp;rft.aulast=Hart&amp;rft.aufirst=Peter+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-MirkesKnn-24"><span class="mw-cite-backlink">^ <a href="#cite_ref-MirkesKnn_24-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-MirkesKnn_24-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Mirkes, Evgeny M.; <a class="external text" href="http://www.math.le.ac.uk/people/ag153/homepage/KNN/KNN3.html" rel="nofollow"><i>KNN and Potential Energy: applet</i></a>, University of Leicester, 2011</span>\n</li>\n<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><cite class="citation conference cs1" id="CITEREFRamaswamyRastogiShim2000">Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000). <i>Efficient algorithms for mining outliers from large data sets</i>. Proceedings of the 2000 ACM SIGMOD international conference on Management of data \xe2\x80\x93 SIGMOD \'00. p.\xc2\xa0427. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1145%2F342009.335437" rel="nofollow">10.1145/342009.335437</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/1-58113-217-4" title="Special:BookSources/1-58113-217-4"><bdi>1-58113-217-4</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Efficient+algorithms+for+mining+outliers+from+large+data+sets&amp;rft.pages=427&amp;rft.date=2000&amp;rft_id=info%3Adoi%2F10.1145%2F342009.335437&amp;rft.isbn=1-58113-217-4&amp;rft.aulast=Ramaswamy&amp;rft.aufirst=Sridhar&amp;rft.au=Rastogi%2C+Rajeev&amp;rft.au=Shim%2C+Kyuseok&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-CamposZimek2016-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-CamposZimek2016_26-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFCamposZimekSanderCampello2016">Campos, Guilherme O.; Zimek, Arthur; Sander, J\xc3\xb6rg; Campello, Ricardo J. G. B.; Micenkov\xc3\xa1, Barbora; Schubert, Erich; Assent, Ira; Houle, Michael E. (2016). "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study". <i>Data Mining and Knowledge Discovery</i>. <b>30</b> (4): 891\xe2\x80\x93927. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2Fs10618-015-0444-8" rel="nofollow">10.1007/s10618-015-0444-8</a>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>\xc2\xa0<a class="external text" href="//www.worldcat.org/issn/1384-5810" rel="nofollow">1384-5810</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>\xc2\xa0<a class="external text" href="https://api.semanticscholar.org/CorpusID:1952214" rel="nofollow">1952214</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Data+Mining+and+Knowledge+Discovery&amp;rft.atitle=On+the+evaluation+of+unsupervised+outlier+detection%3A+measures%2C+datasets%2C+and+an+empirical+study&amp;rft.volume=30&amp;rft.issue=4&amp;rft.pages=891-927&amp;rft.date=2016&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1952214&amp;rft.issn=1384-5810&amp;rft_id=info%3Adoi%2F10.1007%2Fs10618-015-0444-8&amp;rft.aulast=Campos&amp;rft.aufirst=Guilherme+O.&amp;rft.au=Zimek%2C+Arthur&amp;rft.au=Sander%2C+J%C3%B6rg&amp;rft.au=Campello%2C+Ricardo+J.+G.+B.&amp;rft.au=Micenkov%C3%A1%2C+Barbora&amp;rft.au=Schubert%2C+Erich&amp;rft.au=Assent%2C+Ira&amp;rft.au=Houle%2C+Michael+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n</ol></div>\n<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=20" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><cite class="citation book cs1" id="CITEREFDasarathy,_Belur_V.1991"><a href="/wiki/Belur_V._Dasarathy" title="Belur V. Dasarathy">Dasarathy, Belur V.</a>, ed. (1991). <i>Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques</i>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0-8186-8930-7" title="Special:BookSources/978-0-8186-8930-7"><bdi>978-0-8186-8930-7</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nearest+Neighbor+%28NN%29+Norms%3A+NN+Pattern+Classification+Techniques&amp;rft.date=1991&amp;rft.isbn=978-0-8186-8930-7&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></li>\n<li><cite class="citation book cs1" id="CITEREFShakhnarovich,_GregoryDarrell,_TrevorIndyk,_Piotr2005">Shakhnarovich, Gregory; Darrell, Trevor; Indyk, Piotr, eds. (2005). <i>Nearest-Neighbor Methods in Learning and Vision</i>. <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0-262-19547-8" title="Special:BookSources/978-0-262-19547-8"><bdi>978-0-262-19547-8</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nearest-Neighbor+Methods+in+Learning+and+Vision&amp;rft.pub=MIT+Press&amp;rft.date=2005&amp;rft.isbn=978-0-262-19547-8&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></li></ul>\n<!-- \nNewPP limit report\nParsed by mw2269\nCached time: 20201013113015\nCache expiry: 2592000\nDynamic content: false\nComplications: [vary\xe2\x80\x90revision\xe2\x80\x90sha1]\nCPU time usage: 0.604 seconds\nReal time usage: 0.842 seconds\nPreprocessor visited node count: 2950/1000000\nPost\xe2\x80\x90expand include size: 92237/2097152 bytes\nTemplate argument size: 8073/2097152 bytes\nHighest expansion depth: 16/40\nExpensive parser function count: 6/500\nUnstrip recursion depth: 1/20\nUnstrip post\xe2\x80\x90expand size: 84944/5000000 bytes\nLua time usage: 0.268/10.000 seconds\nLua memory usage: 6.06 MB/50 MB\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  632.973      1 -total\n 50.80%  321.533      1 Template:Reflist\n 32.66%  206.702     17 Template:Cite_journal\n 16.47%  104.259      3 Template:Citation_needed\n 15.62%   98.848      5 Template:Fix\n 10.23%   64.765      8 Template:Category_handler\n  9.29%   58.788      1 Template:ISBN\n  7.99%   50.574      1 Template:Machine_learning_bar\n  7.39%   46.792      1 Template:Sidebar_with_collapsible_lists\n  4.78%   30.269      1 Template:Distinguish\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:1775388-0!canonical!math=5 and timestamp 20201013113015 and revision id 982934962\n -->\n</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>\n<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=982934962">https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=982934962</a>"</div></div>\n<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></li><li><a href="/wiki/Category:Search_algorithms" title="Category:Search algorithms">Search algorithms</a></li><li><a href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></li><li><a href="/wiki/Category:Statistical_classification" title="Category:Statistical classification">Statistical classification</a></li><li><a href="/wiki/Category:Nonparametric_statistics" title="Category:Nonparametric statistics">Nonparametric statistics</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_March_2013" title="Category:Articles with unsourced statements from March 2013">Articles with unsourced statements from March 2013</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_December_2008" title="Category:Articles with unsourced statements from December 2008">Articles with unsourced statements from December 2008</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_January_2019" title="Category:Wikipedia articles needing clarification from January 2019">Wikipedia articles needing clarification from January 2019</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_September_2019" title="Category:Articles with unsourced statements from September 2019">Articles with unsourced statements from September 2019</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_July_2020" title="Category:Wikipedia articles needing clarification from July 2020">Wikipedia articles needing clarification from July 2020</a></li></ul></div></div>\n</div>\n</div>\n<div id="mw-data-after-content">\n<div class="read-more-container"></div>\n</div>\n<div id="mw-navigation">\n<h2>Navigation menu</h2>\n<div id="mw-head">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-personal-label" class="mw-portlet mw-portlet-personal vector-menu" id="p-personal" role="navigation">\n<h3 id="p-personal-label">\n<span>Personal tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=K-nearest+neighbors+algorithm" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=K-nearest+neighbors+algorithm" title="You\'re encouraged to log in; however, it\'s not mandatory. [o]">Log in</a></li></ul>\n</div>\n</nav>\n<div id="left-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-namespaces-label" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" id="p-namespaces" role="navigation">\n<h3 id="p-namespaces-label">\n<span>Namespaces</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/K-nearest_neighbors_algorithm" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:K-nearest_neighbors_algorithm" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-variants-label" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu vector-menu-dropdown" id="p-variants" role="navigation">\n<input aria-labelledby="p-variants-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-variants-label">\n<span>Variants</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n</div>\n<div id="right-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-views-label" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" id="p-views" role="navigation">\n<h3 id="p-views-label">\n<span>Views</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/K-nearest_neighbors_algorithm">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=history" title="Past revisions of this page [h]">View history</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-cactions-label" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu vector-menu-dropdown" id="p-cactions" role="navigation">\n<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-cactions-label">\n<span>More</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n<div id="p-search" role="search">\n<h3>\n<label for="searchInput">Search</label>\n</h3>\n<form action="/w/index.php" id="searchform">\n<div data-search-loc="header-navigation" id="simpleSearch">\n<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>\n<input name="title" type="hidden" value="Special:Search"/>\n<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">\n<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>\n</input></div>\n</form>\n</div>\n</div>\n</div>\n<div id="mw-panel">\n<div id="p-logo" role="banner">\n<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>\n</div>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-navigation-label" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">\n<h3 id="p-navigation-label">\n<span>Navigation</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Articles related to current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-interaction-label" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">\n<h3 id="p-interaction-label">\n<span>Contribute</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-introduction"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia">Learn to edit</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-tb-label" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" id="p-tb" role="navigation">\n<h3 id="p-tb-label">\n<span>Tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/K-nearest_neighbors_algorithm" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/K-nearest_neighbors_algorithm" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=982934962" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=K-nearest_neighbors_algorithm&amp;id=982934962&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q1071612" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-coll-print_export-label" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">\n<h3 id="p-coll-print_export-label">\n<span>Print/export</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=K-nearest_neighbors_algorithm&amp;action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-lang-label" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" id="p-lang" role="navigation">\n<h3 id="p-lang-label">\n<span>Languages</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-ar"><a class="interlanguage-link-target" href="https://ar.wikipedia.org/wiki/%D9%83%D9%8A_%D8%A3%D9%82%D8%B1%D8%A8_%D8%AC%D8%A7%D8%B1" hreflang="ar" lang="ar" title="\xd9\x83\xd9\x8a \xd8\xa3\xd9\x82\xd8\xb1\xd8\xa8 \xd8\xac\xd8\xa7\xd8\xb1 \xe2\x80\x93 Arabic">\xd8\xa7\xd9\x84\xd8\xb9\xd8\xb1\xd8\xa8\xd9\x8a\xd8\xa9</a></li><li class="interlanguage-link interwiki-ca"><a class="interlanguage-link-target" href="https://ca.wikipedia.org/wiki/Knn" hreflang="ca" lang="ca" title="Knn \xe2\x80\x93 Catalan">Catal\xc3\xa0</a></li><li class="interlanguage-link interwiki-cs"><a class="interlanguage-link-target" href="https://cs.wikipedia.org/wiki/Algoritmus_k-nejbli%C5%BE%C5%A1%C3%ADch_soused%C5%AF" hreflang="cs" lang="cs" title="Algoritmus k-nejbli\xc5\xbe\xc5\xa1\xc3\xadch soused\xc5\xaf \xe2\x80\x93 Czech">\xc4\x8ce\xc5\xa1tina</a></li><li class="interlanguage-link interwiki-da"><a class="interlanguage-link-target" href="https://da.wikipedia.org/wiki/K-n%C3%A6rmeste_naboer" hreflang="da" lang="da" title="K-n\xc3\xa6rmeste naboer \xe2\x80\x93 Danish">Dansk</a></li><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/N%C3%A4chste-Nachbarn-Klassifikation" hreflang="de" lang="de" title="N\xc3\xa4chste-Nachbarn-Klassifikation \xe2\x80\x93 German">Deutsch</a></li><li class="interlanguage-link interwiki-es"><a class="interlanguage-link-target" href="https://es.wikipedia.org/wiki/K_vecinos_m%C3%A1s_pr%C3%B3ximos" hreflang="es" lang="es" title="K vecinos m\xc3\xa1s pr\xc3\xb3ximos \xe2\x80\x93 Spanish">Espa\xc3\xb1ol</a></li><li class="interlanguage-link interwiki-eu"><a class="interlanguage-link-target" href="https://eu.wikipedia.org/wiki/K_auzokide_hurbilenak" hreflang="eu" lang="eu" title="K auzokide hurbilenak \xe2\x80\x93 Basque">Euskara</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85_%DA%A9%DB%8C-%D9%86%D8%B2%D8%AF%DB%8C%DA%A9%E2%80%8C%D8%AA%D8%B1%DB%8C%D9%86_%D9%87%D9%85%D8%B3%D8%A7%DB%8C%D9%87" hreflang="fa" lang="fa" title="\xd8\xa7\xd9\x84\xda\xaf\xd9\x88\xd8\xb1\xdb\x8c\xd8\xaa\xd9\x85 \xda\xa9\xdb\x8c-\xd9\x86\xd8\xb2\xd8\xaf\xdb\x8c\xda\xa9\xe2\x80\x8c\xd8\xaa\xd8\xb1\xdb\x8c\xd9\x86 \xd9\x87\xd9\x85\xd8\xb3\xd8\xa7\xdb\x8c\xd9\x87 \xe2\x80\x93 Persian">\xd9\x81\xd8\xa7\xd8\xb1\xd8\xb3\xdb\x8c</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/M%C3%A9thode_des_k_plus_proches_voisins" hreflang="fr" lang="fr" title="M\xc3\xa9thode des k plus proches voisins \xe2\x80\x93 French">Fran\xc3\xa7ais</a></li><li class="interlanguage-link interwiki-ko"><a class="interlanguage-link-target" href="https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98" hreflang="ko" lang="ko" title="K-\xec\xb5\x9c\xea\xb7\xbc\xec\xa0\x91 \xec\x9d\xb4\xec\x9b\x83 \xec\x95\x8c\xea\xb3\xa0\xeb\xa6\xac\xec\xa6\x98 \xe2\x80\x93 Korean">\xed\x95\x9c\xea\xb5\xad\xec\x96\xb4</a></li><li class="interlanguage-link interwiki-id"><a class="interlanguage-link-target" href="https://id.wikipedia.org/wiki/KNN" hreflang="id" lang="id" title="KNN \xe2\x80\x93 Indonesian">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it"><a class="interlanguage-link-target" href="https://it.wikipedia.org/wiki/K-nearest_neighbors" hreflang="it" lang="it" title="K-nearest neighbors \xe2\x80\x93 Italian">Italiano</a></li><li class="interlanguage-link interwiki-he"><a class="interlanguage-link-target" href="https://he.wikipedia.org/wiki/%D7%90%D7%9C%D7%92%D7%95%D7%A8%D7%99%D7%AA%D7%9D_%D7%A9%D7%9B%D7%9F_%D7%A7%D7%A8%D7%95%D7%91" hreflang="he" lang="he" title="\xd7\x90\xd7\x9c\xd7\x92\xd7\x95\xd7\xa8\xd7\x99\xd7\xaa\xd7\x9d \xd7\xa9\xd7\x9b\xd7\x9f \xd7\xa7\xd7\xa8\xd7\x95\xd7\x91 \xe2\x80\x93 Hebrew">\xd7\xa2\xd7\x91\xd7\xa8\xd7\x99\xd7\xaa</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/K%E8%BF%91%E5%82%8D%E6%B3%95" hreflang="ja" lang="ja" title="K\xe8\xbf\x91\xe5\x82\x8d\xe6\xb3\x95 \xe2\x80\x93 Japanese">\xe6\x97\xa5\xe6\x9c\xac\xe8\xaa\x9e</a></li><li class="interlanguage-link interwiki-no"><a class="interlanguage-link-target" href="https://no.wikipedia.org/wiki/K-NN" hreflang="nb" lang="nb" title="K-NN \xe2\x80\x93 Norwegian Bokm\xc3\xa5l">Norsk bokm\xc3\xa5l</a></li><li class="interlanguage-link interwiki-pl"><a class="interlanguage-link-target" href="https://pl.wikipedia.org/wiki/K_najbli%C5%BCszych_s%C4%85siad%C3%B3w" hreflang="pl" lang="pl" title="K najbli\xc5\xbcszych s\xc4\x85siad\xc3\xb3w \xe2\x80\x93 Polish">Polski</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9" hreflang="ru" lang="ru" title="\xd0\x9c\xd0\xb5\xd1\x82\xd0\xbe\xd0\xb4 k-\xd0\xb1\xd0\xbb\xd0\xb8\xd0\xb6\xd0\xb0\xd0\xb9\xd1\x88\xd0\xb8\xd1\x85 \xd1\x81\xd0\xbe\xd1\x81\xd0\xb5\xd0\xb4\xd0\xb5\xd0\xb9 \xe2\x80\x93 Russian">\xd0\xa0\xd1\x83\xd1\x81\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9</a></li><li class="interlanguage-link interwiki-ckb"><a class="interlanguage-link-target" href="https://ckb.wikipedia.org/wiki/%DA%A9%DB%95%DB%8C_%D9%86%D8%B2%DB%8C%DA%A9%D8%AA%D8%B1%DB%8C%D9%86_%DA%BE%D8%A7%D9%88%D8%B3%DB%8E%DA%A9%D8%A7%D9%86" hreflang="ckb" lang="ckb" title="\xda\xa9\xdb\x95\xdb\x8c \xd9\x86\xd8\xb2\xdb\x8c\xda\xa9\xd8\xaa\xd8\xb1\xdb\x8c\xd9\x86 \xda\xbe\xd8\xa7\xd9\x88\xd8\xb3\xdb\x8e\xda\xa9\xd8\xa7\xd9\x86 \xe2\x80\x93 Central Kurdish">\xda\xa9\xd9\x88\xd8\xb1\xd8\xaf\xdb\x8c</a></li><li class="interlanguage-link interwiki-sr"><a class="interlanguage-link-target" href="https://sr.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%B0%D0%BC_%D0%BA_%D0%BD%D0%B0%D1%98%D0%B1%D0%BB%D0%B8%D0%B6%D0%B8%D1%85_%D1%81%D1%83%D1%81%D0%B5%D0%B4%D0%B0" hreflang="sr" lang="sr" title="\xd0\x90\xd0\xbb\xd0\xb3\xd0\xbe\xd1\x80\xd0\xb8\xd1\x82\xd0\xb0\xd0\xbc \xd0\xba \xd0\xbd\xd0\xb0\xd1\x98\xd0\xb1\xd0\xbb\xd0\xb8\xd0\xb6\xd0\xb8\xd1\x85 \xd1\x81\xd1\x83\xd1\x81\xd0\xb5\xd0\xb4\xd0\xb0 \xe2\x80\x93 Serbian">\xd0\xa1\xd1\x80\xd0\xbf\xd1\x81\xd0\xba\xd0\xb8 / srpski</a></li><li class="interlanguage-link interwiki-th"><a class="interlanguage-link-target" href="https://th.wikipedia.org/wiki/%E0%B8%82%E0%B8%B1%E0%B9%89%E0%B8%99%E0%B8%95%E0%B8%AD%E0%B8%99%E0%B8%A7%E0%B8%B4%E0%B8%98%E0%B8%B5%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%84%E0%B9%89%E0%B8%99%E0%B8%AB%E0%B8%B2%E0%B9%80%E0%B8%9E%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%99%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%99%E0%B9%83%E0%B8%81%E0%B8%A5%E0%B9%89%E0%B8%AA%E0%B8%B8%E0%B8%94_k_%E0%B8%95%E0%B8%B1%E0%B8%A7" hreflang="th" lang="th" title="\xe0\xb8\x82\xe0\xb8\xb1\xe0\xb9\x89\xe0\xb8\x99\xe0\xb8\x95\xe0\xb8\xad\xe0\xb8\x99\xe0\xb8\xa7\xe0\xb8\xb4\xe0\xb8\x98\xe0\xb8\xb5\xe0\xb8\x81\xe0\xb8\xb2\xe0\xb8\xa3\xe0\xb8\x84\xe0\xb9\x89\xe0\xb8\x99\xe0\xb8\xab\xe0\xb8\xb2\xe0\xb9\x80\xe0\xb8\x9e\xe0\xb8\xb7\xe0\xb9\x88\xe0\xb8\xad\xe0\xb8\x99\xe0\xb8\x9a\xe0\xb9\x89\xe0\xb8\xb2\xe0\xb8\x99\xe0\xb9\x83\xe0\xb8\x81\xe0\xb8\xa5\xe0\xb9\x89\xe0\xb8\xaa\xe0\xb8\xb8\xe0\xb8\x94 k \xe0\xb8\x95\xe0\xb8\xb1\xe0\xb8\xa7 \xe2\x80\x93 Thai">\xe0\xb9\x84\xe0\xb8\x97\xe0\xb8\xa2</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%BD%D0%B0%D0%B9%D0%B1%D0%BB%D0%B8%D0%B6%D1%87%D0%B8%D1%85_%D1%81%D1%83%D1%81%D1%96%D0%B4%D1%96%D0%B2" hreflang="uk" lang="uk" title="\xd0\x9c\xd0\xb5\xd1\x82\xd0\xbe\xd0\xb4 k-\xd0\xbd\xd0\xb0\xd0\xb9\xd0\xb1\xd0\xbb\xd0\xb8\xd0\xb6\xd1\x87\xd0\xb8\xd1\x85 \xd1\x81\xd1\x83\xd1\x81\xd1\x96\xd0\xb4\xd1\x96\xd0\xb2 \xe2\x80\x93 Ukrainian">\xd0\xa3\xd0\xba\xd1\x80\xd0\xb0\xd1\x97\xd0\xbd\xd1\x81\xd1\x8c\xd0\xba\xd0\xb0</a></li><li class="interlanguage-link interwiki-vi"><a class="interlanguage-link-target" href="https://vi.wikipedia.org/wiki/Gi%E1%BA%A3i_thu%E1%BA%ADt_k_h%C3%A0ng_x%C3%B3m_g%E1%BA%A7n_nh%E1%BA%A5t" hreflang="vi" lang="vi" title="Gi\xe1\xba\xa3i thu\xe1\xba\xadt k h\xc3\xa0ng x\xc3\xb3m g\xe1\xba\xa7n nh\xe1\xba\xa5t \xe2\x80\x93 Vietnamese">Ti\xe1\xba\xbfng Vi\xe1\xbb\x87t</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" hreflang="zh" lang="zh" title="K-\xe8\xbf\x91\xe9\x82\xbb\xe7\xae\x97\xe6\xb3\x95 \xe2\x80\x93 Chinese">\xe4\xb8\xad\xe6\x96\x87</a></li></ul>\n<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q1071612#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class="mw-footer" id="footer" role="contentinfo">\n<ul id="footer-info">\n<li id="footer-info-lastmod"> This page was last edited on 11 October 2020, at 07:59<span class="anonymous-show">\xc2\xa0(UTC)</span>.</li>\n<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia\xc2\xae is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id="footer-places">\n<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>\n<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>\n<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>\n<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>\n<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>\n<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>\n<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>\n<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>\n</ul>\n<ul class="noprint" id="footer-icons">\n<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>\n<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>\n</ul>\n<div style="clear: both;"></div>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.604","walltime":"0.842","ppvisitednodes":{"value":2950,"limit":1000000},"postexpandincludesize":{"value":92237,"limit":2097152},"templateargumentsize":{"value":8073,"limit":2097152},"expansiondepth":{"value":16,"limit":40},"expensivefunctioncount":{"value":6,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":84944,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  632.973      1 -total"," 50.80%  321.533      1 Template:Reflist"," 32.66%  206.702     17 Template:Cite_journal"," 16.47%  104.259      3 Template:Citation_needed"," 15.62%   98.848      5 Template:Fix"," 10.23%   64.765      8 Template:Category_handler","  9.29%   58.788      1 Template:ISBN","  7.99%   50.574      1 Template:Machine_learning_bar","  7.39%   46.792      1 Template:Sidebar_with_collapsible_lists","  4.78%   30.269      1 Template:Distinguish"]},"scribunto":{"limitreport-timeusage":{"value":"0.268","limit":"10.000"},"limitreport-memusage":{"value":6357603,"limit":52428800}},"cachereport":{"origin":"mw2269","timestamp":"20201013113015","ttl":2592000,"transientcontent":false}}});});</script>\n<script type="application/ld+json">{"@context":"https:\\/\\/schema.org","@type":"Article","name":"K-nearest neighbors algorithm","url":"https:\\/\\/en.wikipedia.org\\/wiki\\/K-nearest_neighbors_algorithm","sameAs":"http:\\/\\/www.wikidata.org\\/entity\\/Q1071612","mainEntity":"http:\\/\\/www.wikidata.org\\/entity\\/Q1071612","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png"}},"datePublished":"2005-04-21T15:50:19Z","dateModified":"2020-10-11T07:59:20Z","headline":"algorithm"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":164,"wgHostname":"mw1354"});});</script>\n</body></html>'