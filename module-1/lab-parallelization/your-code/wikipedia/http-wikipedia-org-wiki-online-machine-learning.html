b'<!DOCTYPE html>\n\n<html class="client-nojs" dir="ltr" lang="en">\n<head>\n<meta charset="utf8"/>\n<title>Online machine learning - Wikipedia</title>\n<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"f64125a0-5f72-402d-8fff-6d070cadea6f","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Online_machine_learning","wgTitle":"Online machine learning","wgCurRevisionId":980724312,"wgRevisionId":980724312,"wgArticleId":19892153,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","Short description matches Wikidata","Wikipedia articles needing clarification from September 2019","All articles with unsourced statements","Articles with unsourced statements from September 2019","Machine learning algorithms"],\n"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Online_machine_learning","wgRelevantArticleId":19892153,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q7094097"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options"\n:"loading","ext.math.styles":"ready","ext.cite.styles":"ready","skins.vector.styles.legacy":"ready","mediawiki.toc.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.math.scripts","ext.cite.ux-enhancements","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\\\","watchToken":"+\\\\","csrfToken":"+\\\\"});\n});});</script>\n<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>\n<meta content="" name="ResourceLoaderDynamicStyles"/>\n<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<meta content="MediaWiki 1.36.0-wmf.14" name="generator"/>\n<meta content="origin" name="referrer"/>\n<meta content="origin-when-crossorigin" name="referrer"/>\n<meta content="origin-when-cross-origin" name="referrer"/>\n<link href="//en.m.wikipedia.org/wiki/Online_machine_learning" media="only screen and (max-width: 720px)" rel="alternate"/>\n<link href="/w/index.php?title=Online_machine_learning&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>\n<link href="/w/index.php?title=Online_machine_learning&amp;action=edit" rel="edit" title="Edit this page"/>\n<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>\n<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>\n<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>\n<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>\n<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>\n<link href="https://en.wikipedia.org/wiki/Online_machine_learning" rel="canonical"/>\n<link href="//login.wikimedia.org" rel="dns-prefetch"/>\n<link href="//meta.wikimedia.org" rel="dns-prefetch"/>\n</head>\n<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Online_machine_learning rootpage-Online_machine_learning skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>\n<div class="noprint" id="mw-head-base"></div>\n<div class="mw-body" id="content" role="main">\n<a id="top"></a>\n<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>\n<div class="mw-indicators mw-body-content">\n</div>\n<h1 class="firstHeading" id="firstHeading" lang="en">Online machine learning</h1>\n<div class="mw-body-content" id="bodyContent">\n<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>\n<div id="contentSub"></div>\n<div id="contentSub2"></div>\n<div id="jump-to-nav"></div>\n<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>\n<a class="mw-jump-link" href="#searchInput">Jump to search</a>\n<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Method of machine learning</div>\n<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em">Part of a series on</td></tr><tr><th style="padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br/>and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>\n<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>\n<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>\n<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>\n<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>\n<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>\n<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>\n<li><a class="mw-selflink selflink">Online learning</a></li>\n<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>\n<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>\n<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>\n<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>\xc2\xa0\xe2\x80\xa2 <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>\n<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>\n<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>\n<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>\n<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>\n<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>\n<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>\n<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>\n<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>\n<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>\n<li><a class="mw-redirect" href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>\n<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>\n<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>\n<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>\n<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation\xe2\x80\x93maximization algorithm">Expectation\xe2\x80\x93maximization (EM)</a></li>\n<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>\n<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>\n<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>\n<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>\n<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>\n<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>\n<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>\n<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>\n<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>\n<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>\n<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>\n<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>\n<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>\n<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>\n<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>\n<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>\n<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>\n<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>\n<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>\n<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li></ul></li>\n<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>\n<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>\n<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>\n<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>\n<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>\n<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias\xe2\x80\x93variance dilemma">Bias\xe2\x80\x93variance dilemma</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>\n<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>\n<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>\n<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>\n<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>\n<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik\xe2\x80\x93Chervonenkis theory">VC theory</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>\n<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>\n<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>\n<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>\n<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>\n<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p>In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b>online machine learning</b> is a method of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of <a class="mw-redirect" href="/wiki/Out-of-core" title="Out-of-core">out-of-core</a> algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., <a href="/wiki/Stock_market_prediction" title="Stock market prediction">stock price prediction</a>.\nOnline learning algorithms may be prone to <a href="/wiki/Catastrophic_interference" title="Catastrophic interference">catastrophic interference</a>, a problem that can be addressed by <a href="/wiki/Incremental_learning" title="Incremental learning">incremental learning</a> approaches.\n</p>\n<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>\n<ul>\n<li class="toclevel-1 tocsection-1"><a href="#Introduction"><span class="tocnumber">1</span> <span class="toctext">Introduction</span></a></li>\n<li class="toclevel-1 tocsection-2"><a href="#Statistical_view_of_online_learning"><span class="tocnumber">2</span> <span class="toctext">Statistical view of online learning</span></a>\n<ul>\n<li class="toclevel-2 tocsection-3"><a href="#Example:_linear_least_squares"><span class="tocnumber">2.1</span> <span class="toctext">Example: linear least squares</span></a></li>\n<li class="toclevel-2 tocsection-4"><a href="#Batch_learning"><span class="tocnumber">2.2</span> <span class="toctext">Batch learning</span></a></li>\n<li class="toclevel-2 tocsection-5"><a href="#Online_learning:_recursive_least_squares"><span class="tocnumber">2.3</span> <span class="toctext">Online learning: recursive least squares</span></a></li>\n<li class="toclevel-2 tocsection-6"><a href="#Stochastic_gradient_descent"><span class="tocnumber">2.4</span> <span class="toctext">Stochastic gradient descent</span></a></li>\n<li class="toclevel-2 tocsection-7"><a href="#Incremental_stochastic_gradient_descent"><span class="tocnumber">2.5</span> <span class="toctext">Incremental stochastic gradient descent</span></a></li>\n<li class="toclevel-2 tocsection-8"><a href="#Kernel_methods"><span class="tocnumber">2.6</span> <span class="toctext">Kernel methods</span></a></li>\n<li class="toclevel-2 tocsection-9"><a href="#Online_convex_optimization"><span class="tocnumber">2.7</span> <span class="toctext">Online convex optimization</span></a>\n<ul>\n<li class="toclevel-3 tocsection-10"><a href="#Follow_the_leader_(FTL)"><span class="tocnumber">2.7.1</span> <span class="toctext">Follow the leader (FTL)</span></a></li>\n<li class="toclevel-3 tocsection-11"><a href="#Follow_the_regularised_leader_(FTRL)"><span class="tocnumber">2.7.2</span> <span class="toctext">Follow the regularised leader (FTRL)</span></a></li>\n</ul>\n</li>\n<li class="toclevel-2 tocsection-12"><a href="#Online_subgradient_descent_(OSD)"><span class="tocnumber">2.8</span> <span class="toctext">Online subgradient descent (OSD)</span></a></li>\n<li class="toclevel-2 tocsection-13"><a href="#Other_algorithms"><span class="tocnumber">2.9</span> <span class="toctext">Other algorithms</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-14"><a href="#Interpretations_of_online_learning"><span class="tocnumber">3</span> <span class="toctext">Interpretations of online learning</span></a></li>\n<li class="toclevel-1 tocsection-15"><a href="#Implementations"><span class="tocnumber">4</span> <span class="toctext">Implementations</span></a></li>\n<li class="toclevel-1 tocsection-16"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>\n<li class="toclevel-1 tocsection-17"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>\n<li class="toclevel-1 tocsection-18"><a href="#External_links"><span class="tocnumber">7</span> <span class="toctext">External links</span></a></li>\n</ul>\n</div>\n<h2><span class="mw-headline" id="Introduction">Introduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=1" title="Edit section: Introduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>In the setting of <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, a function of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f:X\\to Y}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo>:</mo>\n<mi>X</mi>\n<mo stretchy="false">\xe2\x86\x92<!-- \xe2\x86\x92 --></mo>\n<mi>Y</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f:X\\to Y}</annotation>\n</semantics>\n</math></span><img alt="f:X\\to Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/abd1e080abef4bbdab67b43819c6431e7561361c" style="vertical-align: -0.671ex; width:10.583ex; height:2.509ex;"/></span> is to be learned, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>X</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle X}</annotation>\n</semantics>\n</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span> is thought of as a space of inputs and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Y}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Y</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Y}</annotation>\n</semantics>\n</math></span><img alt="Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;"/></span> as a space of outputs, that predicts well on instances that are drawn from a <a href="/wiki/Joint_probability_distribution" title="Joint probability distribution">joint probability distribution</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle p(x,y)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>p</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle p(x,y)}</annotation>\n</semantics>\n</math></span><img alt="p(x,y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/089e91a1824e14cebc8e8d04dc652c61b3008e0a" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:6.587ex; height:2.843ex;"/></span> on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle X\\times Y}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>X</mi>\n<mo>\xc3\x97<!-- \xc3\x97 --></mo>\n<mi>Y</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle X\\times Y}</annotation>\n</semantics>\n</math></span><img alt="X\\times Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1613c1ff4b6fbfb6c80a8da83e90ad28f0ab3483" style="vertical-align: -0.338ex; width:6.594ex; height:2.176ex;"/></span>. In reality, the learner never knows the true distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle p(x,y)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>p</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle p(x,y)}</annotation>\n</semantics>\n</math></span><img alt="p(x,y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/089e91a1824e14cebc8e8d04dc652c61b3008e0a" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:6.587ex; height:2.843ex;"/></span> over instances. Instead, the learner usually has access to a training set of examples <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (x_{1},y_{1}),\\ldots ,(x_{n},y_{n})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (x_{1},y_{1}),\\ldots ,(x_{n},y_{n})}</annotation>\n</semantics>\n</math></span><img alt="(x_{1},y_{1}),\\ldots ,(x_{n},y_{n})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b3459c38f5205bf3b65166ca15cdde5574997413" style="vertical-align: -0.838ex; width:20.348ex; height:2.843ex;"/></span>. In this setting, the <a href="/wiki/Loss_function" title="Loss function">loss function</a> is given as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle V:Y\\times Y\\to \\mathbb {R} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>V</mi>\n<mo>:</mo>\n<mi>Y</mi>\n<mo>\xc3\x97<!-- \xc3\x97 --></mo>\n<mi>Y</mi>\n<mo stretchy="false">\xe2\x86\x92<!-- \xe2\x86\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle V:Y\\times Y\\to \\mathbb {R} }</annotation>\n</semantics>\n</math></span><img alt="V:Y\\times Y\\to \\mathbb {R} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/939f48aa832d9e4bf64c9e65199d8c56098c4caf" style="vertical-align: -0.338ex; width:15.404ex; height:2.176ex;"/></span>, such that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle V(f(x),y)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle V(f(x),y)}</annotation>\n</semantics>\n</math></span><img alt="V(f(x),y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f381ec03a204bc068d396f8778f61925459fc01" style="vertical-align: -0.838ex; width:10.204ex; height:2.843ex;"/></span> measures the difference between the predicted value <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f(x)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f(x)}</annotation>\n</semantics>\n</math></span><img alt="f(x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/202945cce41ecebb6f643f31d119c514bec7a074" style="vertical-align: -0.838ex; width:4.418ex; height:2.843ex;"/></span> and the true value <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle y}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>y</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle y}</annotation>\n</semantics>\n</math></span><img alt="y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;"/></span>. The ideal goal is to select a function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f\\in {\\mathcal {H}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">H</mi>\n</mrow>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f\\in {\\mathcal {H}}}</annotation>\n</semantics>\n</math></span><img alt="f\\in {\\mathcal {H}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2634be9ba2369b9d92861b6d574a471eae58ff81" style="vertical-align: -0.671ex; width:6.083ex; height:2.509ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\mathcal {H}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi class="MJX-tex-caligraphic" mathvariant="script">H</mi>\n</mrow>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {H}}}</annotation>\n</semantics>\n</math></span><img alt="{\\mathcal {H}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/19ef4c7b923a5125ac91aa491838a95ee15b804f" style="vertical-align: -0.338ex; width:1.964ex; height:2.176ex;"/></span> is a space of functions called a hypothesis space, so that some notion of total loss is minimised. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.\n</p>\n<h2><span class="mw-headline" id="Statistical_view_of_online_learning">Statistical view of online learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=2" title="Edit section: Statistical view of online learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>In statistical learning models, the training sample <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (x_{i},y_{i})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (x_{i},y_{i})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle (x_{i},y_{i})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d6dbb919b91ccacf17ed47898048428a1baf9703" style="vertical-align: -0.838ex; width:6.912ex; height:2.843ex;"/></span> are assumed to have been drawn from the true distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle p(x,y)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>p</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle p(x,y)}</annotation>\n</semantics>\n</math></span><img alt="p(x,y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/089e91a1824e14cebc8e8d04dc652c61b3008e0a" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:6.587ex; height:2.843ex;"/></span> and the objective is to minimize the expected "risk"\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I[f]=\\mathbb {E} [V(f(x),y)]=\\int V(f(x),y)\\,dp(x,y)\\ .}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>I</mi>\n<mo stretchy="false">[</mo>\n<mi>f</mi>\n<mo stretchy="false">]</mo>\n<mo>=</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">E</mi>\n</mrow>\n<mo stretchy="false">[</mo>\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n<mo stretchy="false">]</mo>\n<mo>=</mo>\n<mo>\xe2\x88\xab<!-- \xe2\x88\xab --></mo>\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n<mspace width="thinmathspace"></mspace>\n<mi>d</mi>\n<mi>p</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n<mtext>\xc2\xa0</mtext>\n<mo>.</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I[f]=\\mathbb {E} [V(f(x),y)]=\\int V(f(x),y)\\,dp(x,y)\\ .}</annotation>\n</semantics>\n</math></span><img alt="I[f]=\\mathbb {E} [V(f(x),y)]=\\int V(f(x),y)\\,dp(x,y)\\ ." aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/237446394be1d6f2da9eec0e73de114a485687d7" style="vertical-align: -2.338ex; width:45.101ex; height:5.676ex;"/></span></dd></dl>\n<p>A common paradigm in this situation is to estimate a function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\hat {f}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mover>\n<mi>f</mi>\n<mo stretchy="false">^<!-- ^ --></mo>\n</mover>\n</mrow>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\hat {f}}}</annotation>\n</semantics>\n</math></span><img alt="{\\hat {f}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/14ce989fd75da938ec6f95a0cdb71037b23a11cb" style="vertical-align: -0.671ex; width:1.699ex; height:3.176ex;"/></span> through <a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">empirical risk minimization</a> or regularized empirical risk minimization (usually <a href="/wiki/Tikhonov_regularization" title="Tikhonov regularization">Tikhonov regularization</a>). The choice of loss function here gives rise to several well-known learning algorithms such as regularized <a href="/wiki/Least_squares" title="Least squares">least squares</a> and <a class="mw-redirect" href="/wiki/Support_vector_machines" title="Support vector machines">support vector machines</a>.\nA purely online model in this category would learn based on just the new input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (x_{t+1},y_{t+1})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (x_{t+1},y_{t+1})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle (x_{t+1},y_{t+1})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/deb5ac4033c4db1921ba4b12d1fdc701f0d75831" style="vertical-align: -0.838ex; width:11.165ex; height:2.843ex;"/></span>, the current best predictor <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle f_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/874c306411e808e8191e8aeb95e3440e1c68d6e9" style="vertical-align: -0.671ex; width:1.965ex; height:2.509ex;"/></span> and some extra stored information (which is usually expected to have storage requirements independent of training data size). For many formulations, for example nonlinear <a class="mw-redirect" href="/wiki/Kernel_methods" title="Kernel methods">kernel methods</a>, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f_{t+1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f_{t+1}}</annotation>\n</semantics>\n</math></span><img alt="f_{t+1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/393d15e53d6203ef7335a3ed55cfa5d418db4294" style="vertical-align: -0.671ex; width:4.066ex; height:2.509ex;"/></span> is permitted to depend on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f_{t}}</annotation>\n</semantics>\n</math></span><img alt="f_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/874c306411e808e8191e8aeb95e3440e1c68d6e9" style="vertical-align: -0.671ex; width:1.965ex; height:2.509ex;"/></span> and all previous data points <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (x_{1},y_{1}),\\ldots ,(x_{t},y_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (x_{1},y_{1}),\\ldots ,(x_{t},y_{t})}</annotation>\n</semantics>\n</math></span><img alt="(x_{1},y_{1}),\\ldots ,(x_{t},y_{t})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb94c7629caf4ee2868da17d639d32188feb3597" style="vertical-align: -0.838ex; width:19.563ex; height:2.843ex;"/></span>. In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.\n</p><p>A common strategy to overcome the above issues is to learn using mini-batches, which process a small batch of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle b\\geq 1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>b</mi>\n<mo>\xe2\x89\xa5<!-- \xe2\x89\xa5 --></mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle b\\geq 1}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle b\\geq 1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2ca25b8344866ed3caaddc966d0bcbfb003ade6c" style="vertical-align: -0.505ex; width:5.258ex; height:2.343ex;"/></span> data points at a time, this can be considered as pseudo-online learning for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle b}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>b</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle b}</annotation>\n</semantics>\n</math></span><img alt="b" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f11423fbb2e967f986e36804a8ae4271734917c3" style="vertical-align: -0.338ex; width:0.998ex; height:2.176ex;"/></span> much smaller than the total number of training points. Mini-batch techniques are used with repeated passing over the training data to obtain optimized out-of-core<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title=\'Define the expression "out-of-core" (September 2019)\'>clarification needed</span></a></i>]</sup> versions of machine learning algorithms, for example, <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">stochastic gradient descent</a>. When combined with <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>, this is currently the de facto training method for training <a class="mw-redirect" href="/wiki/Artificial_neural_networks" title="Artificial neural networks">artificial neural networks</a>.\n</p>\n<h3><span class="mw-headline" id="Example:_linear_least_squares">Example: linear least squares</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=3" title="Edit section: Example: linear least squares">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a class="mw-redirect" href="/wiki/Linear_least_squares_(mathematics)" title="Linear least squares (mathematics)">Linear least squares (mathematics)</a></div>\n<p>The simple example of linear least squares is used to explain a variety of ideas in online learning. The ideas are general enough to be applied to other settings, for example, with other convex loss functions.\n</p>\n<h3><span class="mw-headline" id="Batch_learning">Batch learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=4" title="Edit section: Batch learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Consider the setting of supervised learning with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f}</annotation>\n</semantics>\n</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span> being a linear function to be learned:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f(x_{j})=\\langle w,x_{j}\\rangle =w\\cdot x_{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<mi>w</mi>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>=</mo>\n<mi>w</mi>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f(x_{j})=\\langle w,x_{j}\\rangle =w\\cdot x_{j}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle f(x_{j})=\\langle w,x_{j}\\rangle =w\\cdot x_{j}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1e6dd69280d9eb6ffd771da65be5a2126c99a2ad" style="vertical-align: -1.005ex; width:23.854ex; height:3.009ex;"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x_{j}\\in \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x_{j}\\in \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle x_{j}\\in \\mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8e18fbfc87dc1f77e3578022c609e2918516c972" style="vertical-align: -1.005ex; width:7.85ex; height:3.343ex;"/></span> is a vector of inputs (data points) and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w\\in \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>w</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w\\in \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w\\in \\mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a4ebc4af2c511658b3e378e73b257436dc61e2d" style="vertical-align: -0.338ex; width:7.275ex; height:2.676ex;"/></span> is a linear filter vector.\nThe goal is to compute the filter vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>w</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w}</annotation>\n</semantics>\n</math></span><img alt="w" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/88b1e0c8e1be5ebe69d18a8010676fa42d7961e6" style="vertical-align: -0.338ex; width:1.664ex; height:1.676ex;"/></span>.\nTo this end, a square loss function \n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\\langle w,x_{j}\\rangle -y_{j})^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mo stretchy="false">(</mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo>=</mo>\n<mo stretchy="false">(</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<mi>w</mi>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\\langle w,x_{j}\\rangle -y_{j})^{2}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\\langle w,x_{j}\\rangle -y_{j})^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bcc76b1426014902f82424376228f16617d2f1f3" style="vertical-align: -1.005ex; width:45.783ex; height:3.343ex;"/></span></dd></dl>\n<p>is used to compute the vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>w</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w}</annotation>\n</semantics>\n</math></span><img alt="w" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/88b1e0c8e1be5ebe69d18a8010676fa42d7961e6" style="vertical-align: -0.338ex; width:1.664ex; height:1.676ex;"/></span> that minimizes the empirical loss\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I_{n}[w]=\\sum _{j=1}^{n}V(\\langle w,x_{j}\\rangle ,y_{j})=\\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>I</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">[</mo>\n<mi>w</mi>\n<mo stretchy="false">]</mo>\n<mo>=</mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</munderover>\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<mi>w</mi>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</munderover>\n<mo stretchy="false">(</mo>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n<mi>w</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I_{n}[w]=\\sum _{j=1}^{n}V(\\langle w,x_{j}\\rangle ,y_{j})=\\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle I_{n}[w]=\\sum _{j=1}^{n}V(\\langle w,x_{j}\\rangle ,y_{j})=\\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ae54835f75d260132fd1defd504d048c543f3675" style="vertical-align: -3.338ex; width:44.056ex; height:7.176ex;"/></span></dd></dl>\n<p>where\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle y_{j}\\in \\mathbb {R} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle y_{j}\\in \\mathbb {R} }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle y_{j}\\in \\mathbb {R} }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d426fb0c9e2292e5d7b476b7bcb78a1e6201ef40" style="vertical-align: -1.005ex; width:6.568ex; height:2.843ex;"/></span>.</dd></dl>\n<p>Let  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>X</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle X}</annotation>\n</semantics>\n</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span> be the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i\\times d}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n<mo>\xc3\x97<!-- \xc3\x97 --></mo>\n<mi>d</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i\\times d}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle i\\times d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/66fe523de06a09ecddf014e17f0eb5c2788c1fa8" style="vertical-align: -0.338ex; width:4.859ex; height:2.176ex;"/></span> data matrix and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle y\\in \\mathbb {R} ^{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>y</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle y\\in \\mathbb {R} ^{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle y\\in \\mathbb {R} ^{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/89d452dd1da1223b1b000784a7bd31921e12a19e" style="vertical-align: -0.671ex; width:6.474ex; height:3.009ex;"/></span> is the column vector of target values after the arrival of the first <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> data points.\nAssuming that the covariance matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Sigma _{i}=X^{T}X}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\xa3<!-- \xce\xa3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msup>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msup>\n<mi>X</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Sigma _{i}=X^{T}X}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\Sigma _{i}=X^{T}X}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a91886e7f2604876696945ac252f02eedd583f18" style="vertical-align: -0.671ex; width:10.942ex; height:3.009ex;"/></span> is invertible (otherwise it is preferential to proceed in a similar fashion with Tikhonov regularization), the best solution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f^{*}(x)=\\langle w^{*},x\\rangle }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<msup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mo>,</mo>\n<mi>x</mi>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f^{*}(x)=\\langle w^{*},x\\rangle }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle f^{*}(x)=\\langle w^{*},x\\rangle }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf939613e9bcbee271dae3e0472b9acd150bd637" style="vertical-align: -0.838ex; width:15.503ex; height:2.843ex;"/></span> to the linear least squares problem is given by\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w^{*}=(X^{T}X)^{-1}X^{T}y=\\Sigma _{i}^{-1}\\sum _{j=1}^{i}x_{j}y_{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mo>=</mo>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msup>\n<mi>X</mi>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msup>\n<msup>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msup>\n<mi>y</mi>\n<mo>=</mo>\n<msubsup>\n<mi mathvariant="normal">\xce\xa3<!-- \xce\xa3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msubsup>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</munderover>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w^{*}=(X^{T}X)^{-1}X^{T}y=\\Sigma _{i}^{-1}\\sum _{j=1}^{i}x_{j}y_{j}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w^{*}=(X^{T}X)^{-1}X^{T}y=\\Sigma _{i}^{-1}\\sum _{j=1}^{i}x_{j}y_{j}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3f01052c9508f15a88ffa127c7951a260bd47961" style="vertical-align: -3.338ex; width:35.393ex; height:7.509ex;"/></span>.</dd></dl>\n<p>Now, calculating the covariance matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Sigma _{i}=\\sum _{j=1}^{i}x_{j}x_{j}^{T}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\xa3<!-- \xce\xa3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</munderover>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Sigma _{i}=\\sum _{j=1}^{i}x_{j}x_{j}^{T}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\Sigma _{i}=\\sum _{j=1}^{i}x_{j}x_{j}^{T}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e8cbf8addae70f77ce8a936879c9b6ca988a979b" style="vertical-align: -3.338ex; width:14.277ex; height:7.509ex;"/></span> takes time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(id^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>i</mi>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(id^{2})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(id^{2})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5ea2c75ae97c0821ace838351d558b113709eb85" style="vertical-align: -0.838ex; width:6.657ex; height:3.176ex;"/></span>, inverting the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle d\\times d}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>d</mi>\n<mo>\xc3\x97<!-- \xc3\x97 --></mo>\n<mi>d</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle d\\times d}</annotation>\n</semantics>\n</math></span><img alt="d \\times d" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8832c0aa14719499bb50f640be353abb7f37f069" style="vertical-align: -0.338ex; width:5.272ex; height:2.176ex;"/></span> matrix takes time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(d^{3})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>3</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(d^{3})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(d^{3})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d688ed8415cdd3e1f53b706d3da73f69c0ea0144" style="vertical-align: -0.838ex; width:5.855ex; height:3.176ex;"/></span>, while the rest of the multiplication takes time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(d^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(d^{2})}</annotation>\n</semantics>\n</math></span><img alt="O(d^{2})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b184eaa03aff77b01bc4c0681501df7505dd4292" style="vertical-align: -0.838ex; width:5.855ex; height:3.176ex;"/></span>, giving a total time of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(id^{2}+d^{3})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>i</mi>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo>+</mo>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>3</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(id^{2}+d^{3})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(id^{2}+d^{3})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5dfe9793032810d52d4facb100516321f2dfcbb5" style="vertical-align: -0.838ex; width:11.77ex; height:3.176ex;"/></span>. When there are <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle n}</annotation>\n</semantics>\n</math></span><img alt="n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span> total points in the dataset, to recompute the solution after the arrival of every datapoint <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i=1,\\ldots ,n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i=1,\\ldots ,n}</annotation>\n</semantics>\n</math></span><img alt="i=1,\\ldots ,n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a5726d00b79af1b4666a6319c45381579dc85a9a" style="vertical-align: -0.671ex; width:11.636ex; height:2.509ex;"/></span>, the naive approach will have a total complexity <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(n^{2}d^{2}+nd^{3})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>n</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo>+</mo>\n<mi>n</mi>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>3</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(n^{2}d^{2}+nd^{3})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(n^{2}d^{2}+nd^{3})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e1b728ed308010eca3bd02442315bafda0518af8" style="vertical-align: -0.838ex; width:14.811ex; height:3.176ex;"/></span>. Note that when storing the matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Sigma _{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\xa3<!-- \xce\xa3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Sigma _{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\Sigma _{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7311e26a182cf249a67308c6ceb4d12d2c6896b8" style="vertical-align: -0.671ex; width:2.478ex; height:2.509ex;"/></span>, then updating it at each step needs only adding <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x_{i+1}x_{i+1}^{T}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x_{i+1}x_{i+1}^{T}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle x_{i+1}x_{i+1}^{T}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a7940740f51886d2754696ce5daeff37b985d47a" style="vertical-align: -1.171ex; width:8.46ex; height:3.343ex;"/></span>, which takes <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(d^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(d^{2})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(d^{2})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b184eaa03aff77b01bc4c0681501df7505dd4292" style="vertical-align: -0.838ex; width:5.855ex; height:3.176ex;"/></span> time, reducing the total time to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>n</mi>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo>+</mo>\n<mi>n</mi>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>3</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>n</mi>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>3</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7826cbfd4595a6c5b92b5dbd47f70ae6eeb7d10d" style="vertical-align: -0.838ex; width:24.105ex; height:3.176ex;"/></span>, but with an additional storage space of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(d^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(d^{2})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(d^{2})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b184eaa03aff77b01bc4c0681501df7505dd4292" style="vertical-align: -0.838ex; width:5.855ex; height:3.176ex;"/></span> to store <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Sigma _{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\xa3<!-- \xce\xa3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Sigma _{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\Sigma _{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7311e26a182cf249a67308c6ceb4d12d2c6896b8" style="vertical-align: -0.671ex; width:2.478ex; height:2.509ex;"/></span>.<sup class="reference" id="cite_ref-lorenzo_1-0"><a href="#cite_note-lorenzo-1">[1]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Online_learning:_recursive_least_squares">Online learning: recursive least squares</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=5" title="Edit section: Online learning: recursive least squares">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>The recursive least squares (RLS) algorithm considers an online approach to the least squares problem. It can be shown that by initialising <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle w_{0}=0\\in \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>0</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>0</mn>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle w_{0}=0\\in \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle w_{0}=0\\in \\mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/705559eea79df0dda6c0b92aec5fb6b470656364" style="vertical-align: -0.671ex; width:12.59ex; height:3.009ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle \\Gamma _{0}=I\\in \\mathbb {R} ^{d\\times d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>0</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<mi>I</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n<mo>\xc3\x97<!-- \xc3\x97 --></mo>\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle \\Gamma _{0}=I\\in \\mathbb {R} ^{d\\times d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle \\Gamma _{0}=I\\in \\mathbb {R} ^{d\\times d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0204688743496375136edeb9e9ffaf1b1af5c199" style="vertical-align: -0.671ex; width:14.526ex; height:3.009ex;"/></span>, the solution of the linear least squares problem given in the previous section can be computed by the following iteration:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Gamma _{i}=\\Gamma _{i-1}-{\\frac {\\Gamma _{i-1}x_{i}x_{i}^{T}\\Gamma _{i-1}}{1+x_{i}^{T}\\Gamma _{i-1}x_{i}}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mrow>\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n</mrow>\n<mrow>\n<mn>1</mn>\n<mo>+</mo>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mrow>\n</mfrac>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Gamma _{i}=\\Gamma _{i-1}-{\\frac {\\Gamma _{i-1}x_{i}x_{i}^{T}\\Gamma _{i-1}}{1+x_{i}^{T}\\Gamma _{i-1}x_{i}}}}</annotation>\n</semantics>\n</math></span><img alt="\\Gamma _{i}=\\Gamma _{i-1}-{\\frac {\\Gamma _{i-1}x_{i}x_{i}^{T}\\Gamma _{i-1}}{1+x_{i}^{T}\\Gamma _{i-1}x_{i}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a5596f56fef5974b5f2b02d2c794176fc1742f2b" style="vertical-align: -3.005ex; width:26.935ex; height:7.176ex;"/></span></dd>\n<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})}</annotation>\n</semantics>\n</math></span><img alt="w_{i}=w_{i-1}-\\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd6b1f8c5de9e49493c2099d46eedf36f02ac55f" style="vertical-align: -1.005ex; width:31.221ex; height:3.176ex;"/></span></dd></dl>\n<p>The above iteration algorithm can be proved using induction on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>.<sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> The proof also shows that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Gamma _{i}=\\Sigma _{i}^{-1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msubsup>\n<mi mathvariant="normal">\xce\xa3<!-- \xce\xa3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Gamma _{i}=\\Sigma _{i}^{-1}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\Gamma _{i}=\\Sigma _{i}^{-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7ff55e32e980ae75dfc31b4b0a5f248b297889f8" style="vertical-align: -1.005ex; width:9.362ex; height:3.343ex;"/></span>. \nOne can look at RLS also in the context of adaptive filters (see <a class="mw-redirect" href="/wiki/Recursive_least_squares" title="Recursive least squares">RLS</a>).\n</p><p>The complexity for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle n}</annotation>\n</semantics>\n</math></span><img alt="n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span> steps of this algorithm is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(nd^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>n</mi>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(nd^{2})}</annotation>\n</semantics>\n</math></span><img alt="O(nd^{2})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6ad3dd0e39bb135e61123a5551b34e8ea33cab1e" style="vertical-align: -0.838ex; width:7.25ex; height:3.176ex;"/></span>, which is an order of magnitude faster than the corresponding batch learning complexity. The storage requirements at every step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> here are to store the matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Gamma _{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Gamma _{i}}</annotation>\n</semantics>\n</math></span><img alt="\\Gamma _{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4497c779a32aa954cc5706fb7f4c2abbe1176dec" style="vertical-align: -0.671ex; width:2.252ex; height:2.509ex;"/></span>, which is constant at <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(d^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(d^{2})}</annotation>\n</semantics>\n</math></span><img alt="O(d^{2})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b184eaa03aff77b01bc4c0681501df7505dd4292" style="vertical-align: -0.838ex; width:5.855ex; height:3.176ex;"/></span>. For the case when <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Sigma _{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\xa3<!-- \xce\xa3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Sigma _{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\Sigma _{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7311e26a182cf249a67308c6ceb4d12d2c6896b8" style="vertical-align: -0.671ex; width:2.478ex; height:2.509ex;"/></span> is not invertible, consider the regularised version of the problem \nloss function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}+\\lambda ||w||_{2}^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</munderover>\n<mo stretchy="false">(</mo>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n<mi>w</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo>+</mo>\n<mi>\xce\xbb<!-- \xce\xbb --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<msubsup>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}+\\lambda ||w||_{2}^{2}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}+\\lambda ||w||_{2}^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8694f77cc8edb0e02b9a40872ff76856cd894a5b" style="vertical-align: -3.338ex; width:24.992ex; height:7.176ex;"/></span>. Then, it\'s easy to show that the same algorithm works with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Gamma _{0}=(I+\\lambda I)^{-1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>0</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<mo stretchy="false">(</mo>\n<mi>I</mi>\n<mo>+</mo>\n<mi>\xce\xbb<!-- \xce\xbb --></mi>\n<mi>I</mi>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Gamma _{0}=(I+\\lambda I)^{-1}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\Gamma _{0}=(I+\\lambda I)^{-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d0439b876beea27010d8d030f1168466bf4fef11" style="vertical-align: -0.838ex; width:16.287ex; height:3.176ex;"/></span>, and the iterations proceed to give <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Gamma _{i}=(\\Sigma _{i}+\\lambda I)^{-1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi mathvariant="normal">\xce\xa3<!-- \xce\xa3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>+</mo>\n<mi>\xce\xbb<!-- \xce\xbb --></mi>\n<mi>I</mi>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Gamma _{i}=(\\Sigma _{i}+\\lambda I)^{-1}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\Gamma _{i}=(\\Sigma _{i}+\\lambda I)^{-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b125856ff14daab5f793c6121c952e2c64a146f9" style="vertical-align: -0.838ex; width:17.338ex; height:3.176ex;"/></span>.<sup class="reference" id="cite_ref-lorenzo_1-1"><a href="#cite_note-lorenzo-1">[1]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Stochastic_gradient_descent">Stochastic gradient descent</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=6" title="Edit section: Stochastic gradient descent">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">Stochastic gradient descent</a></div>\n<p>When this  \n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d1e0efb8ad8c42488286c2bca923945d88ffc77a" style="vertical-align: -1.005ex; width:31.221ex; height:3.176ex;"/></span></dd></dl>\n<p>is replaced by\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{i}\\rangle ,y_{i})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msubsup>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mi mathvariant="normal">\xe2\x88\x87<!-- \xe2\x88\x87 --></mi>\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{i}\\rangle ,y_{i})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{i}\\rangle ,y_{i})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e50627626441a4189fd9b0e17bc4abf4cf13064a" style="vertical-align: -1.005ex; width:61.522ex; height:3.176ex;"/></span></dd></dl>\n<p>or <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Gamma _{i}\\in \\mathbb {R} ^{d\\times d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi mathvariant="normal">\xce\x93<!-- \xce\x93 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n<mo>\xc3\x97<!-- \xc3\x97 --></mo>\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Gamma _{i}\\in \\mathbb {R} ^{d\\times d}}</annotation>\n</semantics>\n</math></span><img alt="\\Gamma _{i}\\in \\mathbb {R} ^{d\\times d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e2366c73e880e6b05c3f0bc25e208465c0ca9536" style="vertical-align: -0.671ex; width:10.001ex; height:3.009ex;"/></span> by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma _{i}\\in \\mathbb {R} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma _{i}\\in \\mathbb {R} }</annotation>\n</semantics>\n</math></span><img alt="\\gamma _{i}\\in \\mathbb {R} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd887ef9a6242fbad4174fb2ea5bd0dd34df8f18" style="vertical-align: -0.838ex; width:6.523ex; height:2.676ex;"/></span>, this becomes the stochastic gradient descent algorithm. In this case, the complexity for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle n}</annotation>\n</semantics>\n</math></span><img alt="n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span> steps of this algorithm reduces to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(nd)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>n</mi>\n<mi>d</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(nd)}</annotation>\n</semantics>\n</math></span><img alt="O(nd)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d627081a425c8784d1e2f4da283e073fd960e881" style="vertical-align: -0.838ex; width:6.193ex; height:2.843ex;"/></span>. The storage requirements at every step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> are constant at <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(d)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>d</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(d)}</annotation>\n</semantics>\n</math></span><img alt="O(d)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e323ee705f0664132bf796619cf0e2b36a1c396" style="vertical-align: -0.838ex; width:4.798ex; height:2.843ex;"/></span>.\n</p><p>However, the stepsize <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma _{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma _{i}}</annotation>\n</semantics>\n</math></span><img alt="\\gamma _{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/907e4a28946d45e5cbf3cf3c6c48de68296039c4" style="vertical-align: -0.838ex; width:2.004ex; height:2.176ex;"/></span> needs to be chosen carefully to solve the expected risk minimization problem, as detailed above. By choosing a decaying step size <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma _{i}\\approx {\\frac {1}{\\sqrt {i}}},}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>\xe2\x89\x88<!-- \xe2\x89\x88 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>1</mn>\n<msqrt>\n<mi>i</mi>\n</msqrt>\n</mfrac>\n</mrow>\n<mo>,</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma _{i}\\approx {\\frac {1}{\\sqrt {i}}},}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\gamma _{i}\\approx {\\frac {1}{\\sqrt {i}}},}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cefcfa8e97ed0c7352edacaeb4b05affe02bbb75" style="vertical-align: -2.838ex; width:9.324ex; height:6.176ex;"/></span> one can prove the convergence of the average iterate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\overline {w}}_{n}={\\frac {1}{n}}\\sum _{i=1}^{n}w_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mover>\n<mi>w</mi>\n<mo accent="false">\xc2\xaf<!-- \xc2\xaf --></mo>\n</mover>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>1</mn>\n<mi>n</mi>\n</mfrac>\n</mrow>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</munderover>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\overline {w}}_{n}={\\frac {1}{n}}\\sum _{i=1}^{n}w_{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle {\\overline {w}}_{n}={\\frac {1}{n}}\\sum _{i=1}^{n}w_{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d9119c4b00f636c8510b87321eabe8aa5962814b" style="vertical-align: -3.005ex; width:14.92ex; height:6.843ex;"/></span>. This setting is a special case of <a href="/wiki/Stochastic_optimization" title="Stochastic optimization">stochastic optimization</a>, a well known problem in optimization.<sup class="reference" id="cite_ref-lorenzo_1-2"><a href="#cite_note-lorenzo-1">[1]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Incremental_stochastic_gradient_descent">Incremental stochastic gradient descent</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=7" title="Edit section: Incremental stochastic gradient descent">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>In practice, one can perform multiple stochastic gradient passes (also called cycles or epochs) over the data. The algorithm thus obtained is\ncalled incremental gradient method and corresponds to an iteration\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{t_{i}}\\rangle ,y_{t_{i}})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mi mathvariant="normal">\xe2\x88\x87<!-- \xe2\x88\x87 --></mi>\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<msub>\n<mi>t</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<msub>\n<mi>t</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{t_{i}}\\rangle ,y_{t_{i}})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle w_{i}=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{t_{i}}\\rangle ,y_{t_{i}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7c5cb0e51191931233dcd567146a874fc2ca7f27" style="vertical-align: -1.005ex; width:34.316ex; height:3.009ex;"/></span></dd></dl>\n<p>The main difference with the stochastic gradient method is that here a sequence <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle t_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>t</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle t_{i}}</annotation>\n</semantics>\n</math></span><img alt=" t_i " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8b61e3d4d909be4a19c9a554a301684232f59e5a" style="vertical-align: -0.671ex; width:1.639ex; height:2.343ex;"/></span> is chosen to decide which training point is visited in the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>-th step. Such a sequence can be stochastic or deterministic. The number of iterations is then decoupled to the number of points (each point can be considered more than once). The incremental gradient method can be shown to provide a minimizer to the empirical risk.<sup class="reference" id="cite_ref-bertsekas_3-0"><a href="#cite_note-bertsekas-3">[3]</a></sup> Incremental techniques can be advantageous when considering objective functions made up of a sum of many terms e.g. an empirical error corresponding to a very large dataset.<sup class="reference" id="cite_ref-lorenzo_1-3"><a href="#cite_note-lorenzo-1">[1]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Kernel_methods">Kernel methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=8" title="Edit section: Kernel methods">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">See also: <a href="/wiki/Kernel_method" title="Kernel method">Kernel method</a></div>\n<p>Kernels can be used to extend the above algorithms to non-parametric models (or models where the parameters form an infinite dimensional space). The corresponding procedure will no longer be truly online and instead involve storing all the data points, but is still faster than the brute force method.\nThis discussion is restricted to the case of the square loss, though it can be extended to any convex loss. It can be shown by an easy induction <sup class="reference" id="cite_ref-lorenzo_1-4"><a href="#cite_note-lorenzo-1">[1]</a></sup> that if <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle X_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle X_{i}}</annotation>\n</semantics>\n</math></span><img alt="X_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af4a0955af42beb5f85aa05fb8c07abedc13990d" style="vertical-align: -0.671ex; width:2.724ex; height:2.509ex;"/></span> is the data matrix and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{i}}</annotation>\n</semantics>\n</math></span><img alt="w_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fe22f0329d3ecb2e1880d44d191aba0e5475db68" style="vertical-align: -0.671ex; width:2.464ex; height:2.009ex;"/></span> is the output after <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> steps of the SGD algorithm, then,\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{i}=X_{i}^{T}c_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msubsup>\n<mi>X</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>T</mi>\n</mrow>\n</msubsup>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{i}=X_{i}^{T}c_{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{i}=X_{i}^{T}c_{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f29d157f5f850ed11e4854f9fc4e8b0e70f6c3d" style="vertical-align: -1.005ex; width:10.755ex; height:3.176ex;"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\\in \\mathbb {R} ^{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mo stretchy="false">(</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<mo>.</mo>\n<mo>.</mo>\n<mo>.</mo>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msup>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\\in \\mathbb {R} ^{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\\in \\mathbb {R} ^{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e6dfd05754d64e285b923684e8ca3b4d0600d5b7" style="vertical-align: -0.838ex; width:31.992ex; height:3.176ex;"/></span> and the sequence <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle c_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle c_{i}}</annotation>\n</semantics>\n</math></span><img alt="c_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01acb7953ba52c2aa44264b5d0f8fd223aa178a2" style="vertical-align: -0.671ex; width:1.807ex; height:2.009ex;"/></span> satisfies the recursion:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle c_{0}=0}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>0</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>0</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle c_{0}=0}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle c_{0}=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/29af3d4e887815bb3b9b9eab4f7540a376fccd73" style="vertical-align: -0.671ex; width:6.322ex; height:2.509ex;"/></span></dd>\n<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n<mo>,</mo>\n<mn>2</mn>\n<mo>,</mo>\n<mo>.</mo>\n<mo>.</mo>\n<mo>.</mo>\n<mo>,</mo>\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4f9a0002c727d6162eaf7e8422526d6ebb8ee49" style="vertical-align: -1.005ex; width:32.674ex; height:3.009ex;"/></span> and</dd>\n<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x_{i}\\rangle {\\Big )}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mo maxsize="1.623em" minsize="1.623em">(</mo>\n</mrow>\n</mrow>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</munderover>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mo maxsize="1.623em" minsize="1.623em">)</mo>\n</mrow>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x_{i}\\rangle {\\Big )}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x_{i}\\rangle {\\Big )}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/42e34e123abeee13bd9d0a81aeac12f9211f9005" style="vertical-align: -3.338ex; width:34.266ex; height:7.676ex;"/></span></dd></dl>\n<p>Notice that here <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\langle x_{j},x_{i}\\rangle }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\langle x_{j},x_{i}\\rangle }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\langle x_{j},x_{i}\\rangle }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3c3ff392933a4e4200440f1955f96374a4cfd8f7" style="vertical-align: -1.005ex; width:7.212ex; height:3.009ex;"/></span> is just the standard Kernel on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="\\mathbb {R} ^{d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a713426956296f1668fce772df3c60b9dde8a685" style="vertical-align: -0.338ex; width:2.77ex; height:2.676ex;"/></span>, and the predictor is of the form \n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f_{i}(x)=\\langle w_{i-1},x\\rangle =\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x\\rangle }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>x</mi>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>=</mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</munderover>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>x</mi>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f_{i}(x)=\\langle w_{i-1},x\\rangle =\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x\\rangle }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle f_{i}(x)=\\langle w_{i-1},x\\rangle =\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x\\rangle }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b3a36b77e65399171e05dfa946aeb41da101029a" style="vertical-align: -3.338ex; width:36.405ex; height:7.676ex;"/></span>.</dd></dl>\n<p>Now, if  a general kernel <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle K}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>K</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle K}</annotation>\n</semantics>\n</math></span><img alt="K" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;"/></span> is introduced instead and let the predictor be \n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f_{i}(x)=\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</munderover>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mi>K</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>x</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f_{i}(x)=\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle f_{i}(x)=\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/03e34890f01157953b3c2e4cf9b7d4a2894352fb" style="vertical-align: -3.338ex; width:26.636ex; height:7.676ex;"/></span></dd></dl>\n<p>then the same proof will also show that predictor minimising the least squares loss is obtained by changing the above recursion to\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\\Big )}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mo maxsize="1.623em" minsize="1.623em">(</mo>\n</mrow>\n</mrow>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</munderover>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mi>K</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mo maxsize="1.623em" minsize="1.623em">)</mo>\n</mrow>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\\Big )}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\\Big )}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86b9e3c39a28c669b174001027cc216197155076" style="vertical-align: -3.338ex; width:36.332ex; height:7.676ex;"/></span></dd></dl>\n<p>The above expression requires storing all the data for updating <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle c_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle c_{i}}</annotation>\n</semantics>\n</math></span><img alt="c_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01acb7953ba52c2aa44264b5d0f8fd223aa178a2" style="vertical-align: -0.671ex; width:1.807ex; height:2.009ex;"/></span>. The total time complexity for the recursion when evaluating for the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle n}</annotation>\n</semantics>\n</math></span><img alt="n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span>-th datapoint is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(n^{2}dk)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>n</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mi>d</mi>\n<mi>k</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(n^{2}dk)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(n^{2}dk)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1e16d08a5cc43315d54e67fdc304b4ac645be891" style="vertical-align: -0.838ex; width:8.459ex; height:3.176ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>k</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k}</annotation>\n</semantics>\n</math></span><img alt="k" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" style="vertical-align: -0.338ex; width:1.211ex; height:2.176ex;"/></span> is the cost of evaluating the kernel on a single pair of points.<sup class="reference" id="cite_ref-lorenzo_1-5"><a href="#cite_note-lorenzo-1">[1]</a></sup>\nThus, the use of the kernel has allowed the movement from a finite dimensional parameter space <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle w_{i}\\in \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle w_{i}\\in \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle w_{i}\\in \\mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e501c93c63db39cee8f8585f7c791dffa514bf63" style="vertical-align: -0.671ex; width:8.075ex; height:3.009ex;"/></span> to a possibly infinite dimensional feature represented by a kernel <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle K}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>K</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle K}</annotation>\n</semantics>\n</math></span><img alt="K" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;"/></span> by instead performing the recursion on the space of parameters <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle c_{i}\\in \\mathbb {R} ^{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi>c</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msup>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle c_{i}\\in \\mathbb {R} ^{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle c_{i}\\in \\mathbb {R} ^{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e488a21fc80f8cdacfcfca57c446178bc6bb7785" style="vertical-align: -0.671ex; width:7.125ex; height:3.009ex;"/></span>, whose dimension is the same as the size of the training dataset. In general, this is a consequence of the <a href="/wiki/Representer_theorem" title="Representer theorem">representer theorem</a>.<sup class="reference" id="cite_ref-lorenzo_1-6"><a href="#cite_note-lorenzo-1">[1]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Online_convex_optimization">Online convex optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=9" title="Edit section: Online convex optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Online convex optimization (OCO) <sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup> is a general framework for decision making which leverages <a href="/wiki/Convex_optimization" title="Convex optimization">convex optimization</a> to allow for efficient algorithms. The framework is that of repeated game playing as follows:\n</p><p>For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle t=1,2,...,T}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>t</mi>\n<mo>=</mo>\n<mn>1</mn>\n<mo>,</mo>\n<mn>2</mn>\n<mo>,</mo>\n<mo>.</mo>\n<mo>.</mo>\n<mo>.</mo>\n<mo>,</mo>\n<mi>T</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle t=1,2,...,T}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle t=1,2,...,T}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e782a9b81091d11641be9afacfd9b02a85e3e81" style="vertical-align: -0.671ex; width:14.103ex; height:2.509ex;"/></span>\n</p>\n<ul><li>Learner receives input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle x_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f279a30bc8eabc788f3fe81c9cfb674e72e858db" style="vertical-align: -0.671ex; width:2.156ex; height:2.009ex;"/></span></li>\n<li>Learner outputs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6cc7b029066adbf68868f39f3dceb58eab2d1a12" style="vertical-align: -0.671ex; width:2.49ex; height:2.009ex;"/></span> from a fixed convex set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle S}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>S</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle S}</annotation>\n</semantics>\n</math></span><img alt="S" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2" style="vertical-align: -0.338ex; width:1.499ex; height:2.176ex;"/></span></li>\n<li>Nature sends back a convex loss function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>:</mo>\n<mi>S</mi>\n<mo stretchy="false">\xe2\x86\x92<!-- \xe2\x86\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f4aac9a64a3f96504a3b330866a2494dda6d9968" style="vertical-align: -0.671ex; width:10.682ex; height:2.509ex;"/></span>.</li>\n<li>Learner suffers loss <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}(w_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}(w_{t})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}(w_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/576bc9c7780495a7cfe56b9d746665d29d4adcd0" style="vertical-align: -0.838ex; width:6.253ex; height:2.843ex;"/></span> and updates its model</li></ul>\n<p>The goal is to minimize <a href="/wiki/Regret" title="Regret">regret</a>, or the difference between cumulative loss and the loss of the best fixed point  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle u\\in S}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>u</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<mi>S</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle u\\in S}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle u\\in S}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a81bd6308fa73ba1bec537a7b5f20429936b0fa2" style="vertical-align: -0.338ex; width:5.67ex; height:2.176ex;"/></span> in hindsight.\nAs an example, consider the case of online least squares linear regression. Here, the weight vectors come from the convex set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle S=\\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>S</mi>\n<mo>=</mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle S=\\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle S=\\mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b0bf04d6266ea81c5279a23971eacd19af797b2" style="vertical-align: -0.338ex; width:7.368ex; height:2.676ex;"/></span>, and nature sends back the convex loss function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}(w)=(\\langle w,x_{t}\\rangle -y_{t})^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mo stretchy="false">(</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<mi>w</mi>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<msup>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}(w)=(\\langle w,x_{t}\\rangle -y_{t})^{2}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}(w)=(\\langle w,x_{t}\\rangle -y_{t})^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/db2a94478ee70fb26e74740bb1ea6b930b6ce7fb" style="vertical-align: -0.838ex; width:22.858ex; height:3.176ex;"/></span>. Note here that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle y_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle y_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle y_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0fe9554452b93508c9d2479414a45981ecc75a2d" style="vertical-align: -0.671ex; width:1.965ex; height:2.009ex;"/></span> is implicitly sent with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7be17a7e5bec78adf7afc13d266bdd9514963783" style="vertical-align: -0.671ex; width:1.954ex; height:2.009ex;"/></span>.\n</p><p>Some online prediction problems however cannot fit in the framework of OCO. For example, in online classification, the prediction domain and the loss functions are not convex. In such scenarios, two simple techniques for convexification are used: randomisation and surrogate loss functions<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2019)">citation needed</span></a></i>]</sup>.\n</p><p>Some simple online convex optimisation algorithms are:\n</p>\n<h4><span id="Follow_the_leader_.28FTL.29"></span><span class="mw-headline" id="Follow_the_leader_(FTL)">Follow the leader (FTL)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=10" title="Edit section: Follow the leader (FTL)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>\n<p>The simplest learning rule to try is to select (at the current step) the hypothesis that has the least loss over all past rounds. This algorithm is called Follow the leader, and is simply given round <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>t</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle t}</annotation>\n</semantics>\n</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span> by:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t}=\\operatorname {arg\\,min} _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<mi mathvariant="normal">a</mi>\n<mi mathvariant="normal">r</mi>\n<mi mathvariant="normal">g</mi>\n<mspace width="thinmathspace"></mspace>\n<mi mathvariant="normal">m</mi>\n<mi mathvariant="normal">i</mi>\n<mi mathvariant="normal">n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>w</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<mi>S</mi>\n</mrow>\n</msub>\n<mo>\xe2\x81\xa1<!-- \xe2\x81\xa1 --></mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</munderover>\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t}=\\operatorname {arg\\,min} _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t}=\\operatorname {arg\\,min} _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ceade2d09ce6f405180e085f4a021a5db8d8bb04" style="vertical-align: -3.005ex; width:26.183ex; height:7.343ex;"/></span></dd></dl>\n<p>This method can thus be looked as a <a href="/wiki/Greedy_algorithm" title="Greedy algorithm">greedy algorithm</a>. For the case of online quadratic optimization (where the loss function is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}(w)=||w-x_{t}||_{2}^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mi>w</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<msubsup>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}(w)=||w-x_{t}||_{2}^{2}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}(w)=||w-x_{t}||_{2}^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6e68fc1d850d32301d3549a601d1a229cf1233d" style="vertical-align: -0.838ex; width:18.827ex; height:3.343ex;"/></span>), one can show a regret bound that grows as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\log(T)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>log</mi>\n<mo>\xe2\x81\xa1<!-- \xe2\x81\xa1 --></mo>\n<mo stretchy="false">(</mo>\n<mi>T</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\log(T)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\log(T)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1bee4e727686d591fcbe58960c39d6046ba79e66" style="vertical-align: -0.838ex; width:6.417ex; height:2.843ex;"/></span>. However, similar bounds cannot be obtained for the FTL algorithm for other important families of models like online linear optimization. To do so, one modifies FTL by adding regularisation.\n</p>\n<h4><span id="Follow_the_regularised_leader_.28FTRL.29"></span><span class="mw-headline" id="Follow_the_regularised_leader_(FTRL)">Follow the regularised leader (FTRL)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=11" title="Edit section: Follow the regularised leader (FTRL)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>\n<p>This is a natural modification of FTL that is used to stabilise the FTL solutions and obtain better regret bounds. A regularisation function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle R:S\\rightarrow \\mathbb {R} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>R</mi>\n<mo>:</mo>\n<mi>S</mi>\n<mo stretchy="false">\xe2\x86\x92<!-- \xe2\x86\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle R:S\\rightarrow \\mathbb {R} }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle R:S\\rightarrow \\mathbb {R} }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/023345b0d7a4491239be3169c47686d3acba21da" style="vertical-align: -0.338ex; width:10.493ex; height:2.176ex;"/></span>  is chosen and learning performed in round <span class="texhtml mvar" style="font-style:italic;">t</span> as follows:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t}=\\operatorname {arg\\,min} _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)+R(w)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<mi mathvariant="normal">a</mi>\n<mi mathvariant="normal">r</mi>\n<mi mathvariant="normal">g</mi>\n<mspace width="thinmathspace"></mspace>\n<mi mathvariant="normal">m</mi>\n<mi mathvariant="normal">i</mi>\n<mi mathvariant="normal">n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>w</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<mi>S</mi>\n</mrow>\n</msub>\n<mo>\xe2\x81\xa1<!-- \xe2\x81\xa1 --></mo>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</munderover>\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n<mo>+</mo>\n<mi>R</mi>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t}=\\operatorname {arg\\,min} _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)+R(w)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t}=\\operatorname {arg\\,min} _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)+R(w)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5a43f1cf74fd1e4e8462c14833db3d0911e21b5b" style="vertical-align: -3.005ex; width:34.261ex; height:7.343ex;"/></span></dd></dl>\n<p>As a special example, consider the case of online linear optimisation i.e. where nature sends back loss functions of the form <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<mi>w</mi>\n<mo>,</mo>\n<msub>\n<mi>z</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/be15a7bfc7977ef704a367523ba85eb83607250f" style="vertical-align: -0.838ex; width:14.94ex; height:2.843ex;"/></span>. Also, let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle S=\\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>S</mi>\n<mo>=</mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle S=\\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle S=\\mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b0bf04d6266ea81c5279a23971eacd19af797b2" style="vertical-align: -0.338ex; width:7.368ex; height:2.676ex;"/></span>. Suppose the regularisation function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle R(w)={\\frac {1}{2\\eta }}||w||_{2}^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>R</mi>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>1</mn>\n<mrow>\n<mn>2</mn>\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n</mrow>\n</mfrac>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<msubsup>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle R(w)={\\frac {1}{2\\eta }}||w||_{2}^{2}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle R(w)={\\frac {1}{2\\eta }}||w||_{2}^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/76e80bacb4d8df9ac05ad519ca04be0e7f5cc8d5" style="vertical-align: -2.338ex; width:16.81ex; height:5.676ex;"/></span> is chosen for some positive number <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\eta }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\eta }</annotation>\n</semantics>\n</math></span><img alt="\\eta " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;"/></span>. Then, one can show that the regret minimising iteration becomes \n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t+1}=-\\eta \\sum _{i=1}^{t}z_{i}=w_{t}-\\eta z_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</munderover>\n<msub>\n<mi>z</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n<msub>\n<mi>z</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t+1}=-\\eta \\sum _{i=1}^{t}z_{i}=w_{t}-\\eta z_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t+1}=-\\eta \\sum _{i=1}^{t}z_{i}=w_{t}-\\eta z_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d421801678c57a86cff53a73029e97cbf76a2b8a" style="vertical-align: -3.005ex; width:28.182ex; height:7.176ex;"/></span></dd></dl>\n<p>Note that this can be rewritten as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t+1}=w_{t}-\\eta \\nabla v_{t}(w_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n<mi mathvariant="normal">\xe2\x88\x87<!-- \xe2\x88\x87 --></mi>\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t+1}=w_{t}-\\eta \\nabla v_{t}(w_{t})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t+1}=w_{t}-\\eta \\nabla v_{t}(w_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/45395b53a2136da7f278c854e7c21cdb65b9e67c" style="vertical-align: -0.838ex; width:22.378ex; height:2.843ex;"/></span>, which looks exactly like online gradient descent.\n</p><p>If <span class="texhtml mvar" style="font-style:italic;">S</span> is instead some convex subspace of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="\\mathbb {R} ^{d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a713426956296f1668fce772df3c60b9dde8a685" style="vertical-align: -0.338ex; width:2.77ex; height:2.676ex;"/></span>, <span class="texhtml mvar" style="font-style:italic;">S</span> would need to be projected onto, leading to the modified update rule\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t+1}=\\Pi _{S}(-\\eta \\sum _{i=1}^{t}z_{i})=\\Pi _{S}(\\eta \\theta _{t+1})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi mathvariant="normal">\xce\xa0<!-- \xce\xa0 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>S</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</munderover>\n<msub>\n<mi>z</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<msub>\n<mi mathvariant="normal">\xce\xa0<!-- \xce\xa0 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>S</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n<msub>\n<mi>\xce\xb8<!-- \xce\xb8 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t+1}=\\Pi _{S}(-\\eta \\sum _{i=1}^{t}z_{i})=\\Pi _{S}(\\eta \\theta _{t+1})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t+1}=\\Pi _{S}(-\\eta \\sum _{i=1}^{t}z_{i})=\\Pi _{S}(\\eta \\theta _{t+1})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ca5a1de357e76aaaee1139b530c10753d9a69960" style="vertical-align: -3.005ex; width:34.651ex; height:7.176ex;"/></span></dd></dl>\n<p>This algorithm is known as lazy projection, as the vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\theta _{t+1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>\xce\xb8<!-- \xce\xb8 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\theta _{t+1}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\theta _{t+1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3623a056f7192e02615ab4e408ef4392c27129b7" style="vertical-align: -0.671ex; width:4.017ex; height:2.509ex;"/></span> accumulates the gradients. It is also known as Nesterov\'s dual averaging algorithm. In this scenario of linear loss functions and quadratic regularisation, the regret is bounded by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O({\\sqrt {T}})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<msqrt>\n<mi>T</mi>\n</msqrt>\n</mrow>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O({\\sqrt {T}})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O({\\sqrt {T}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b08c799ec97aa1891429c5deea1bea86b1c8701e" style="vertical-align: -0.838ex; width:7.155ex; height:3.176ex;"/></span>, and thus the average regret goes to <span class="texhtml mvar" style="font-style:italic;">0</span> as desired.\n</p>\n<h3><span id="Online_subgradient_descent_.28OSD.29"></span><span class="mw-headline" id="Online_subgradient_descent_(OSD)">Online subgradient descent (OSD)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=12" title="Edit section: Online subgradient descent (OSD)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">See also: <a href="/wiki/Subgradient_method" title="Subgradient method">Subgradient method</a></div>\n<p>The above proved a regret bound for linear loss functions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<mi>w</mi>\n<mo>,</mo>\n<msub>\n<mi>z</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/be15a7bfc7977ef704a367523ba85eb83607250f" style="vertical-align: -0.838ex; width:14.94ex; height:2.843ex;"/></span>. To generalise the algorithm to any convex loss function, the <a class="mw-redirect" href="/wiki/Subgradient" title="Subgradient">subgradient</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\partial v_{t}(w_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi mathvariant="normal">\xe2\x88\x82<!-- \xe2\x88\x82 --></mi>\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\partial v_{t}(w_{t})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\partial v_{t}(w_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/78863717aca31128f37dc026c157daa289930765" style="vertical-align: -0.838ex; width:7.571ex; height:2.843ex;"/></span> of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7be17a7e5bec78adf7afc13d266bdd9514963783" style="vertical-align: -0.671ex; width:1.954ex; height:2.009ex;"/></span> is used as a linear approximation to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7be17a7e5bec78adf7afc13d266bdd9514963783" style="vertical-align: -0.671ex; width:1.954ex; height:2.009ex;"/></span> near <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6cc7b029066adbf68868f39f3dceb58eab2d1a12" style="vertical-align: -0.671ex; width:2.49ex; height:2.009ex;"/></span>, leading to the online subgradient descent algorithm:\n</p><p>Initialise parameter <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\eta ,w_{1}=0}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n<mo>,</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>0</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\eta ,w_{1}=0}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\eta ,w_{1}=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/46e2f700839d6d66e587c029fbc72c9b8364378b" style="vertical-align: -0.838ex; width:9.183ex; height:2.676ex;"/></span>\n</p><p>For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle t=1,2,...,T}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>t</mi>\n<mo>=</mo>\n<mn>1</mn>\n<mo>,</mo>\n<mn>2</mn>\n<mo>,</mo>\n<mo>.</mo>\n<mo>.</mo>\n<mo>.</mo>\n<mo>,</mo>\n<mi>T</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle t=1,2,...,T}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle t=1,2,...,T}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e782a9b81091d11641be9afacfd9b02a85e3e81" style="vertical-align: -0.671ex; width:14.103ex; height:2.509ex;"/></span>\n</p>\n<ul><li>Predict using <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6cc7b029066adbf68868f39f3dceb58eab2d1a12" style="vertical-align: -0.671ex; width:2.49ex; height:2.009ex;"/></span>, receive <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f_{t}}</annotation>\n</semantics>\n</math></span><img alt="f_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/874c306411e808e8191e8aeb95e3440e1c68d6e9" style="vertical-align: -0.671ex; width:1.965ex; height:2.509ex;"/></span> from nature.</li>\n<li>Choose <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle z_{t}\\in \\partial v_{t}(w_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>z</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<mi mathvariant="normal">\xe2\x88\x82<!-- \xe2\x88\x82 --></mi>\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle z_{t}\\in \\partial v_{t}(w_{t})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle z_{t}\\in \\partial v_{t}(w_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bb890df9aba35d93e2f0564af6cec6da3678de41" style="vertical-align: -0.838ex; width:12.319ex; height:2.843ex;"/></span></li>\n<li>If <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle S=\\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>S</mi>\n<mo>=</mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle S=\\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle S=\\mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b0bf04d6266ea81c5279a23971eacd19af797b2" style="vertical-align: -0.338ex; width:7.368ex; height:2.676ex;"/></span>, update as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t+1}=w_{t}-\\eta z_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n<msub>\n<mi>z</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t+1}=w_{t}-\\eta z_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t+1}=w_{t}-\\eta z_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fbdeb93195963cda0c2afabd10df5b69ddce62b6" style="vertical-align: -0.838ex; width:16.096ex; height:2.509ex;"/></span></li>\n<li>If <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle S\\subset \\mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>S</mi>\n<mo>\xe2\x8a\x82<!-- \xe2\x8a\x82 --></mo>\n<msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>d</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle S\\subset \\mathbb {R} ^{d}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle S\\subset \\mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7396d3e00a93e85d9cd724ce29e250b645d3684c" style="vertical-align: -0.338ex; width:7.368ex; height:2.676ex;"/></span>, project cumulative gradients onto <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle S}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>S</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle S}</annotation>\n</semantics>\n</math></span><img alt="S" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2" style="vertical-align: -0.338ex; width:1.499ex; height:2.176ex;"/></span> i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t+1}=\\Pi _{S}(\\eta \\theta _{t+1}),\\theta _{t+1}=\\theta _{t}+z_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi mathvariant="normal">\xce\xa0<!-- \xce\xa0 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>S</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>\xce\xb7<!-- \xce\xb7 --></mi>\n<msub>\n<mi>\xce\xb8<!-- \xce\xb8 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<msub>\n<mi>\xce\xb8<!-- \xce\xb8 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>\xce\xb8<!-- \xce\xb8 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>+</mo>\n<msub>\n<mi>z</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t+1}=\\Pi _{S}(\\eta \\theta _{t+1}),\\theta _{t+1}=\\theta _{t}+z_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{t+1}=\\Pi _{S}(\\eta \\theta _{t+1}),\\theta _{t+1}=\\theta _{t}+z_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4cb83ab441ee54f02cf8beedecaba2d96d2a8cc2" style="vertical-align: -0.838ex; width:32.533ex; height:2.843ex;"/></span></li></ul>\n<p>One can use the OSD algorithm to derive <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O({\\sqrt {T}})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<msqrt>\n<mi>T</mi>\n</msqrt>\n</mrow>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O({\\sqrt {T}})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O({\\sqrt {T}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b08c799ec97aa1891429c5deea1bea86b1c8701e" style="vertical-align: -0.838ex; width:7.155ex; height:3.176ex;"/></span> regret bounds for the online version of <a href="/wiki/Support_vector_machine" title="Support vector machine">SVM\'s</a> for classification, which use the <a href="/wiki/Hinge_loss" title="Hinge loss">hinge loss</a><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{t}(w)=\\max\\{0,1-y_{t}(w\\cdot x_{t})\\}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mo form="prefix" movablelimits="true">max</mo>\n<mo fence="false" stretchy="false">{</mo>\n<mn>0</mn>\n<mo>,</mo>\n<mn>1</mn>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>w</mi>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo fence="false" stretchy="false">}</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{t}(w)=\\max\\{0,1-y_{t}(w\\cdot x_{t})\\}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{t}(w)=\\max\\{0,1-y_{t}(w\\cdot x_{t})\\}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/70230f311468716562e6e8feb5193d5ec59d4743" style="vertical-align: -0.838ex; width:30.649ex; height:2.843ex;"/></span>\n</p>\n<h3><span class="mw-headline" id="Other_algorithms">Other algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=13" title="Edit section: Other algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Quadratically regularised FTRL algorithms lead to lazily projected gradient algorithms as described above. To use the above for arbitrary convex functions and regularisers, one uses online mirror descent.  The optimal regularization in hindsight can be derived for linear loss functions, this leads to the <a class="mw-redirect" href="/wiki/AdaGrad" title="AdaGrad">AdaGrad</a> algorithm.\nFor the Euclidean regularisation, one can show a regret bound of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O({\\sqrt {T}})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<msqrt>\n<mi>T</mi>\n</msqrt>\n</mrow>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O({\\sqrt {T}})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O({\\sqrt {T}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b08c799ec97aa1891429c5deea1bea86b1c8701e" style="vertical-align: -0.838ex; width:7.155ex; height:3.176ex;"/></span>, which can be improved further to a <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(\\log T)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>log</mi>\n<mo>\xe2\x81\xa1<!-- \xe2\x81\xa1 --></mo>\n<mi>T</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(\\log T)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(\\log T)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/060927e569d07c3fa5fc077114289c62f0d43bd3" style="vertical-align: -0.838ex; width:8.578ex; height:2.843ex;"/></span> for strongly convex and exp-concave loss functions.\n</p>\n<h2><span class="mw-headline" id="Interpretations_of_online_learning">Interpretations of online learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=14" title="Edit section: Interpretations of online learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The paradigm of online learning has different interpretations depending on the choice of the learning model, each of which has distinct implications about the predictive quality of the sequence of functions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f_{1},f_{2},\\ldots ,f_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<msub>\n<mi>f</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f_{1},f_{2},\\ldots ,f_{n}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle f_{1},f_{2},\\ldots ,f_{n}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9fe58abcecb7415dea502115e7db74734f145584" style="vertical-align: -0.671ex; width:12.957ex; height:2.509ex;"/></span>. The prototypical stochastic gradient descent algorithm is used for this discussion. As noted above, its recursion is given by\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\textstyle w_{t}=w_{t-1}-\\gamma _{t}\\nabla V(\\langle w_{t-1},x_{t}\\rangle ,y_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mstyle displaystyle="false" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mi mathvariant="normal">\xe2\x88\x87<!-- \xe2\x88\x87 --></mi>\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\textstyle w_{t}=w_{t-1}-\\gamma _{t}\\nabla V(\\langle w_{t-1},x_{t}\\rangle ,y_{t})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\textstyle w_{t}=w_{t-1}-\\gamma _{t}\\nabla V(\\langle w_{t-1},x_{t}\\rangle ,y_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f88e9b146d3e739845791afb783a9ee63c32529f" style="vertical-align: -0.838ex; width:33.171ex; height:2.843ex;"/></span></dd></dl>\n<p>The first interpretation consider the <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">stochastic gradient descent</a> method as applied to the problem of minimizing the expected risk <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I[w]}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>I</mi>\n<mo stretchy="false">[</mo>\n<mi>w</mi>\n<mo stretchy="false">]</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I[w]}</annotation>\n</semantics>\n</math></span><img alt="I[w]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/42ebf51718836c1a9f3919691fc4f1d81d7d481d" style="vertical-align: -0.838ex; width:4.13ex; height:2.843ex;"/></span> defined above.<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> Indeed, in the case of an infinite stream of data, since the examples <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (x_{1},y_{1}),(x_{2},y_{2}),\\ldots }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (x_{1},y_{1}),(x_{2},y_{2}),\\ldots }</annotation>\n</semantics>\n</math></span><img alt="(x_{1},y_{1}),(x_{2},y_{2}),\\ldots " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0ab216837403ea50bd6cddf544a29e4e8491bca0" style="vertical-align: -0.838ex; width:19.632ex; height:2.843ex;"/></span> are assumed to be drawn i.i.d. from the distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle p(x,y)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>p</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle p(x,y)}</annotation>\n</semantics>\n</math></span><img alt="p(x,y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/089e91a1824e14cebc8e8d04dc652c61b3008e0a" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:6.587ex; height:2.843ex;"/></span>, the sequence of gradients of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle V(\\cdot ,\\cdot )}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mo>,</mo>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle V(\\cdot ,\\cdot )}</annotation>\n</semantics>\n</math></span><img alt="V(\\cdot ,\\cdot )" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e8a5bfec9da68c91698ea8828a02982e2ce01b7f" style="vertical-align: -0.838ex; width:5.924ex; height:2.843ex;"/></span> in the above iteration are an i.i.d. sample of stochastic estimates of the gradient of the expected risk <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I[w]}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>I</mi>\n<mo stretchy="false">[</mo>\n<mi>w</mi>\n<mo stretchy="false">]</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I[w]}</annotation>\n</semantics>\n</math></span><img alt="I[w]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/42ebf51718836c1a9f3919691fc4f1d81d7d481d" style="vertical-align: -0.838ex; width:4.13ex; height:2.843ex;"/></span> and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I[w_{t}]-I[w^{\\ast }]}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>I</mi>\n<mo stretchy="false">[</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">]</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>I</mi>\n<mo stretchy="false">[</mo>\n<msup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n<mo stretchy="false">]</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I[w_{t}]-I[w^{\\ast }]}</annotation>\n</semantics>\n</math></span><img alt="I[w_{t}]-I[w^{\\ast }]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6c0e3532c4a8fa3b6bdbf730941948ca8ffcb3f9" style="vertical-align: -0.838ex; width:12.98ex; height:2.843ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w^{\\ast }}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w^{\\ast }}</annotation>\n</semantics>\n</math></span><img alt="w^{\\ast }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/88bf1b79b1bf570879555bfc5a296fe90617589c" style="vertical-align: -0.338ex; width:2.718ex; height:2.343ex;"/></span> is the minimizer of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I[w]}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>I</mi>\n<mo stretchy="false">[</mo>\n<mi>w</mi>\n<mo stretchy="false">]</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I[w]}</annotation>\n</semantics>\n</math></span><img alt="I[w]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/42ebf51718836c1a9f3919691fc4f1d81d7d481d" style="vertical-align: -0.838ex; width:4.13ex; height:2.843ex;"/></span>.<sup class="reference" id="cite_ref-kushneryin_6-0"><a href="#cite_note-kushneryin-6">[6]</a></sup> This interpretation is also valid in the case of a finite training set; although with multiple passes through the data the gradients are no longer independent, still complexity results can be obtained in special cases.\n</p><p>The second interpretation applies to the case of a finite training set and considers the SGD algorithm as an instance of incremental gradient descent method.<sup class="reference" id="cite_ref-bertsekas_3-1"><a href="#cite_note-bertsekas-3">[3]</a></sup> In this case, one instead looks at the empirical risk:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I_{n}[w]={\\frac {1}{n}}\\sum _{i=1}^{n}V(\\langle w,x_{i}\\rangle ,y_{i})\\ .}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>I</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">[</mo>\n<mi>w</mi>\n<mo stretchy="false">]</mo>\n<mo>=</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>1</mn>\n<mi>n</mi>\n</mfrac>\n</mrow>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</munderover>\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mo fence="false" stretchy="false">\xe2\x9f\xa8<!-- \xe2\x9f\xa8 --></mo>\n<mi>w</mi>\n<mo>,</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo fence="false" stretchy="false">\xe2\x9f\xa9<!-- \xe2\x9f\xa9 --></mo>\n<mo>,</mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mtext>\xc2\xa0</mtext>\n<mo>.</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I_{n}[w]={\\frac {1}{n}}\\sum _{i=1}^{n}V(\\langle w,x_{i}\\rangle ,y_{i})\\ .}</annotation>\n</semantics>\n</math></span><img alt="I_{n}[w]={\\frac {1}{n}}\\sum _{i=1}^{n}V(\\langle w,x_{i}\\rangle ,y_{i})\\ ." aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9b8f34491d029210c40d1ae50439e6b8940a89d7" style="vertical-align: -3.005ex; width:29.091ex; height:6.843ex;"/></span></dd></dl>\n<p>Since the gradients of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle V(\\cdot ,\\cdot )}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>V</mi>\n<mo stretchy="false">(</mo>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mo>,</mo>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle V(\\cdot ,\\cdot )}</annotation>\n</semantics>\n</math></span><img alt="V(\\cdot ,\\cdot )" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e8a5bfec9da68c91698ea8828a02982e2ce01b7f" style="vertical-align: -0.838ex; width:5.924ex; height:2.843ex;"/></span> in the incremental gradient descent iterations are also stochastic estimates of the gradient of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I_{n}[w]}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>I</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">[</mo>\n<mi>w</mi>\n<mo stretchy="false">]</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I_{n}[w]}</annotation>\n</semantics>\n</math></span><img alt="I_{n}[w]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d3517a76840cb00d16481a59a5e0eb5efaa54a9" style="vertical-align: -0.838ex; width:5.199ex; height:2.843ex;"/></span>, this interpretation is also related to the stochastic gradient descent method, but applied to minimize the empirical risk as opposed to the expected risk. Since this interpretation concerns the empirical risk and not the expected risk, multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I_{n}[w_{t}]-I_{n}[w_{n}^{\\ast }]}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>I</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">[</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">]</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>I</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">[</mo>\n<msubsup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msubsup>\n<mo stretchy="false">]</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I_{n}[w_{t}]-I_{n}[w_{n}^{\\ast }]}</annotation>\n</semantics>\n</math></span><img alt="I_{n}[w_{t}]-I_{n}[w_{n}^{\\ast }]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0e2d73a59f3e75a8d8df94eae93e3d4ac3223ccf" style="vertical-align: -0.838ex; width:15.284ex; height:2.843ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{n}^{\\ast }}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>\xe2\x88\x97<!-- \xe2\x88\x97 --></mo>\n</mrow>\n</msubsup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{n}^{\\ast }}</annotation>\n</semantics>\n</math></span><img alt="w_{n}^{\\ast }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e987128a9c50932c880e5c5b2fa168a91b044b5a" style="vertical-align: -0.671ex; width:2.883ex; height:2.509ex;"/></span> is the minimizer of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle I_{n}[w]}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>I</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">[</mo>\n<mi>w</mi>\n<mo stretchy="false">]</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle I_{n}[w]}</annotation>\n</semantics>\n</math></span><img alt="I_{n}[w]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d3517a76840cb00d16481a59a5e0eb5efaa54a9" style="vertical-align: -0.838ex; width:5.199ex; height:2.843ex;"/></span>.\n</p>\n<h2><span class="mw-headline" id="Implementations">Implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=15" title="Edit section: Implementations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><a href="/wiki/Vowpal_Wabbit" title="Vowpal Wabbit">Vowpal Wabbit</a>: Open-source fast out-of-core online learning system which is notable for supporting a number of machine learning reductions, importance weighting and a selection of different loss functions and optimisation algorithms. It uses the <a href="/wiki/Feature_hashing" title="Feature hashing">hashing trick</a> for bounding the size of the set of features independent of the amount of training data.</li>\n<li><a href="/wiki/Scikit-learn" title="Scikit-learn">scikit-learn</a>: Provides out-of-core implementations of algorithms for\n<ul><li>Classification: <a href="/wiki/Perceptron" title="Perceptron">Perceptron</a>, <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD classifier</a>, <a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive bayes classifier</a>.</li>\n<li>Regression: SGD Regressor, Passive Aggressive regressor.</li>\n<li>Clustering: <a href="/wiki/K-means_clustering" title="K-means clustering">Mini-batch k-means</a>.</li>\n<li>Feature extraction: <a class="mw-redirect" href="/wiki/Dictionary_learning" title="Dictionary learning">Mini-batch dictionary learning</a>, <a href="/wiki/Principal_component_analysis" title="Principal component analysis">Incremental PCA</a>.</li></ul></li></ul>\n<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=16" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p><b>Learning paradigms</b>\n</p>\n<ul><li><a href="/wiki/Incremental_learning" title="Incremental learning">Incremental learning</a></li>\n<li><a href="/wiki/Lazy_learning" title="Lazy learning">Lazy learning</a></li>\n<li><a href="/wiki/Offline_learning" title="Offline learning">Offline learning</a>, the opposite model</li>\n<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li></ul>\n<p><b>General algorithms</b>\n</p>\n<ul><li><a href="/wiki/Online_algorithm" title="Online algorithm">Online algorithm</a></li>\n<li><a href="/wiki/Online_optimization" title="Online optimization">Online optimization</a></li>\n<li><a href="/wiki/Streaming_algorithm" title="Streaming algorithm">Streaming algorithm</a></li>\n<li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">Stochastic gradient descent</a></li></ul>\n<p><b>Learning models</b>\n</p>\n<ul><li><a class="mw-redirect" href="/wiki/Adaptive_Resonance_Theory" title="Adaptive Resonance Theory">Adaptive Resonance Theory</a></li>\n<li><a href="/wiki/Hierarchical_temporal_memory" title="Hierarchical temporal memory">Hierarchical temporal memory</a></li>\n<li><a class="mw-redirect" href="/wiki/K-nearest_neighbor_algorithm" title="K-nearest neighbor algorithm">k-nearest neighbor algorithm</a></li>\n<li><a href="/wiki/Learning_vector_quantization" title="Learning vector quantization">Learning vector quantization</a></li>\n<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li></ul>\n<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=17" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="mw-references-wrap"><ol class="references">\n<li id="cite_note-lorenzo-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-lorenzo_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lorenzo_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-lorenzo_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-lorenzo_1-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-lorenzo_1-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-lorenzo_1-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-lorenzo_1-6"><sup><i><b>g</b></i></sup></a></span> <span class="reference-text">L. Rosasco, T. Poggio, Machine Learning: a Regularization Approach, MIT-9.520 Lectures Notes, Manuscript, Dec. 2015. Chapter 7 - Online Learning</span>\n</li>\n<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFYin2003">Yin, Harold J. Kushner, G. George (2003). <span class="cs1-lock-limited" title="Free access subject to limited trial, subscription normally required"><a class="external text" href="https://archive.org/details/stochasticapprox00yinh" rel="nofollow"><i>Stochastic approximation and recursive algorithms and applications</i></a></span> (Second ed.). New York: Springer. pp.\xc2\xa0<a class="external text" href="https://archive.org/details/stochasticapprox00yinh/page/n30" rel="nofollow">8</a>\xe2\x80\x9312. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0-387-21769-7" title="Special:BookSources/978-0-387-21769-7"><bdi>978-0-387-21769-7</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Stochastic+approximation+and+recursive+algorithms+and+applications&amp;rft.place=New+York&amp;rft.pages=8-12&amp;rft.edition=Second&amp;rft.pub=Springer&amp;rft.date=2003&amp;rft.isbn=978-0-387-21769-7&amp;rft.aulast=Yin&amp;rft.aufirst=Harold+J.+Kushner%2C+G.+George&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fstochasticapprox00yinh&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOnline+machine+learning"></span><style data-mw-deduplicate="TemplateStyles:r982806391">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>\n</li>\n<li id="cite_note-bertsekas-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-bertsekas_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bertsekas_3-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Bertsekas, D. P. (2011). Incremental gradient, subgradient, and proximal methods for convex optimization: a survey. Optimization for Machine Learning, 85.</span>\n</li>\n<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFHazan2015"><a class="new" href="/w/index.php?title=Elad_Hazan&amp;action=edit&amp;redlink=1" title="Elad Hazan (page does not exist)">Hazan, Elad</a> (2015). <a class="external text" href="http://ocobook.cs.princeton.edu/OCObook.pdf" rel="nofollow"><i>Introduction to Online Convex Optimization</i></a> <span class="cs1-format">(PDF)</span>. Foundations and Trends in Optimization.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Online+Convex+Optimization&amp;rft.pub=Foundations+and+Trends+in+Optimization&amp;rft.date=2015&amp;rft.aulast=Hazan&amp;rft.aufirst=Elad&amp;rft_id=http%3A%2F%2Focobook.cs.princeton.edu%2FOCObook.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOnline+machine+learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFBottou1998"><a href="/wiki/L%C3%A9on_Bottou" title="L\xc3\xa9on Bottou">Bottou, L\xc3\xa9on</a> (1998). "Online Algorithms and Stochastic Approximations". <span class="cs1-lock-registration" title="Free registration required"><a class="external text" href="https://archive.org/details/onlinelearningin0000unse" rel="nofollow"><i>Online Learning and Neural Networks</i></a></span>. Cambridge University Press. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0-521-65263-6" title="Special:BookSources/978-0-521-65263-6"><bdi>978-0-521-65263-6</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Online+Algorithms+and+Stochastic+Approximations&amp;rft.btitle=Online+Learning+and+Neural+Networks&amp;rft.pub=Cambridge+University+Press&amp;rft.date=1998&amp;rft.isbn=978-0-521-65263-6&amp;rft.aulast=Bottou&amp;rft.aufirst=L%C3%A9on&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fonlinelearningin0000unse&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOnline+machine+learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-kushneryin-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-kushneryin_6-0">^</a></b></span> <span class="reference-text"><i>Stochastic Approximation Algorithms and Applications</i>, Harold J. Kushner and G. George Yin, New York: Springer-Verlag, 1997.  <link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/><a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/0-387-94916-X" title="Special:BookSources/0-387-94916-X">0-387-94916-X</a>; 2nd ed., titled <i>Stochastic Approximation and Recursive Algorithms and Applications</i>, 2003, <link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/><a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/0-387-00894-2" title="Special:BookSources/0-387-00894-2">0-387-00894-2</a>.</span>\n</li>\n</ol></div>\n<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Online_machine_learning&amp;action=edit&amp;section=18" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><a class="external free" href="http://onlineprediction.net/" rel="nofollow">http://onlineprediction.net/</a>, Wiki for On-Line Prediction.</li>\n<li><a class="external text" href="https://www.mit.edu/~rakhlin/6.883/" rel="nofollow">6.883: Online Methods in Machine Learning: Theory and Applications. Alexander Rakhlin. MIT</a></li></ul>\n<!-- \nNewPP limit report\nParsed by mw2313\nCached time: 20201016045049\nCache expiry: 2592000\nDynamic content: false\nComplications: [vary\xe2\x80\x90revision\xe2\x80\x90sha1]\nCPU time usage: 0.512 seconds\nReal time usage: 0.879 seconds\nPreprocessor visited node count: 2273/1000000\nPost\xe2\x80\x90expand include size: 40068/2097152 bytes\nTemplate argument size: 3037/2097152 bytes\nHighest expansion depth: 15/40\nExpensive parser function count: 2/500\nUnstrip recursion depth: 1/20\nUnstrip post\xe2\x80\x90expand size: 23269/5000000 bytes\nLua time usage: 0.134/10.000 seconds\nLua memory usage: 3.93 MB/50 MB\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  376.382      1 -total\n 25.94%   97.644      3 Template:Cite_book\n 19.05%   71.705      1 Template:Short_description\n 13.10%   49.292      1 Template:Clarify\n 11.96%   45.030      2 Template:ISBN\n 11.65%   43.845      1 Template:Fix-span\n 11.00%   41.402      1 Template:Machine_learning_bar\n 10.18%   38.331      1 Template:Pagetype\n  9.91%   37.306      1 Template:Sidebar_with_collapsible_lists\n  7.68%   28.914      4 Template:Category_handler\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:19892153-0!canonical!math=5 and timestamp 20201016045048 and revision id 980724312\n -->\n</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>\n<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Online_machine_learning&amp;oldid=980724312">https://en.wikipedia.org/w/index.php?title=Online_machine_learning&amp;oldid=980724312</a>"</div></div>\n<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_September_2019" title="Category:Wikipedia articles needing clarification from September 2019">Wikipedia articles needing clarification from September 2019</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_September_2019" title="Category:Articles with unsourced statements from September 2019">Articles with unsourced statements from September 2019</a></li></ul></div></div>\n</div>\n</div>\n<div id="mw-data-after-content">\n<div class="read-more-container"></div>\n</div>\n<div id="mw-navigation">\n<h2>Navigation menu</h2>\n<div id="mw-head">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-personal-label" class="mw-portlet mw-portlet-personal vector-menu" id="p-personal" role="navigation">\n<h3 id="p-personal-label">\n<span>Personal tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Online+machine+learning" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Online+machine+learning" title="You\'re encouraged to log in; however, it\'s not mandatory. [o]">Log in</a></li></ul>\n</div>\n</nav>\n<div id="left-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-namespaces-label" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" id="p-namespaces" role="navigation">\n<h3 id="p-namespaces-label">\n<span>Namespaces</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Online_machine_learning" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Online_machine_learning" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-variants-label" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu vector-menu-dropdown" id="p-variants" role="navigation">\n<input aria-labelledby="p-variants-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-variants-label">\n<span>Variants</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n</div>\n<div id="right-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-views-label" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" id="p-views" role="navigation">\n<h3 id="p-views-label">\n<span>Views</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/Online_machine_learning">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=Online_machine_learning&amp;action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=Online_machine_learning&amp;action=history" title="Past revisions of this page [h]">View history</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-cactions-label" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu vector-menu-dropdown" id="p-cactions" role="navigation">\n<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-cactions-label">\n<span>More</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n<div id="p-search" role="search">\n<h3>\n<label for="searchInput">Search</label>\n</h3>\n<form action="/w/index.php" id="searchform">\n<div data-search-loc="header-navigation" id="simpleSearch">\n<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>\n<input name="title" type="hidden" value="Special:Search"/>\n<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">\n<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>\n</input></div>\n</form>\n</div>\n</div>\n</div>\n<div id="mw-panel">\n<div id="p-logo" role="banner">\n<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>\n</div>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-navigation-label" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">\n<h3 id="p-navigation-label">\n<span>Navigation</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Articles related to current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-interaction-label" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">\n<h3 id="p-interaction-label">\n<span>Contribute</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-introduction"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia">Learn to edit</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-tb-label" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" id="p-tb" role="navigation">\n<h3 id="p-tb-label">\n<span>Tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Online_machine_learning" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Online_machine_learning" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Online_machine_learning&amp;oldid=980724312" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Online_machine_learning&amp;action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Online_machine_learning&amp;id=980724312&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q7094097" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-coll-print_export-label" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">\n<h3 id="p-coll-print_export-label">\n<span>Print/export</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Online_machine_learning&amp;action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Online_machine_learning&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-lang-label" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" id="p-lang" role="navigation">\n<h3 id="p-lang-label">\n<span>Languages</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D9%85%D8%A7%D8%B4%DB%8C%D9%86_%D8%A8%D8%B1%D8%AE%D8%B7" hreflang="fa" lang="fa" title="\xdb\x8c\xd8\xa7\xd8\xaf\xda\xaf\xdb\x8c\xd8\xb1\xdb\x8c \xd9\x85\xd8\xa7\xd8\xb4\xdb\x8c\xd9\x86 \xd8\xa8\xd8\xb1\xd8\xae\xd8\xb7 \xe2\x80\x93 Persian">\xd9\x81\xd8\xa7\xd8\xb1\xd8\xb3\xdb\x8c</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/Algorithme_d%27apprentissage_incr%C3%A9mental" hreflang="fr" lang="fr" title="Algorithme d\'apprentissage incr\xc3\xa9mental \xe2\x80\x93 French">Fran\xc3\xa7ais</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%9E%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD%D0%BE%D0%B2%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD" hreflang="ru" lang="ru" title="\xd0\x9e\xd0\xbd\xd0\xbb\xd0\xb0\xd0\xb9\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb5 \xd0\xbe\xd0\xb1\xd1\x83\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xbc\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbd \xe2\x80\x93 Russian">\xd0\xa0\xd1\x83\xd1\x81\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9</a></li><li class="interlanguage-link interwiki-sr"><a class="interlanguage-link-target" href="https://sr.wikipedia.org/wiki/%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D1%81%D0%BA%D0%BE_%D1%83%D1%87%D0%B5%D1%9A%D0%B5_%D0%BD%D0%B0_%D0%BC%D1%80%D0%B5%D0%B6%D0%B8" hreflang="sr" lang="sr" title="\xd0\x9c\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbd\xd1\x81\xd0\xba\xd0\xbe \xd1\x83\xd1\x87\xd0\xb5\xd1\x9a\xd0\xb5 \xd0\xbd\xd0\xb0 \xd0\xbc\xd1\x80\xd0\xb5\xd0\xb6\xd0\xb8 \xe2\x80\x93 Serbian">\xd0\xa1\xd1\x80\xd0\xbf\xd1\x81\xd0\xba\xd0\xb8 / srpski</a></li><li class="interlanguage-link interwiki-zh-yue"><a class="interlanguage-link-target" href="https://zh-yue.wikipedia.org/wiki/%E5%9C%A8%E7%B7%9A%E6%A9%9F%E6%A2%B0%E5%AD%B8%E7%BF%92" hreflang="yue" lang="yue" title="\xe5\x9c\xa8\xe7\xb7\x9a\xe6\xa9\x9f\xe6\xa2\xb0\xe5\xad\xb8\xe7\xbf\x92 \xe2\x80\x93 Cantonese">\xe7\xb2\xb5\xe8\xaa\x9e</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/%E7%B7%9A%E4%B8%8A%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92" hreflang="zh" lang="zh" title="\xe7\xb7\x9a\xe4\xb8\x8a\xe6\xa9\x9f\xe5\x99\xa8\xe5\xad\xb8\xe7\xbf\x92 \xe2\x80\x93 Chinese">\xe4\xb8\xad\xe6\x96\x87</a></li></ul>\n<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q7094097#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class="mw-footer" id="footer" role="contentinfo">\n<ul id="footer-info">\n<li id="footer-info-lastmod"> This page was last edited on 28 September 2020, at 02:33<span class="anonymous-show">\xc2\xa0(UTC)</span>.</li>\n<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia\xc2\xae is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id="footer-places">\n<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>\n<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>\n<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>\n<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>\n<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Online_machine_learning&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>\n<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>\n<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>\n<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>\n</ul>\n<ul class="noprint" id="footer-icons">\n<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>\n<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>\n</ul>\n<div style="clear: both;"></div>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.512","walltime":"0.879","ppvisitednodes":{"value":2273,"limit":1000000},"postexpandincludesize":{"value":40068,"limit":2097152},"templateargumentsize":{"value":3037,"limit":2097152},"expansiondepth":{"value":15,"limit":40},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":23269,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  376.382      1 -total"," 25.94%   97.644      3 Template:Cite_book"," 19.05%   71.705      1 Template:Short_description"," 13.10%   49.292      1 Template:Clarify"," 11.96%   45.030      2 Template:ISBN"," 11.65%   43.845      1 Template:Fix-span"," 11.00%   41.402      1 Template:Machine_learning_bar"," 10.18%   38.331      1 Template:Pagetype","  9.91%   37.306      1 Template:Sidebar_with_collapsible_lists","  7.68%   28.914      4 Template:Category_handler"]},"scribunto":{"limitreport-timeusage":{"value":"0.134","limit":"10.000"},"limitreport-memusage":{"value":4124408,"limit":52428800}},"cachereport":{"origin":"mw2313","timestamp":"20201016045049","ttl":2592000,"transientcontent":false}}});});</script>\n<script type="application/ld+json">{"@context":"https:\\/\\/schema.org","@type":"Article","name":"Online machine learning","url":"https:\\/\\/en.wikipedia.org\\/wiki\\/Online_machine_learning","sameAs":"http:\\/\\/www.wikidata.org\\/entity\\/Q7094097","mainEntity":"http:\\/\\/www.wikidata.org\\/entity\\/Q7094097","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png"}},"datePublished":"2008-10-22T19:43:10Z","dateModified":"2020-09-28T02:33:15Z","headline":"Method of machine learning"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":151,"wgHostname":"mw1271"});});</script>\n</body></html>'