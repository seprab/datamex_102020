b'<!DOCTYPE html>\n\n<html class="client-nojs" dir="ltr" lang="en">\n<head>\n<meta charset="utf8"/>\n<title>Dimensionality reduction - Wikipedia</title>\n<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"e0c26ab2-065b-4273-bf05-8f258a32fb64","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Dimensionality_reduction","wgTitle":"Dimensionality reduction","wgCurRevisionId":985328333,"wgRevisionId":985328333,"wgArticleId":579867,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","Short description matches Wikidata","All articles with unsourced statements","Articles with unsourced statements from September 2017","Articles with unsourced statements from June 2017","Dimension reduction","Machine learning"],\n"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Dimensionality_reduction","wgRelevantArticleId":579867,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q16000077"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options"\n:"loading","ext.cite.styles":"ready","skins.vector.styles.legacy":"ready","mediawiki.toc.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\\\","watchToken":"+\\\\","csrfToken":"+\\\\"});\n});});</script>\n<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>\n<meta content="" name="ResourceLoaderDynamicStyles"/>\n<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<meta content="MediaWiki 1.36.0-wmf.14" name="generator"/>\n<meta content="origin" name="referrer"/>\n<meta content="origin-when-crossorigin" name="referrer"/>\n<meta content="origin-when-cross-origin" name="referrer"/>\n<link href="//en.m.wikipedia.org/wiki/Dimensionality_reduction" media="only screen and (max-width: 720px)" rel="alternate"/>\n<link href="/w/index.php?title=Dimensionality_reduction&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>\n<link href="/w/index.php?title=Dimensionality_reduction&amp;action=edit" rel="edit" title="Edit this page"/>\n<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>\n<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>\n<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>\n<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>\n<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>\n<link href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="canonical"/>\n<link href="//login.wikimedia.org" rel="dns-prefetch"/>\n<link href="//meta.wikimedia.org" rel="dns-prefetch"/>\n</head>\n<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Dimensionality_reduction rootpage-Dimensionality_reduction skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>\n<div class="noprint" id="mw-head-base"></div>\n<div class="mw-body" id="content" role="main">\n<a id="top"></a>\n<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>\n<div class="mw-indicators mw-body-content">\n</div>\n<h1 class="firstHeading" id="firstHeading" lang="en">Dimensionality reduction</h1>\n<div class="mw-body-content" id="bodyContent">\n<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>\n<div id="contentSub"></div>\n<div id="contentSub2"></div>\n<div id="jump-to-nav"></div>\n<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>\n<a class="mw-jump-link" href="#searchInput">Jump to search</a>\n<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Process of reducing the number of random variables under consideration</div>\n<div class="hatnote navigation-not-searchable" role="note">For dimensional reduction in physics, see <a href="/wiki/Dimensional_reduction" title="Dimensional reduction">Dimensional reduction</a>.</div>\n<p><b>Dimensionality reduction</b>, or <b>dimension reduction</b>, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its <a href="/wiki/Intrinsic_dimension" title="Intrinsic dimension">intrinsic dimension</a>. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often <a href="/wiki/Sparse_matrix" title="Sparse matrix">sparse</a> as a consequence of the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>, and analyzing the data is usually <a href="/wiki/Computational_complexity_theory#Intractability" title="Computational complexity theory">computationally intractable</a>. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as <a href="/wiki/Signal_processing" title="Signal processing">signal processing</a>, <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, <a href="/wiki/Neuroinformatics" title="Neuroinformatics">neuroinformatics</a>, and <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>.<sup class="reference" id="cite_ref-dr_review_1-0"><a href="#cite_note-dr_review-1">[1]</a></sup>\n</p><p>Methods are commonly divided into linear and non-linear approaches.<sup class="reference" id="cite_ref-dr_review_1-1"><a href="#cite_note-dr_review-1">[1]</a></sup> Approaches can also be divided into <a href="/wiki/Feature_selection" title="Feature selection">feature selection</a> and <a href="/wiki/Feature_extraction" title="Feature extraction">feature extraction</a>.<sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> Dimensionality reduction can be used for <a href="/wiki/Noise_reduction" title="Noise reduction">noise reduction</a>, <a href="/wiki/Data_visualization" title="Data visualization">data visualization</a>, <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a>, or as an intermediate step to facilitate other analyses. \n</p>\n<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>\n<ul>\n<li class="toclevel-1 tocsection-1"><a href="#Feature_selection"><span class="tocnumber">1</span> <span class="toctext">Feature selection</span></a></li>\n<li class="toclevel-1 tocsection-2"><a href="#Feature_projection"><span class="tocnumber">2</span> <span class="toctext">Feature projection</span></a>\n<ul>\n<li class="toclevel-2 tocsection-3"><a href="#Principal_component_analysis_(PCA)"><span class="tocnumber">2.1</span> <span class="toctext">Principal component analysis (PCA)</span></a></li>\n<li class="toclevel-2 tocsection-4"><a href="#Non-negative_matrix_factorization_(NMF)"><span class="tocnumber">2.2</span> <span class="toctext">Non-negative matrix factorization (NMF)</span></a></li>\n<li class="toclevel-2 tocsection-5"><a href="#Kernel_PCA"><span class="tocnumber">2.3</span> <span class="toctext">Kernel PCA</span></a></li>\n<li class="toclevel-2 tocsection-6"><a href="#Graph-based_kernel_PCA"><span class="tocnumber">2.4</span> <span class="toctext">Graph-based kernel PCA</span></a></li>\n<li class="toclevel-2 tocsection-7"><a href="#Linear_discriminant_analysis_(LDA)"><span class="tocnumber">2.5</span> <span class="toctext">Linear discriminant analysis (LDA)</span></a></li>\n<li class="toclevel-2 tocsection-8"><a href="#Generalized_discriminant_analysis_(GDA)"><span class="tocnumber">2.6</span> <span class="toctext">Generalized discriminant analysis (GDA)</span></a></li>\n<li class="toclevel-2 tocsection-9"><a href="#Autoencoder"><span class="tocnumber">2.7</span> <span class="toctext">Autoencoder</span></a></li>\n<li class="toclevel-2 tocsection-10"><a href="#t-SNE"><span class="tocnumber">2.8</span> <span class="toctext">t-SNE</span></a></li>\n<li class="toclevel-2 tocsection-11"><a href="#UMAP"><span class="tocnumber">2.9</span> <span class="toctext">UMAP</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-12"><a href="#Dimension_reduction"><span class="tocnumber">3</span> <span class="toctext">Dimension reduction</span></a></li>\n<li class="toclevel-1 tocsection-13"><a href="#Applications"><span class="tocnumber">4</span> <span class="toctext">Applications</span></a></li>\n<li class="toclevel-1 tocsection-14"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>\n<li class="toclevel-1 tocsection-15"><a href="#Notes"><span class="tocnumber">6</span> <span class="toctext">Notes</span></a></li>\n<li class="toclevel-1 tocsection-16"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>\n<li class="toclevel-1 tocsection-17"><a href="#External_links"><span class="tocnumber">8</span> <span class="toctext">External links</span></a></li>\n</ul>\n</div>\n<h2><span class="mw-headline" id="Feature_selection">Feature selection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=1" title="Edit section: Feature selection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Feature_selection" title="Feature selection">Feature selection</a></div><div class="hatnote navigation-not-searchable" role="note">See also: <a href="/wiki/Combinatorial_optimization" title="Combinatorial optimization">Combinatorial optimization</a></div>\n<p><a href="/wiki/Feature_selection" title="Feature selection">Feature selection</a> approaches try to find a subset of the input variables (also called features or attributes). The three strategies are: the <i>filter</i> strategy (e.g. <a href="/wiki/Information_gain_in_decision_trees" title="Information gain in decision trees">information gain</a>), the <i>wrapper</i> strategy (e.g. search guided by accuracy), and the <i>embedded</i> strategy (selected features add or are removed while building the model based on prediction errors).\n</p><p><a href="/wiki/Data_analysis" title="Data analysis">Data analysis</a> such as <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> or <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> can be done in the reduced space more accurately than in the original space.<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup>\n</p>\n<h2><span class="mw-headline" id="Feature_projection">Feature projection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=2" title="Edit section: Feature projection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a></div>\n<p>Feature projection (also called Feature extraction) transforms the data from the <a class="mw-redirect" href="/wiki/High-dimensional_space" title="High-dimensional space">high-dimensional space</a> to a space of fewer dimensions. The data transformation may be linear, as in <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA), but many <a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">nonlinear dimensionality reduction</a> techniques also exist.<sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup><sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> For multidimensional data, <a href="/wiki/Tensor" title="Tensor">tensor</a> representation can be used in dimensionality reduction through <a href="/wiki/Multilinear_subspace_learning" title="Multilinear subspace learning">multilinear subspace learning</a>.<sup class="reference" id="cite_ref-MSLsurvey_6-0"><a href="#cite_note-MSLsurvey-6">[6]</a></sup>\n</p>\n<h3><span id="Principal_component_analysis_.28PCA.29"></span><span class="mw-headline" id="Principal_component_analysis_(PCA)">Principal component analysis (PCA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=3" title="Edit section: Principal component analysis (PCA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a></div>\n<p>The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the <a href="/wiki/Covariance" title="Covariance">covariance</a> (and sometimes the <a href="/wiki/Correlation_and_dependence" title="Correlation and dependence">correlation</a>) <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a> of the data is constructed and the <a class="mw-redirect" href="/wiki/Eigenvalue,_eigenvector_and_eigenspace" title="Eigenvalue, eigenvector and eigenspace">eigenvectors</a> on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system\'s energy, especially in low-dimensional systems. Still, this must be proven on a case-by-case basis as not all systems exhibit this behavior.   The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2017)">citation needed</span></a></i>]</sup>\n</p>\n<h3><span id="Non-negative_matrix_factorization_.28NMF.29"></span><span class="mw-headline" id="Non-negative_matrix_factorization_(NMF)">Non-negative matrix factorization (NMF)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=4" title="Edit section: Non-negative matrix factorization (NMF)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">Non-negative matrix factorization</a></div>\n<p>NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist,<sup class="reference" id="cite_ref-lee-seung_7-0"><a href="#cite_note-lee-seung-7">[7]</a></sup><sup class="reference" id="cite_ref-lee2001algorithms_8-0"><a href="#cite_note-lee2001algorithms-8">[8]</a></sup> such as astronomy.<sup class="reference" id="cite_ref-blantonRoweis07_9-0"><a href="#cite_note-blantonRoweis07-9">[9]</a></sup><sup class="reference" id="cite_ref-ren18_10-0"><a href="#cite_note-ren18-10">[10]</a></sup> NMF is well known since the multiplicative update rule by Lee &amp; Seung,<sup class="reference" id="cite_ref-lee-seung_7-1"><a href="#cite_note-lee-seung-7">[7]</a></sup> which has been continuously developed: the inclusion of uncertainties,<sup class="reference" id="cite_ref-blantonRoweis07_9-1"><a href="#cite_note-blantonRoweis07-9">[9]</a></sup> the consideration of missing data and parallel computation,<sup class="reference" id="cite_ref-zhu16_11-0"><a href="#cite_note-zhu16-11">[11]</a></sup> sequential construction<sup class="reference" id="cite_ref-zhu16_11-1"><a href="#cite_note-zhu16-11">[11]</a></sup> which leads to the stability and linearity of NMF,<sup class="reference" id="cite_ref-ren18_10-1"><a href="#cite_note-ren18-10">[10]</a></sup> as well as other <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">updates</a> including handling missing data in <a href="/wiki/Digital_image_processing" title="Digital image processing">digital image processing</a>.<sup class="reference" id="cite_ref-ren20\xe2\x80\x9d_12-0"><a href="#cite_note-ren20\xe2\x80\x9d-12">[12]</a></sup>\n</p><p>With a stable component basis during construction, and a linear modeling process, <a href="/wiki/Non-negative_matrix_factorization#Sequential_NMF" title="Non-negative matrix factorization">sequential NMF</a><sup class="reference" id="cite_ref-zhu16_11-2"><a href="#cite_note-zhu16-11">[11]</a></sup> is able to preserve the flux in direct imaging of circumstellar structures in astromony,<sup class="reference" id="cite_ref-ren18_10-2"><a href="#cite_note-ren18-10">[10]</a></sup> as one of the <a href="/wiki/Methods_of_detecting_exoplanets" title="Methods of detecting exoplanets">methods of detecting exoplanets</a>, especially for the direct imaging of <a class="mw-redirect" href="/wiki/Circumstellar_disks" title="Circumstellar disks">circumstellar disks</a>. In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al.<sup class="reference" id="cite_ref-ren18_10-3"><a href="#cite_note-ren18-10">[10]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Kernel_PCA">Kernel PCA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=5" title="Edit section: Kernel PCA">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a class="mw-redirect" href="/wiki/Kernel_PCA" title="Kernel PCA">Kernel PCA</a></div>\n<p>Principal component analysis can be employed in a nonlinear way by means of the <a class="mw-redirect" href="/wiki/Kernel_trick" title="Kernel trick">kernel trick</a>. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled <a class="mw-redirect" href="/wiki/Kernel_PCA" title="Kernel PCA">kernel PCA</a>.\n</p>\n<h3><span class="mw-headline" id="Graph-based_kernel_PCA">Graph-based kernel PCA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=6" title="Edit section: Graph-based kernel PCA">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Other prominent nonlinear techniques include <a class="mw-redirect" href="/wiki/Manifold_learning" title="Manifold learning">manifold learning</a> techniques such as <a href="/wiki/Isomap" title="Isomap">Isomap</a>, <a class="mw-redirect" href="/wiki/Locally_linear_embedding" title="Locally linear embedding">locally linear embedding</a> (LLE),<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis.<sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup><sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.\n</p><p>More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using <a href="/wiki/Semidefinite_programming" title="Semidefinite programming">semidefinite programming</a>. The most prominent example of such a technique is <a class="mw-redirect" href="/wiki/Maximum_variance_unfolding" title="Maximum variance unfolding">maximum variance unfolding</a> (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.\n</p><p>An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical <a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a>, which is identical to PCA; <a href="/wiki/Isomap" title="Isomap">Isomap</a>, which uses geodesic distances in the data space; <a href="/wiki/Diffusion_map" title="Diffusion map">diffusion maps</a>, which use diffusion distances in the data space; <a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-distributed stochastic neighbor embedding</a> (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.\n</p><p>A different approach to nonlinear dimensionality reduction is through the use of <a href="/wiki/Autoencoder" title="Autoencoder">autoencoders</a>, a special kind of feed-forward <a href="/wiki/Neural_network" title="Neural network">neural networks</a> with a bottle-neck hidden layer.<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup> The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machines</a>) that is followed by a finetuning stage based on <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>.\n</p>\n<h3><span id="Linear_discriminant_analysis_.28LDA.29"></span><span class="mw-headline" id="Linear_discriminant_analysis_(LDA)">Linear discriminant analysis (LDA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=7" title="Edit section: Linear discriminant analysis (LDA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Linear discriminant analysis</a></div>\n<p>Linear discriminant analysis (LDA) is a generalization of Fisher\'s linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.\n</p>\n<h3><span id="Generalized_discriminant_analysis_.28GDA.29"></span><span class="mw-headline" id="Generalized_discriminant_analysis_(GDA)">Generalized discriminant analysis (GDA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=8" title="Edit section: Generalized discriminant analysis (GDA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machines</a> (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space.<sup class="reference" id="cite_ref-gda_17-0"><a href="#cite_note-gda-17">[17]</a></sup><sup class="reference" id="cite_ref-cloudid_18-0"><a href="#cite_note-cloudid-18">[18]</a></sup> Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.\n</p>\n<h3><span class="mw-headline" id="Autoencoder">Autoencoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=9" title="Edit section: Autoencoder">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></div>\n<p>Autoencoders can be used to learn non-linear dimension reduction functions and codings together with an inverse function from the coding to the original representation.\n</p>\n<h3><span class="mw-headline" id="t-SNE">t-SNE</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=10" title="Edit section: t-SNE">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a class="mw-redirect" href="/wiki/TSNE" title="TSNE">tSNE</a></div>\n<p>T-distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique useful for visualization of high-dimensional datasets.\n</p>\n<h3><span class="mw-headline" id="UMAP">UMAP</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=11" title="Edit section: UMAP">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a class="mw-redirect" href="/wiki/Uniform_Manifold_Approximation_and_Projection" title="Uniform Manifold Approximation and Projection">Uniform Manifold Approximation and Projection</a></div>\n<p><a class="mw-redirect" href="/wiki/Uniform_manifold_approximation_and_projection" title="Uniform manifold approximation and projection">Uniform manifold approximation and projection</a> (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a <a class="mw-redirect" href="/wiki/Locally_connected" title="Locally connected">locally connected</a> <a href="/wiki/Riemannian_manifold" title="Riemannian manifold">Riemannian manifold</a> and that the <a class="mw-redirect" href="/wiki/Riemannian_metric" title="Riemannian metric">Riemannian metric</a> is locally constant or approximately locally constant.\n</p>\n<h2><span class="mw-headline" id="Dimension_reduction">Dimension reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=12" title="Edit section: Dimension reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a <a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm">K-nearest neighbors algorithm</a> (k-NN) in order to avoid the effects of the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>.<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup>\n</p><p><a href="/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a> and  dimension reduction can be combined in one step using <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA),  <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">linear discriminant analysis</a> (LDA), <a class="mw-redirect" href="/wiki/Canonical_correlation_analysis" title="Canonical correlation analysis">canonical correlation analysis</a> (CCA), or <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">non-negative matrix factorization</a> (NMF) techniques as a pre-processing step followed by clustering by K-NN on <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature vectors</a> in reduced-dimension space. In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> this process is also called low-dimensional <a href="/wiki/Embedding" title="Embedding">embedding</a>.<sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup>\n</p><p>For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional <a href="/wiki/Time_series" title="Time series">time series</a>) running a fast <b>approximate</b> K-NN search using <a class="mw-redirect" href="/wiki/Locality_sensitive_hashing" title="Locality sensitive hashing">locality sensitive hashing</a>, <a href="/wiki/Random_projection" title="Random projection">random projection</a>,<sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> "sketches" <sup class="reference" id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup> or other high-dimensional similarity search  techniques from the <a class="mw-redirect" href="/wiki/VLDB_conference" title="VLDB conference">VLDB</a> toolbox might be the only feasible option.\n</p>\n<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=13" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>A dimensionality reduction technique that is sometimes used in <a href="/wiki/Neuroscience" title="Neuroscience">neuroscience</a> is <a href="/wiki/Maximally_informative_dimensions" title="Maximally informative dimensions">maximally informative dimensions</a>,<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (June 2017)">citation needed</span></a></i>]</sup> which finds a lower-dimensional representation of a dataset such that as much <a href="/wiki/Mutual_information" title="Mutual information">information</a> as possible about the original data is preserved.\n</p>\n<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=14" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:18.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Recommender_system" title="Recommender system">Recommender systems</a></th></tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">\nConcepts</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">\n<ul><li><a href="/wiki/Collective_intelligence" title="Collective intelligence">Collective intelligence</a></li>\n<li><a href="/wiki/Relevance" title="Relevance">Relevance</a></li>\n<li><a href="/wiki/Star_(classification)" title="Star (classification)">Star ratings</a></li>\n<li><a href="/wiki/Long_tail" title="Long tail">Long tail</a></li></ul></td>\n</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">\nMethods and challenges</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">\n<ul><li><a href="/wiki/Cold_start_(recommender_systems)" title="Cold start (recommender systems)">Cold start</a></li>\n<li><a href="/wiki/Collaborative_filtering" title="Collaborative filtering">Collaborative filtering</a></li>\n<li><a class="mw-selflink selflink">Dimensionality reduction</a></li>\n<li><a href="/wiki/Implicit_data_collection" title="Implicit data collection">Implicit data collection</a></li>\n<li><a href="/wiki/Item-item_collaborative_filtering" title="Item-item collaborative filtering">Item-item collaborative filtering</a></li>\n<li><a href="/wiki/Matrix_factorization_(recommender_systems)" title="Matrix factorization (recommender systems)">Matrix factorization</a></li>\n<li><a href="/wiki/Preference_elicitation" title="Preference elicitation">Preference elicitation</a></li>\n<li><a href="/wiki/Similarity_search" title="Similarity search">Similarity search</a></li></ul></td>\n</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">\nImplementations</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">\n<ul><li><a href="/wiki/Collaborative_search_engine" title="Collaborative search engine">Collaborative search engine</a></li>\n<li><a href="/wiki/Content_discovery_platform" title="Content discovery platform">Content discovery platform</a></li>\n<li><a href="/wiki/Decision_support_system" title="Decision support system">Decision support system</a></li>\n<li><a href="/wiki/Music_Genome_Project" title="Music Genome Project">Music Genome Project</a></li>\n<li><a href="/wiki/Product_finder" title="Product finder">Product finder</a></li></ul></td>\n</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">\nResearch</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">\n<ul><li><a href="/wiki/GroupLens_Research" title="GroupLens Research">GroupLens Research</a></li>\n<li><a href="/wiki/MovieLens" title="MovieLens">MovieLens</a></li>\n<li><a href="/wiki/Netflix_Prize" title="Netflix Prize">Netflix Prize</a></li></ul></td>\n</tr><tr><td style="text-align:right;font-size:115%"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Recommender_systems" title="Template:Recommender systems"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Recommender_systems" title="Template talk:Recommender systems"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Recommender_systems&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<div class="div-col columns column-width" style="-moz-column-width: 20em; -webkit-column-width: 20em; column-width: 20em;">\n<ul><li><a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">Nearest neighbor search</a></li>\n<li><a href="/wiki/MinHash" title="MinHash">MinHash</a></li>\n<li><a href="/wiki/Information_gain_in_decision_trees" title="Information gain in decision trees">Information gain in decision trees</a></li>\n<li><a href="/wiki/Semidefinite_embedding" title="Semidefinite embedding">Semidefinite embedding</a></li>\n<li><a href="/wiki/Multifactor_dimensionality_reduction" title="Multifactor dimensionality reduction">Multifactor dimensionality reduction</a></li>\n<li><a href="/wiki/Multilinear_subspace_learning" title="Multilinear subspace learning">Multilinear subspace learning</a></li>\n<li><a class="mw-redirect" href="/wiki/Multilinear_PCA" title="Multilinear PCA">Multilinear PCA</a></li>\n<li><a href="/wiki/Random_projection" title="Random projection">Random projection</a></li>\n<li><a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">Singular value decomposition</a></li>\n<li><a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">Latent semantic analysis</a></li>\n<li><a href="/wiki/Semantic_mapping_(statistics)" title="Semantic mapping (statistics)">Semantic mapping</a></li>\n<li><a class="mw-redirect" href="/wiki/Tensorsketch" title="Tensorsketch">Tensorsketch</a></li>\n<li><a href="/wiki/Topological_data_analysis" title="Topological data analysis">Topological data analysis</a></li>\n<li><a class="mw-redirect" href="/wiki/Locality_sensitive_hashing" title="Locality sensitive hashing">Locality sensitive hashing</a></li>\n<li><a href="/wiki/Sufficient_dimension_reduction" title="Sufficient dimension reduction">Sufficient dimension reduction</a></li>\n<li><a href="/wiki/Data_transformation_(statistics)" title="Data transformation (statistics)">Data transformation (statistics)</a></li>\n<li><a href="/wiki/Weighted_correlation_network_analysis" title="Weighted correlation network analysis">Weighted correlation network analysis</a></li>\n<li><a href="/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">Hyperparameter optimization</a></li>\n<li><a href="/wiki/CUR_matrix_approximation" title="CUR matrix approximation">CUR matrix approximation</a></li>\n<li>Envelope model</li>\n<li><a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">Nonlinear dimensionality reduction</a></li>\n<li><a href="/wiki/Sammon_mapping" title="Sammon mapping">Sammon mapping</a></li>\n<li><a href="/wiki/Johnson%E2%80%93Lindenstrauss_lemma" title="Johnson\xe2\x80\x93Lindenstrauss lemma">Johnson\xe2\x80\x93Lindenstrauss lemma</a></li>\n<li><a href="/wiki/Local_tangent_space_alignment" title="Local tangent space alignment">Local tangent space alignment</a></li></ul>\n</div>\n<div style="clear:both;"></div>\n<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=15" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="reflist" style="list-style-type: decimal;">\n<div class="mw-references-wrap mw-references-columns"><ol class="references">\n<li id="cite_note-dr_review-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-dr_review_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-dr_review_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFvan_der_MaatenPostmavan_den_Herik2009">van der Maaten, Laurens; Postma, Eric; van den Herik, Jaap (October 26, 2009). <a class="external text" href="https://members.loria.fr/moberger/Enseignement/AVR/Exposes/TR_Dimensiereductie.pdf" rel="nofollow">"Dimensionality Reduction: A Comparative Review"</a> <span class="cs1-format">(PDF)</span>. <i>J Mach Learn Res</i>. <b>10</b>: 66\xe2\x80\x9371.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J+Mach+Learn+Res&amp;rft.atitle=Dimensionality+Reduction%3A+A+Comparative+Review&amp;rft.volume=10&amp;rft.pages=66-71&amp;rft.date=2009-10-26&amp;rft.aulast=van+der+Maaten&amp;rft.aufirst=Laurens&amp;rft.au=Postma%2C+Eric&amp;rft.au=van+den+Herik%2C+Jaap&amp;rft_id=https%3A%2F%2Fmembers.loria.fr%2Fmoberger%2FEnseignement%2FAVR%2FExposes%2FTR_Dimensiereductie.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><style data-mw-deduplicate="TemplateStyles:r982806391">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>\n</li>\n<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFPudilNovovi\xc4\x8dov\xc3\xa11998">Pudil, P.; Novovi\xc4\x8dov\xc3\xa1, J. (1998). "Novel Methods for Feature Subset Selection with Respect to Problem Knowledge".  In Liu, Huan; Motoda, Hiroshi (eds.). <i>Feature Extraction, Construction and Selection</i>. p.\xc2\xa0101. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2F978-1-4615-5725-8_7" rel="nofollow">10.1007/978-1-4615-5725-8_7</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-1-4613-7622-4" title="Special:BookSources/978-1-4613-7622-4"><bdi>978-1-4613-7622-4</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Novel+Methods+for+Feature+Subset+Selection+with+Respect+to+Problem+Knowledge&amp;rft.btitle=Feature+Extraction%2C+Construction+and+Selection&amp;rft.pages=101&amp;rft.date=1998&amp;rft_id=info%3Adoi%2F10.1007%2F978-1-4615-5725-8_7&amp;rft.isbn=978-1-4613-7622-4&amp;rft.aulast=Pudil&amp;rft.aufirst=P.&amp;rft.au=Novovi%C4%8Dov%C3%A1%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFRico-Sulayes2017">Rico-Sulayes, Antonio (2017). <a class="external text" href="http://rielac.cujae.edu.cu/index.php/rieac/article/download/478/278" rel="nofollow">"Reducing Vector Space Dimensionality in Automatic Classification for Authorship Attribution"</a>. <i>Revista Ingenier\xc3\xada Electr\xc3\xb3nica, Autom\xc3\xa1tica y Comunicaciones</i>. <b>38</b> (3): 26\xe2\x80\x9335.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Revista+Ingenier%C3%ADa+Electr%C3%B3nica%2C+Autom%C3%A1tica+y+Comunicaciones&amp;rft.atitle=Reducing+Vector+Space+Dimensionality+in+Automatic+Classification+for+Authorship+Attribution&amp;rft.volume=38&amp;rft.issue=3&amp;rft.pages=26-35&amp;rft.date=2017&amp;rft.aulast=Rico-Sulayes&amp;rft.aufirst=Antonio&amp;rft_id=http%3A%2F%2Frielac.cujae.edu.cu%2Findex.php%2Frieac%2Farticle%2Fdownload%2F478%2F278&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text">Samet, H. (2006) <i>Foundations of Multidimensional and Metric Data Structures</i>. Morgan Kaufmann. <link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/><a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/0-12-369446-9" title="Special:BookSources/0-12-369446-9">0-12-369446-9</a></span>\n</li>\n<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">C. Ding, X. He, H. Zha, H.D. Simon, <a class="external text" href="https://cloudfront.escholarship.org/dist/prd/content/qt8pv153t1/qt8pv153t1.pdf" rel="nofollow">Adaptive Dimension Reduction for Clustering High Dimensional Data</a>, Proceedings of International Conference on Data Mining, 2002</span>\n</li>\n<li id="cite_note-MSLsurvey-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-MSLsurvey_6-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFLuPlataniotisVenetsanopoulos2011">Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). <a class="external text" href="http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf" rel="nofollow">"A Survey of Multilinear Subspace Learning for Tensor Data"</a> <span class="cs1-format">(PDF)</span>. <i>Pattern Recognition</i>. <b>44</b> (7): 1540\xe2\x80\x931551. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.patcog.2011.01.004" rel="nofollow">10.1016/j.patcog.2011.01.004</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition&amp;rft.atitle=A+Survey+of+Multilinear+Subspace+Learning+for+Tensor+Data&amp;rft.volume=44&amp;rft.issue=7&amp;rft.pages=1540-1551&amp;rft.date=2011&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patcog.2011.01.004&amp;rft.aulast=Lu&amp;rft.aufirst=Haiping&amp;rft.au=Plataniotis%2C+K.N.&amp;rft.au=Venetsanopoulos%2C+A.N.&amp;rft_id=http%3A%2F%2Fwww.dsp.utoronto.ca%2F~haiping%2FPublication%2FSurveyMSL_PR2011.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-lee-seung-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-lee-seung_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lee-seung_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFDaniel_D._LeeH._Sebastian_Seung1999">Daniel D. Lee &amp; <a href="/wiki/Sebastian_Seung" title="Sebastian Seung">H. Sebastian Seung</a> (1999). "Learning the parts of objects by non-negative matrix factorization". <i><a href="/wiki/Nature_(journal)" title="Nature (journal)">Nature</a></i>. <b>401</b> (6755): 788\xe2\x80\x93791. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/1999Natur.401..788L" rel="nofollow">1999Natur.401..788L</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1038%2F44565" rel="nofollow">10.1038/44565</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>\xc2\xa0<a class="external text" href="//pubmed.ncbi.nlm.nih.gov/10548103" rel="nofollow">10548103</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Learning+the+parts+of+objects+by+non-negative+matrix+factorization&amp;rft.volume=401&amp;rft.issue=6755&amp;rft.pages=788-791&amp;rft.date=1999&amp;rft_id=info%3Apmid%2F10548103&amp;rft_id=info%3Adoi%2F10.1038%2F44565&amp;rft_id=info%3Abibcode%2F1999Natur.401..788L&amp;rft.au=Daniel+D.+Lee&amp;rft.au=H.+Sebastian+Seung&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-lee2001algorithms-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-lee2001algorithms_8-0">^</a></b></span> <span class="reference-text"><cite class="citation conference cs1" id="CITEREFDaniel_D._LeeH._Sebastian_Seung2001">Daniel D. Lee &amp; H. Sebastian Seung (2001). <a class="external text" href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf" rel="nofollow"><i>Algorithms for Non-negative Matrix Factorization</i></a> <span class="cs1-format">(PDF)</span>. Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a>. pp.\xc2\xa0556\xe2\x80\x93562.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Algorithms+for+Non-negative+Matrix+Factorization&amp;rft.pages=556-562&amp;rft.pub=MIT+Press&amp;rft.date=2001&amp;rft.au=Daniel+D.+Lee&amp;rft.au=H.+Sebastian+Seung&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F1861-algorithms-for-non-negative-matrix-factorization.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-blantonRoweis07-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-blantonRoweis07_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-blantonRoweis07_9-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBlantonRoweis2007">Blanton, Michael R.; Roweis, Sam (2007). "K-corrections and filter transformations in the ultraviolet, optical, and near infrared". <i>The Astronomical Journal</i>. <b>133</b> (2): 734\xe2\x80\x93754. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/astro-ph/0606170" rel="nofollow">astro-ph/0606170</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2007AJ....133..734B" rel="nofollow">2007AJ....133..734B</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1086%2F510127" rel="nofollow">10.1086/510127</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astronomical+Journal&amp;rft.atitle=K-corrections+and+filter+transformations+in+the+ultraviolet%2C+optical%2C+and+near+infrared&amp;rft.volume=133&amp;rft.issue=2&amp;rft.pages=734-754&amp;rft.date=2007&amp;rft_id=info%3Aarxiv%2Fastro-ph%2F0606170&amp;rft_id=info%3Adoi%2F10.1086%2F510127&amp;rft_id=info%3Abibcode%2F2007AJ....133..734B&amp;rft.aulast=Blanton&amp;rft.aufirst=Michael+R.&amp;rft.au=Roweis%2C+Sam&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-ren18-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-ren18_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ren18_10-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ren18_10-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-ren18_10-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFRenPueyoZhuDuch\xc3\xaane2018">Ren, Bin; Pueyo, Laurent; Zhu, Guangtun B.; Duch\xc3\xaane, Gaspard (2018). "Non-negative Matrix Factorization: Robust Extraction of Extended Structures". <i>The Astrophysical Journal</i>. <b>852</b> (2): 104. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1712.10317" rel="nofollow">1712.10317</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2018ApJ...852..104R" rel="nofollow">2018ApJ...852..104R</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.3847%2F1538-4357%2Faaa1f2" rel="nofollow">10.3847/1538-4357/aaa1f2</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astrophysical+Journal&amp;rft.atitle=Non-negative+Matrix+Factorization%3A+Robust+Extraction+of+Extended+Structures&amp;rft.volume=852&amp;rft.issue=2&amp;rft.pages=104&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1712.10317&amp;rft_id=info%3Adoi%2F10.3847%2F1538-4357%2Faaa1f2&amp;rft_id=info%3Abibcode%2F2018ApJ...852..104R&amp;rft.aulast=Ren&amp;rft.aufirst=Bin&amp;rft.au=Pueyo%2C+Laurent&amp;rft.au=Zhu%2C+Guangtun+B.&amp;rft.au=Duch%C3%AAne%2C+Gaspard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-zhu16-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-zhu16_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-zhu16_11-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-zhu16_11-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"> <cite class="citation arxiv cs1" id="CITEREFZhu2016">Zhu, Guangtun B. (2016-12-19). "Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1612.06037" rel="nofollow">1612.06037</a></span> [<a class="external text" href="//arxiv.org/archive/astro-ph.IM" rel="nofollow">astro-ph.IM</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Nonnegative+Matrix+Factorization+%28NMF%29+with+Heteroscedastic+Uncertainties+and+Missing+data&amp;rft.date=2016-12-19&amp;rft_id=info%3Aarxiv%2F1612.06037&amp;rft.aulast=Zhu&amp;rft.aufirst=Guangtun+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-ren20\xe2\x80\x9d-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-ren20\xe2\x80\x9d_12-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFRenPueyoChenChoquet2020">Ren, Bin; Pueyo, Laurent; Chen, Christine; Choquet, Elodie; Debes, John H.; Duechene, Gaspard; Menard, Francois; Perrin, Marshall D. (2020). "Using Data Imputation for Signal Separation in High Contrast Imaging". <i>The Astrophysical Journal</i>. <b>892</b> (2): 74. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/2001.00563" rel="nofollow">2001.00563</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2020ApJ...892...74R" rel="nofollow">2020ApJ...892...74R</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.3847%2F1538-4357%2Fab7024" rel="nofollow">10.3847/1538-4357/ab7024</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astrophysical+Journal&amp;rft.atitle=Using+Data+Imputation+for+Signal+Separation+in+High+Contrast+Imaging&amp;rft.volume=892&amp;rft.issue=2&amp;rft.pages=74&amp;rft.date=2020&amp;rft_id=info%3Aarxiv%2F2001.00563&amp;rft_id=info%3Adoi%2F10.3847%2F1538-4357%2Fab7024&amp;rft_id=info%3Abibcode%2F2020ApJ...892...74R&amp;rft.aulast=Ren&amp;rft.aufirst=Bin&amp;rft.au=Pueyo%2C+Laurent&amp;rft.au=Chen%2C+Christine&amp;rft.au=Choquet%2C+Elodie&amp;rft.au=Debes%2C+John+H.&amp;rft.au=Duechene%2C+Gaspard&amp;rft.au=Menard%2C+Francois&amp;rft.au=Perrin%2C+Marshall+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFRoweisSaul2000">Roweis, S. T.; Saul, L. K. (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". <i>Science</i>. <b>290</b> (5500): 2323\xe2\x80\x932326. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2000Sci...290.2323R" rel="nofollow">2000Sci...290.2323R</a>. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a>\xc2\xa0<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.3313" rel="nofollow">10.1.1.111.3313</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1126%2Fscience.290.5500.2323" rel="nofollow">10.1126/science.290.5500.2323</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>\xc2\xa0<a class="external text" href="//pubmed.ncbi.nlm.nih.gov/11125150" rel="nofollow">11125150</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Nonlinear+Dimensionality+Reduction+by+Locally+Linear+Embedding&amp;rft.volume=290&amp;rft.issue=5500&amp;rft.pages=2323-2326&amp;rft.date=2000&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.111.3313&amp;rft_id=info%3Apmid%2F11125150&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.290.5500.2323&amp;rft_id=info%3Abibcode%2F2000Sci...290.2323R&amp;rft.aulast=Roweis&amp;rft.aufirst=S.+T.&amp;rft.au=Saul%2C+L.+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFZhangZha2004">Zhang, Zhenyue; Zha, Hongyuan (2004). "Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment". <i>SIAM Journal on Scientific Computing</i>. <b>26</b> (1): 313\xe2\x80\x93338. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1137%2Fs1064827502419154" rel="nofollow">10.1137/s1064827502419154</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Journal+on+Scientific+Computing&amp;rft.atitle=Principal+Manifolds+and+Nonlinear+Dimensionality+Reduction+via+Tangent+Space+Alignment&amp;rft.volume=26&amp;rft.issue=1&amp;rft.pages=313-338&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1137%2Fs1064827502419154&amp;rft.aulast=Zhang&amp;rft.aufirst=Zhenyue&amp;rft.au=Zha%2C+Hongyuan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBengioMonperrusLarochelle2006">Bengio, Yoshua; Monperrus, Martin; Larochelle, Hugo (2006). <a class="external text" href="https://hal.archives-ouvertes.fr/hal-01575345/document" rel="nofollow">"Nonlocal Estimation of Manifold Structure"</a>. <i>Neural Computation</i>. <b>18</b> (10): 2509\xe2\x80\x932528. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a>\xc2\xa0<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.4230" rel="nofollow">10.1.1.116.4230</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1162%2Fneco.2006.18.10.2509" rel="nofollow">10.1162/neco.2006.18.10.2509</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>\xc2\xa0<a class="external text" href="//pubmed.ncbi.nlm.nih.gov/16907635" rel="nofollow">16907635</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Nonlocal+Estimation+of+Manifold+Structure&amp;rft.volume=18&amp;rft.issue=10&amp;rft.pages=2509-2528&amp;rft.date=2006&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.116.4230&amp;rft_id=info%3Apmid%2F16907635&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.10.2509&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft.au=Monperrus%2C+Martin&amp;rft.au=Larochelle%2C+Hugo&amp;rft_id=https%3A%2F%2Fhal.archives-ouvertes.fr%2Fhal-01575345%2Fdocument&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">Hongbing Hu, Stephen A. Zahorian, (2010) <a class="external text" href="http://bingweb.binghamton.edu/~hhu1/paper/Hu2010Dimensionality.pdf" rel="nofollow">"Dimensionality Reduction Methods for HMM Phonetic Recognition,"</a> ICASSP 2010, Dallas, TX</span>\n</li>\n<li id="cite_note-gda-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-gda_17-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBaudatAnouar2000">Baudat, G.; Anouar, F. (2000). "Generalized Discriminant Analysis Using a Kernel Approach". <i>Neural Computation</i>. <b>12</b> (10): 2385\xe2\x80\x932404. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a>\xc2\xa0<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.412.760" rel="nofollow">10.1.1.412.760</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1162%2F089976600300014980" rel="nofollow">10.1162/089976600300014980</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>\xc2\xa0<a class="external text" href="//pubmed.ncbi.nlm.nih.gov/11032039" rel="nofollow">11032039</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Generalized+Discriminant+Analysis+Using+a+Kernel+Approach&amp;rft.volume=12&amp;rft.issue=10&amp;rft.pages=2385-2404&amp;rft.date=2000&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.412.760&amp;rft_id=info%3Apmid%2F11032039&amp;rft_id=info%3Adoi%2F10.1162%2F089976600300014980&amp;rft.aulast=Baudat&amp;rft.aufirst=G.&amp;rft.au=Anouar%2C+F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-cloudid-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-cloudid_18-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFHaghighatZonouzAbdel-Mottaleb2015">Haghighat, Mohammad; Zonouz, Saman; Abdel-Mottaleb, Mohamed (2015). "CloudID: Trustworthy cloud-based and cross-enterprise biometric identification". <i>Expert Systems with Applications</i>. <b>42</b> (21): 7905\xe2\x80\x937916. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.eswa.2015.06.025" rel="nofollow">10.1016/j.eswa.2015.06.025</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Expert+Systems+with+Applications&amp;rft.atitle=CloudID%3A+Trustworthy+cloud-based+and+cross-enterprise+biometric+identification&amp;rft.volume=42&amp;rft.issue=21&amp;rft.pages=7905-7916&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1016%2Fj.eswa.2015.06.025&amp;rft.aulast=Haghighat&amp;rft.aufirst=Mohammad&amp;rft.au=Zonouz%2C+Saman&amp;rft.au=Abdel-Mottaleb%2C+Mohamed&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft (1999) <a class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.1422" rel="nofollow">"When is \xe2\x80\x9cnearest neighbor\xe2\x80\x9d meaningful?"</a>. <i>Database Theory\xe2\x80\x94ICDT99</i>,  217\xe2\x80\x93235</span>\n</li>\n<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFShawJebara2009">Shaw, B.; Jebara, T. (2009). <a class="external text" href="https://www.cs.columbia.edu/~jebara/papers/spe-icml09.pdf" rel="nofollow">"Structure preserving embedding"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of the 26th Annual International Conference on Machine Learning \xe2\x80\x93 ICML \'09</i>. p.\xc2\xa01. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a>\xc2\xa0<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.161.451" rel="nofollow">10.1.1.161.451</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1145%2F1553374.1553494" rel="nofollow">10.1145/1553374.1553494</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/9781605585161" title="Special:BookSources/9781605585161"><bdi>9781605585161</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Structure+preserving+embedding&amp;rft.btitle=Proceedings+of+the+26th+Annual+International+Conference+on+Machine+Learning+%E2%80%93+ICML+%2709&amp;rft.pages=1&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.161.451&amp;rft_id=info%3Adoi%2F10.1145%2F1553374.1553494&amp;rft.isbn=9781605585161&amp;rft.aulast=Shaw&amp;rft.aufirst=B.&amp;rft.au=Jebara%2C+T.&amp;rft_id=https%3A%2F%2Fwww.cs.columbia.edu%2F~jebara%2Fpapers%2Fspe-icml09.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFBinghamMannila2001">Bingham, E.; Mannila, H. (2001). "Random projection in dimensionality reduction". <i>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining \xe2\x80\x93 KDD \'01</i>. p.\xc2\xa0245. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1145%2F502512.502546" rel="nofollow">10.1145/502512.502546</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-1581133912" title="Special:BookSources/978-1581133912"><bdi>978-1581133912</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Random+projection+in+dimensionality+reduction&amp;rft.btitle=Proceedings+of+the+seventh+ACM+SIGKDD+international+conference+on+Knowledge+discovery+and+data+mining+%E2%80%93+KDD+%2701&amp;rft.pages=245&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1145%2F502512.502546&amp;rft.isbn=978-1581133912&amp;rft.aulast=Bingham&amp;rft.aufirst=E.&amp;rft.au=Mannila%2C+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text">Shasha, D High (2004) <i>Performance Discovery in Time Series</i> Berlin: Springer. <link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/><a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/0-387-00857-8" title="Special:BookSources/0-387-00857-8">0-387-00857-8</a></span>\n</li>\n</ol></div></div>\n<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=16" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<style data-mw-deduplicate="TemplateStyles:r886047268">.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}</style><div class="refbegin reflist" style="">\n<ul><li><cite class="citation book cs1" id="CITEREFBoehmkeGreenwell2019">Boehmke, Brad; Greenwell, Brandon M. (2019). <a class="external text" href="https://books.google.com/books?id=aXC9DwAAQBAJ&amp;pg=PA343" rel="nofollow">"Dimension Reduction"</a>. <i>Hands-On Machine Learning with R</i>. Chapman &amp; Hall. pp.\xc2\xa0343\xe2\x80\x93396. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-1-138-49568-5" title="Special:BookSources/978-1-138-49568-5"><bdi>978-1-138-49568-5</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Dimension+Reduction&amp;rft.btitle=Hands-On+Machine+Learning+with+R&amp;rft.pages=343-396&amp;rft.pub=Chapman+%26+Hall&amp;rft.date=2019&amp;rft.isbn=978-1-138-49568-5&amp;rft.aulast=Boehmke&amp;rft.aufirst=Brad&amp;rft.au=Greenwell%2C+Brandon+M.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DaXC9DwAAQBAJ%26pg%3DPA343&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></li>\n<li><cite class="citation techreport cs1" id="CITEREFFodor2002">Fodor, I. (2002). <a class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.8.5098" rel="nofollow"><i>A survey of dimension reduction techniques</i></a> (Technical report). Center for Applied Scientific Computing, Lawrence Livermore National. UCRL-ID-148494.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=A+survey+of+dimension+reduction+techniques&amp;rft.pub=Center+for+Applied+Scientific+Computing%2C+Lawrence+Livermore+National&amp;rft.date=2002&amp;rft.aulast=Fodor&amp;rft.aufirst=I.&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fversions%3Fdoi%3D10.1.1.8.5098&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></li>\n<li><cite class="citation techreport cs1" id="CITEREFCunningham2007">Cunningham, P. (2007). <a class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.98.1478" rel="nofollow"><i>Dimension Reduction</i></a> (Technical report). University College Dublin. UCD-CSI-2007-7.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Dimension+Reduction&amp;rft.pub=University+College+Dublin&amp;rft.date=2007&amp;rft.aulast=Cunningham&amp;rft.aufirst=P.&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.98.1478&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></li>\n<li><cite class="citation book cs1" id="CITEREFLakshmi_PadmajaVishnuvardhan2016">Lakshmi Padmaja, Dhyaram; Vishnuvardhan, B (2016). "Comparative Study of Feature Subset Selection Methods for Dimensionality Reduction on Scientific Data". <i>2016 IEEE 6th International Conference on Advanced Computing (IACC)</i>. pp.\xc2\xa031\xe2\x80\x9334. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FIACC.2016.16" rel="nofollow">10.1109/IACC.2016.16</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-1-4673-8286-1" title="Special:BookSources/978-1-4673-8286-1"><bdi>978-1-4673-8286-1</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Comparative+Study+of+Feature+Subset+Selection+Methods+for+Dimensionality+Reduction+on+Scientific+Data&amp;rft.btitle=2016+IEEE+6th+International+Conference+on+Advanced+Computing+%28IACC%29&amp;rft.pages=31-34&amp;rft.date=2016&amp;rft_id=info%3Adoi%2F10.1109%2FIACC.2016.16&amp;rft.isbn=978-1-4673-8286-1&amp;rft.aulast=Lakshmi+Padmaja&amp;rft.aufirst=Dhyaram&amp;rft.au=Vishnuvardhan%2C+B&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></li></ul>\n</div>\n<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=17" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><a class="external text" href="http://jmlr.csail.mit.edu/papers/special/feature03.html" rel="nofollow">JMLR Special Issue on Variable and Feature Selection</a></li>\n<li><a class="external text" href="http://bioinfo-out.curie.fr/projects/elmap/" rel="nofollow">ELastic MAPs</a></li>\n<li><a class="external text" href="http://www.cs.toronto.edu/~roweis/lle" rel="nofollow">Locally Linear Embedding</a></li>\n<li><a class="external text" href="https://web.archive.org/web/20040411051530/http://isomap.stanford.edu/" rel="nofollow">A Global Geometric Framework for Nonlinear Dimensionality Reduction</a></li></ul>\n<!-- \nNewPP limit report\nParsed by mw1393\nCached time: 20201029223148\nCache expiry: 2592000\nDynamic content: false\nComplications: [vary\xe2\x80\x90revision\xe2\x80\x90sha1]\nCPU time usage: 0.464 seconds\nReal time usage: 0.619 seconds\nPreprocessor visited node count: 2417/1000000\nPost\xe2\x80\x90expand include size: 62612/2097152 bytes\nTemplate argument size: 3043/2097152 bytes\nHighest expansion depth: 16/40\nExpensive parser function count: 2/500\nUnstrip recursion depth: 1/20\nUnstrip post\xe2\x80\x90expand size: 77636/5000000 bytes\nLua time usage: 0.233/10.000 seconds\nLua memory usage: 6.61 MB/50 MB\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  550.178      1 -total\n 50.53%  277.992      1 Template:Reflist\n 28.40%  156.235     12 Template:Cite_journal\n 13.30%   73.198      1 Template:Short_description\n 11.32%   62.272      2 Template:Citation_needed\n  9.48%   52.161      2 Template:ISBN\n  9.23%   50.786      2 Template:Fix\n  7.27%   40.014      1 Template:Pagetype\n  5.59%   30.739      5 Template:Cite_book\n  5.47%   30.103      4 Template:Category_handler\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:579867-0!canonical and timestamp 20201029223147 and revision id 985328333\n -->\n</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>\n<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;oldid=985328333">https://en.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;oldid=985328333</a>"</div></div>\n<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_September_2017" title="Category:Articles with unsourced statements from September 2017">Articles with unsourced statements from September 2017</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_June_2017" title="Category:Articles with unsourced statements from June 2017">Articles with unsourced statements from June 2017</a></li></ul></div></div>\n</div>\n</div>\n<div id="mw-data-after-content">\n<div class="read-more-container"></div>\n</div>\n<div id="mw-navigation">\n<h2>Navigation menu</h2>\n<div id="mw-head">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-personal-label" class="mw-portlet mw-portlet-personal vector-menu" id="p-personal" role="navigation">\n<h3 id="p-personal-label">\n<span>Personal tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Dimensionality+reduction" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Dimensionality+reduction" title="You\'re encouraged to log in; however, it\'s not mandatory. [o]">Log in</a></li></ul>\n</div>\n</nav>\n<div id="left-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-namespaces-label" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" id="p-namespaces" role="navigation">\n<h3 id="p-namespaces-label">\n<span>Namespaces</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Dimensionality_reduction" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Dimensionality_reduction" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-variants-label" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu vector-menu-dropdown" id="p-variants" role="navigation">\n<input aria-labelledby="p-variants-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-variants-label">\n<span>Variants</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n</div>\n<div id="right-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-views-label" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" id="p-views" role="navigation">\n<h3 id="p-views-label">\n<span>Views</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/Dimensionality_reduction">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=Dimensionality_reduction&amp;action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=Dimensionality_reduction&amp;action=history" title="Past revisions of this page [h]">View history</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-cactions-label" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu vector-menu-dropdown" id="p-cactions" role="navigation">\n<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-cactions-label">\n<span>More</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n<div id="p-search" role="search">\n<h3>\n<label for="searchInput">Search</label>\n</h3>\n<form action="/w/index.php" id="searchform">\n<div data-search-loc="header-navigation" id="simpleSearch">\n<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>\n<input name="title" type="hidden" value="Special:Search"/>\n<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">\n<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>\n</input></div>\n</form>\n</div>\n</div>\n</div>\n<div id="mw-panel">\n<div id="p-logo" role="banner">\n<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>\n</div>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-navigation-label" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">\n<h3 id="p-navigation-label">\n<span>Navigation</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Articles related to current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-interaction-label" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">\n<h3 id="p-interaction-label">\n<span>Contribute</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-introduction"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia">Learn to edit</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-tb-label" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" id="p-tb" role="navigation">\n<h3 id="p-tb-label">\n<span>Tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Dimensionality_reduction" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Dimensionality_reduction" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Dimensionality_reduction&amp;oldid=985328333" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Dimensionality_reduction&amp;action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Dimensionality_reduction&amp;id=985328333&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q16000077" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-coll-print_export-label" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">\n<h3 id="p-coll-print_export-label">\n<span>Print/export</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Dimensionality_reduction&amp;action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Dimensionality_reduction&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-lang-label" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" id="p-lang" role="navigation">\n<h3 id="p-lang-label">\n<span>Languages</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-es"><a class="interlanguage-link-target" href="https://es.wikipedia.org/wiki/Reducci%C3%B3n_de_dimensionalidad" hreflang="es" lang="es" title="Reducci\xc3\xb3n de dimensionalidad \xe2\x80\x93 Spanish">Espa\xc3\xb1ol</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%DA%A9%D8%A7%D9%87%D8%B4_%D8%A7%D8%A8%D8%B9%D8%A7%D8%AF" hreflang="fa" lang="fa" title="\xda\xa9\xd8\xa7\xd9\x87\xd8\xb4 \xd8\xa7\xd8\xa8\xd8\xb9\xd8\xa7\xd8\xaf \xe2\x80\x93 Persian">\xd9\x81\xd8\xa7\xd8\xb1\xd8\xb3\xdb\x8c</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/R%C3%A9duction_de_la_dimensionnalit%C3%A9" hreflang="fr" lang="fr" title="R\xc3\xa9duction de la dimensionnalit\xc3\xa9 \xe2\x80\x93 French">Fran\xc3\xa7ais</a></li><li class="interlanguage-link interwiki-ko"><a class="interlanguage-link-target" href="https://ko.wikipedia.org/wiki/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C" hreflang="ko" lang="ko" title="\xec\xb0\xa8\xec\x9b\x90\xec\xb6\x95\xec\x86\x8c \xe2\x80\x93 Korean">\xed\x95\x9c\xea\xb5\xad\xec\x96\xb4</a></li><li class="interlanguage-link interwiki-he"><a class="interlanguage-link-target" href="https://he.wikipedia.org/wiki/%D7%94%D7%95%D7%A8%D7%93%D7%AA_%D7%9E%D7%9E%D7%93" hreflang="he" lang="he" title="\xd7\x94\xd7\x95\xd7\xa8\xd7\x93\xd7\xaa \xd7\x9e\xd7\x9e\xd7\x93 \xe2\x80\x93 Hebrew">\xd7\xa2\xd7\x91\xd7\xa8\xd7\x99\xd7\xaa</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%A1%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8" hreflang="ru" lang="ru" title="\xd0\xa1\xd0\xbd\xd0\xb8\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xe2\x80\x93 Russian">\xd0\xa0\xd1\x83\xd1\x81\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9</a></li><li class="interlanguage-link interwiki-tr"><a class="interlanguage-link-target" href="https://tr.wikipedia.org/wiki/Boyut_indirgeme" hreflang="tr" lang="tr" title="Boyut indirgeme \xe2\x80\x93 Turkish">T\xc3\xbcrk\xc3\xa7e</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%97%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%BD%D1%8F_%D1%80%D0%BE%D0%B7%D0%BC%D1%96%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%96" hreflang="uk" lang="uk" title="\xd0\x97\xd0\xbd\xd0\xb8\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8f \xd1\x80\xd0\xbe\xd0\xb7\xd0\xbc\xd1\x96\xd1\x80\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd1\x96 \xe2\x80\x93 Ukrainian">\xd0\xa3\xd0\xba\xd1\x80\xd0\xb0\xd1\x97\xd0\xbd\xd1\x81\xd1\x8c\xd0\xba\xd0\xb0</a></li><li class="interlanguage-link interwiki-zh-yue"><a class="interlanguage-link-target" href="https://zh-yue.wikipedia.org/wiki/%E9%99%8D%E7%B6%AD" hreflang="yue" lang="yue" title="\xe9\x99\x8d\xe7\xb6\xad \xe2\x80\x93 Cantonese">\xe7\xb2\xb5\xe8\xaa\x9e</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/%E9%99%8D%E7%BB%B4" hreflang="zh" lang="zh" title="\xe9\x99\x8d\xe7\xbb\xb4 \xe2\x80\x93 Chinese">\xe4\xb8\xad\xe6\x96\x87</a></li></ul>\n<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q16000077#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class="mw-footer" id="footer" role="contentinfo">\n<ul id="footer-info">\n<li id="footer-info-lastmod"> This page was last edited on 25 October 2020, at 09:35<span class="anonymous-show">\xc2\xa0(UTC)</span>.</li>\n<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia\xc2\xae is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id="footer-places">\n<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>\n<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>\n<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>\n<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>\n<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>\n<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>\n<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>\n<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>\n</ul>\n<ul class="noprint" id="footer-icons">\n<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>\n<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>\n</ul>\n<div style="clear: both;"></div>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.464","walltime":"0.619","ppvisitednodes":{"value":2417,"limit":1000000},"postexpandincludesize":{"value":62612,"limit":2097152},"templateargumentsize":{"value":3043,"limit":2097152},"expansiondepth":{"value":16,"limit":40},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":77636,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  550.178      1 -total"," 50.53%  277.992      1 Template:Reflist"," 28.40%  156.235     12 Template:Cite_journal"," 13.30%   73.198      1 Template:Short_description"," 11.32%   62.272      2 Template:Citation_needed","  9.48%   52.161      2 Template:ISBN","  9.23%   50.786      2 Template:Fix","  7.27%   40.014      1 Template:Pagetype","  5.59%   30.739      5 Template:Cite_book","  5.47%   30.103      4 Template:Category_handler"]},"scribunto":{"limitreport-timeusage":{"value":"0.233","limit":"10.000"},"limitreport-memusage":{"value":6931267,"limit":52428800}},"cachereport":{"origin":"mw1393","timestamp":"20201029223148","ttl":2592000,"transientcontent":false}}});});</script>\n<script type="application/ld+json">{"@context":"https:\\/\\/schema.org","@type":"Article","name":"Dimensionality reduction","url":"https:\\/\\/en.wikipedia.org\\/wiki\\/Dimensionality_reduction","sameAs":"http:\\/\\/www.wikidata.org\\/entity\\/Q16000077","mainEntity":"http:\\/\\/www.wikidata.org\\/entity\\/Q16000077","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png"}},"datePublished":"2004-04-06T15:10:25Z","dateModified":"2020-10-25T09:35:17Z","headline":"process of reducing the number of random variables under consideration"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":770,"wgHostname":"mw1393"});});</script>\n</body></html>'