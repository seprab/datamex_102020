b'<!DOCTYPE html>\n\n<html class="client-nojs" dir="ltr" lang="en">\n<head>\n<meta charset="utf8"/>\n<title>Perceptron - Wikipedia</title>\n<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"735a6978-a1ed-4efd-8faf-909d2e8c68b0","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Perceptron","wgTitle":"Perceptron","wgCurRevisionId":984516045,"wgRevisionId":984516045,"wgArticleId":172777,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Wikipedia articles with NDL identifiers","Articles with example Python (programming language) code","Classification algorithms","Artificial neural networks"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Perceptron","wgRelevantArticleId":172777,"wgIsProbablyEditable":\n!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q690207"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","skins.vector.styles.legacy":"ready","mediawiki.toc.styles":"ready",\n"ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\\\","watchToken":"+\\\\","csrfToken":"+\\\\"});\n});});</script>\n<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>\n<meta content="" name="ResourceLoaderDynamicStyles"/>\n<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<meta content="MediaWiki 1.36.0-wmf.14" name="generator"/>\n<meta content="origin" name="referrer"/>\n<meta content="origin-when-crossorigin" name="referrer"/>\n<meta content="origin-when-cross-origin" name="referrer"/>\n<link href="//en.m.wikipedia.org/wiki/Perceptron" media="only screen and (max-width: 720px)" rel="alternate"/>\n<link href="/w/index.php?title=Perceptron&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>\n<link href="/w/index.php?title=Perceptron&amp;action=edit" rel="edit" title="Edit this page"/>\n<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>\n<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>\n<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>\n<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>\n<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>\n<link href="https://en.wikipedia.org/wiki/Perceptron" rel="canonical"/>\n<link href="//login.wikimedia.org" rel="dns-prefetch"/>\n<link href="//meta.wikimedia.org" rel="dns-prefetch"/>\n</head>\n<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Perceptron rootpage-Perceptron skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>\n<div class="noprint" id="mw-head-base"></div>\n<div class="mw-body" id="content" role="main">\n<a id="top"></a>\n<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>\n<div class="mw-indicators mw-body-content">\n</div>\n<h1 class="firstHeading" id="firstHeading" lang="en">Perceptron</h1>\n<div class="mw-body-content" id="bodyContent">\n<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>\n<div id="contentSub"></div>\n<div id="contentSub2"></div>\n<div id="jump-to-nav"></div>\n<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>\n<a class="mw-jump-link" href="#searchInput">Jump to search</a>\n<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="hatnote navigation-not-searchable" role="note">"Perceptrons" redirects here. For the 1969 book, see <a href="/wiki/Perceptrons_(book)" title="Perceptrons (book)">Perceptrons (book)</a>.</div>\n<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em">Part of a series on</td></tr><tr><th style="padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br/>and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>\n<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>\n<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>\n<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>\n<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>\n<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>\n<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>\n<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>\n<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>\n<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>\n<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>\n<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>\xc2\xa0\xe2\x80\xa2 <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>\n<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>\n<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>\n<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>\n<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>\n<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>\n<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>\n<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>\n<li><a class="mw-selflink selflink">Perceptron</a></li>\n<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>\n<li><a class="mw-redirect" href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>\n<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>\n<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>\n<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>\n<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation\xe2\x80\x93maximization algorithm">Expectation\xe2\x80\x93maximization (EM)</a></li>\n<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>\n<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>\n<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>\n<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>\n<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>\n<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>\n<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>\n<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>\n<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>\n<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>\n<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>\n<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>\n<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>\n<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>\n<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>\n<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>\n<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>\n<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>\n<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>\n<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li></ul></li>\n<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>\n<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>\n<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>\n<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>\n<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>\n<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias\xe2\x80\x93variance dilemma">Bias\xe2\x80\x93variance dilemma</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>\n<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>\n<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>\n<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>\n<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>\n<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik\xe2\x80\x93Chervonenkis theory">VC theory</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>\n<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>\n<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>\n<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>\n<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>\n<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p>In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, the <b>perceptron</b> is an algorithm for <a class="mw-redirect" href="/wiki/Supervised_classification" title="Supervised classification">supervised learning</a> of <a href="/wiki/Binary_classification" title="Binary classification">binary classifiers</a>.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.<sup class="reference" id="cite_ref-largemargin_1-0"><a href="#cite_note-largemargin-1">[1]</a></sup>  It is a type of <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>, i.e. a classification algorithm that makes its predictions based on a <a href="/wiki/Linear_predictor_function" title="Linear predictor function">linear predictor function</a> combining a set of weights with the <a class="mw-redirect" href="/wiki/Feature_vector" title="Feature vector">feature vector</a>.\n</p>\n<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>\n<ul>\n<li class="toclevel-1 tocsection-1"><a href="#History"><span class="tocnumber">1</span> <span class="toctext">History</span></a></li>\n<li class="toclevel-1 tocsection-2"><a href="#Definition"><span class="tocnumber">2</span> <span class="toctext">Definition</span></a></li>\n<li class="toclevel-1 tocsection-3"><a href="#Learning_algorithm"><span class="tocnumber">3</span> <span class="toctext">Learning algorithm</span></a>\n<ul>\n<li class="toclevel-2 tocsection-4"><a href="#Definitions"><span class="tocnumber">3.1</span> <span class="toctext">Definitions</span></a></li>\n<li class="toclevel-2 tocsection-5"><a href="#Steps"><span class="tocnumber">3.2</span> <span class="toctext">Steps</span></a></li>\n<li class="toclevel-2 tocsection-6"><a href="#Convergence"><span class="tocnumber">3.3</span> <span class="toctext">Convergence</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-7"><a href="#Variants"><span class="tocnumber">4</span> <span class="toctext">Variants</span></a></li>\n<li class="toclevel-1 tocsection-8"><a href="#Multiclass_perceptron"><span class="tocnumber">5</span> <span class="toctext">Multiclass perceptron</span></a></li>\n<li class="toclevel-1 tocsection-9"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>\n<li class="toclevel-1 tocsection-10"><a href="#Further_reading"><span class="tocnumber">7</span> <span class="toctext">Further reading</span></a></li>\n<li class="toclevel-1 tocsection-11"><a href="#External_links"><span class="tocnumber">8</span> <span class="toctext">External links</span></a></li>\n</ul>\n</div>\n<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=1" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="thumb tleft"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Mark_I_perceptron.jpeg"><img alt="" class="thumbimage" data-file-height="350" data-file-width="285" decoding="async" height="270" src="//upload.wikimedia.org/wikipedia/en/thumb/5/52/Mark_I_perceptron.jpeg/220px-Mark_I_perceptron.jpeg" srcset="//upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg 1.5x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Mark_I_perceptron.jpeg" title="Enlarge"></a></div>Mark I Perceptron machine, the first implementation of the perceptron algorithm. It was connected to a camera with 20\xc3\x9720 <a href="/wiki/Cadmium_sulfide" title="Cadmium sulfide">cadmium sulfide</a> <a class="mw-redirect" href="/wiki/Photocell" title="Photocell">photocells</a> to make a 400-pixel image. The main visible feature is a patch panel that set different combinations of input features. To the right, arrays of <a href="/wiki/Potentiometer" title="Potentiometer">potentiometers</a> that implemented the adaptive weights.<sup class="reference" id="cite_ref-bishop_2-0"><a href="#cite_note-bishop-2">[2]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>213</span></sup></div></div></div>\n<div class="hatnote navigation-not-searchable" role="note">See also: <a href="/wiki/History_of_artificial_intelligence#Perceptrons_and_the_dark_age_of_connectionism" title="History of artificial intelligence">History of artificial intelligence \xc2\xa7\xc2\xa0Perceptrons and the dark age of connectionism</a>, and <a href="/wiki/AI_winter#The_abandonment_of_connectionism_in_1969" title="AI winter">AI winter \xc2\xa7\xc2\xa0The abandonment of connectionism in 1969</a></div>\n<p>The perceptron algorithm was invented in 1958 at the <a class="mw-redirect" href="/wiki/Cornell_Aeronautical_Laboratory" title="Cornell Aeronautical Laboratory">Cornell Aeronautical Laboratory</a> by <a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a>,<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup> funded by the United States <a href="/wiki/Office_of_Naval_Research" title="Office of Naval Research">Office of Naval Research</a>.<sup class="reference" id="cite_ref-Olazaran_4-0"><a href="#cite_note-Olazaran-4">[4]</a></sup>\n</p><p>The perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the <a href="/wiki/IBM_704" title="IBM 704">IBM 704</a>, it was subsequently implemented in custom-built hardware as the "Mark 1 perceptron". This machine was designed for <a class="mw-redirect" href="/wiki/Image_recognition" title="Image recognition">image recognition</a>: it had an array of 400 <a class="mw-redirect" href="/wiki/Photocell" title="Photocell">photocells</a>, randomly connected to the "neurons". Weights were encoded in <a href="/wiki/Potentiometer" title="Potentiometer">potentiometers</a>, and weight updates during learning were performed by electric motors.<sup class="reference" id="cite_ref-bishop_2-1"><a href="#cite_note-bishop-2">[2]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>193</span></sup>\n</p><p>In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">AI</a> community; based on Rosenblatt\'s statements, <i><a href="/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i> reported the perceptron to be "the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence."<sup class="reference" id="cite_ref-Olazaran_4-1"><a href="#cite_note-Olazaran-4">[4]</a></sup>\n</p><p>Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural network</a> with two or more layers (also called a <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptron</a>) had greater processing power than perceptrons with one layer (also called a <a href="/wiki/Feedforward_neural_network#Single-layer_perceptron" title="Feedforward neural network">single layer perceptron</a>).\n</p><p>Single layer perceptrons are only capable of learning <a class="mw-redirect" href="/wiki/Linearly_separable" title="Linearly separable">linearly separable</a> patterns. For a classification task with some step activation function a single node will have a single line dividing the data points forming the patterns. More nodes can create more dividing lines, but those lines must somehow be combined to form more complex classifications. A second layer of perceptrons, or even linear nodes, are sufficient to solve a lot of otherwise non-separable problems.\n</p><p>In 1969 a famous book entitled <i><a href="/wiki/Perceptrons_(book)" title="Perceptrons (book)">Perceptrons</a></i> by <a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a> and <a href="/wiki/Seymour_Papert" title="Seymour Papert">Seymour Papert</a> showed that it was impossible for these classes of network to learn an <a class="mw-redirect" href="/wiki/XOR" title="XOR">XOR</a> function. It is often believed (incorrectly) that they also conjectured that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on <i><a href="/wiki/Perceptrons_(book)" title="Perceptrons (book)">Perceptrons (book)</a></i> for more information.)  Nevertheless, the often-miscited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until <a href="/wiki/Neural_network" title="Neural network">neural network</a> research experienced a resurgence in the 1980s.  This text was reprinted in 1987 as "Perceptrons - Expanded Edition" where some errors in the original text are shown and corrected.\n</p><p>The <a href="/wiki/Kernel_perceptron" title="Kernel perceptron">kernel perceptron</a> algorithm was already introduced in 1964 by Aizerman et al.<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> Margin bounds guarantees were given for the Perceptron algorithm in the general non-separable case first by <a href="/wiki/Yoav_Freund" title="Yoav Freund">Freund</a> and <a href="/wiki/Robert_Schapire" title="Robert Schapire">Schapire</a> (1998),<sup class="reference" id="cite_ref-largemargin_1-1"><a href="#cite_note-largemargin-1">[1]</a></sup> and more recently by <a href="/wiki/Mehryar_Mohri" title="Mehryar Mohri">Mohri</a> and Rostamizadeh (2013) who extend previous results and give new L1 bounds.<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup>\n</p><p>The perceptron is a simplified model of a biological <a href="/wiki/Neuron" title="Neuron">neuron</a>. While the complexity of <a href="/wiki/Biological_neuron_model" title="Biological neuron model">biological neuron models</a> is often required to fully understand neural behavior, research suggests a perceptron-like linear model can produce some behavior seen in real neurons.<sup class="reference" id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>\n</p>\n<h2><span class="mw-headline" id="Definition">Definition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=2" title="Edit section: Definition">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>In the modern sense, the perceptron is an algorithm for learning a binary classifier called a <a href="/wiki/Linear_classifier#Definition" title="Linear classifier">threshold function</a>: a function that maps its input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} }</annotation>\n</semantics>\n</math></span><img alt="\\mathbf {x} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;"/></span> (a real-valued <a href="/wiki/Vector_space" title="Vector space">vector</a>) to an output value <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f(\\mathbf {x} )}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f(\\mathbf {x} )}</annotation>\n</semantics>\n</math></span><img alt="f(\\mathbf {x} )" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e41ea95e6949bf4cef6426116364ba87e0fdcd60" style="vertical-align: -0.838ex; width:4.499ex; height:2.843ex;"/></span> (a single <a href="/wiki/Binary_function" title="Binary function">binary</a> value):\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f(\\mathbf {x} )={\\begin{cases}1&amp;{\\text{if }}\\ \\mathbf {w} \\cdot \\mathbf {x} +b&gt;0,\\\\0&amp;{\\text{otherwise}}\\end{cases}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow>\n<mo>{</mo>\n<mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em">\n<mtr>\n<mtd>\n<mn>1</mn>\n</mtd>\n<mtd>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>if\xc2\xa0</mtext>\n</mrow>\n<mtext>\xc2\xa0</mtext>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mo>+</mo>\n<mi>b</mi>\n<mo>&gt;</mo>\n<mn>0</mn>\n<mo>,</mo>\n</mtd>\n</mtr>\n<mtr>\n<mtd>\n<mn>0</mn>\n</mtd>\n<mtd>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>otherwise</mtext>\n</mrow>\n</mtd>\n</mtr>\n</mtable>\n<mo fence="true" stretchy="true" symmetric="true"></mo>\n</mrow>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f(\\mathbf {x} )={\\begin{cases}1&amp;{\\text{if }}\\ \\mathbf {w} \\cdot \\mathbf {x} +b&gt;0,\\\\0&amp;{\\text{otherwise}}\\end{cases}}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle f(\\mathbf {x} )={\\begin{cases}1&amp;{\\text{if }}\\ \\mathbf {w} \\cdot \\mathbf {x} +b&gt;0,\\\\0&amp;{\\text{otherwise}}\\end{cases}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c27b30e07934b4fc8f346ec6fafd5b077d0d4efc" style="vertical-align: -2.505ex; width:29.864ex; height:6.176ex;"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {w} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {w} }</annotation>\n</semantics>\n</math></span><img alt="\\mathbf {w} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/20795664b5b048744a2fd88977851104cc5816f8" style="vertical-align: -0.338ex; width:1.931ex; height:1.676ex;"/></span> is a vector of real-valued weights, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c208362eb2cb54b1837a8c80a2c097e14f34d7ff" style="vertical-align: -0.338ex; width:5.021ex; height:1.676ex;"/></span> is the <a href="/wiki/Dot_product" title="Dot product">dot product</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\sum _{i=1}^{m}w_{i}x_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>m</mi>\n</mrow>\n</munderover>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\sum _{i=1}^{m}w_{i}x_{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\sum _{i=1}^{m}w_{i}x_{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bf43c01ee8403ea39c2f6d2829576c1769a100d7" style="vertical-align: -3.005ex; width:8.335ex; height:6.843ex;"/></span>, where <span class="texhtml mvar" style="font-style:italic;">m</span> is the number of inputs to the perceptron, and <span class="texhtml mvar" style="font-style:italic;">b</span> is the <i>bias</i>. The bias shifts the decision boundary away from the origin and does not depend on any input value.\n</p><p>The value of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f(\\mathbf {x} )}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f(\\mathbf {x} )}</annotation>\n</semantics>\n</math></span><img alt="f(\\mathbf {x} )" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e41ea95e6949bf4cef6426116364ba87e0fdcd60" style="vertical-align: -0.838ex; width:4.499ex; height:2.843ex;"/></span> (0 or 1) is used to classify <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} }</annotation>\n</semantics>\n</math></span><img alt="\\mathbf {x} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;"/></span> as either a positive or a negative instance, in the case of a binary classification problem. If <span class="texhtml mvar" style="font-style:italic;">b</span> is negative, then the weighted combination of inputs must produce a positive value greater than <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle |b|}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mi>b</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle |b|}</annotation>\n</semantics>\n</math></span><img alt="|b|" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/881f49e94388a46a05d329251551ce20baf4f05d" style="vertical-align: -0.838ex; width:2.291ex; height:2.843ex;"/></span> in order to push the classifier neuron over the 0 threshold. Spatially, the bias alters the position (though not the orientation) of the <a href="/wiki/Decision_boundary" title="Decision boundary">decision boundary</a>. The perceptron learning algorithm does not terminate if the learning set is not <a class="mw-redirect" href="/wiki/Linearly_separable" title="Linearly separable">linearly separable</a>. If the vectors are not linearly separable learning will never reach a point where all vectors are classified properly. The most famous example of the perceptron\'s inability to solve problems with linearly nonseparable vectors is the Boolean <a class="mw-redirect" href="/wiki/Exclusive-or" title="Exclusive-or">exclusive-or</a> problem. The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in the reference.<sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>\n</p><p>In the context of neural networks, a perceptron is an <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neuron</a> using the <a href="/wiki/Heaviside_step_function" title="Heaviside step function">Heaviside step function</a> as the activation function. The perceptron algorithm is also termed the <b>single-layer perceptron</b>, to distinguish it from a <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptron</a>, which is a misnomer for a more complicated neural network.  As a linear classifier, the single-layer perceptron is the simplest <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural network</a>.\n</p>\n<h2><span class="mw-headline" id="Learning_algorithm">Learning algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=3" title="Edit section: Learning algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>Below is an example of a learning algorithm for a single-layer perceptron. For <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a>, where a hidden layer exists, more sophisticated algorithms such as <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> must be used. If the activation function or the underlying process being modeled by the perceptron is <a href="/wiki/Nonlinear_system" title="Nonlinear system">nonlinear</a>, alternative learning algorithms such as the <a href="/wiki/Delta_rule" title="Delta rule">delta rule</a> can be used as long as the activation function is <a href="/wiki/Differentiable_function" title="Differentiable function">differentiable</a>. Nonetheless, the learning algorithm described in the steps below will often work, even for multilayer perceptrons with nonlinear activation functions.\n</p><p>When multiple perceptrons are combined in an artificial neural network, each output neuron operates independently of all the others; thus, learning each output can be considered in isolation.\n</p>\n<div class="thumb tright"><div class="thumbinner" style="width:502px;"><a class="image" href="/wiki/File:Perceptron_example.svg"><img alt="" class="thumbimage" data-file-height="1224" data-file-width="1224" decoding="async" height="500" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/500px-Perceptron_example.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/750px-Perceptron_example.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/1000px-Perceptron_example.svg.png 2x" width="500"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Perceptron_example.svg" title="Enlarge"></a></div>A diagram showing a perceptron updating its linear boundary as more training examples are added.</div></div></div>\n<h3><span class="mw-headline" id="Definitions">Definitions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=4" title="Edit section: Definitions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>We first define some variables:\n</p>\n<ul><li>r is the learning rate of the perceptron. Learning rate is between 0 and 1, larger values make the weight changes more volatile.</li>\n<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle y=f(\\mathbf {z} )}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>y</mi>\n<mo>=</mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">z</mi>\n</mrow>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle y=f(\\mathbf {z} )}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle y=f(\\mathbf {z} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/50743880bc5e43723c312d141edcb652ddeaa759" style="vertical-align: -0.838ex; width:8.53ex; height:2.843ex;"/></span> denotes the <i>output</i> from the perceptron for an input vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {z} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">z</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {z} }</annotation>\n</semantics>\n</math></span><img alt="\\mathbf {z} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82eca5d0928078d5a61b9e7e98cc73db31070909" style="vertical-align: -0.338ex; width:1.188ex; height:1.676ex;"/></span>.</li>\n<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle D=\\{(\\mathbf {x} _{1},d_{1}),\\dots ,(\\mathbf {x} _{s},d_{s})\\}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>D</mi>\n<mo>=</mo>\n<mo fence="false" stretchy="false">{</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>,</mo>\n<mo>\xe2\x80\xa6<!-- \xe2\x80\xa6 --></mo>\n<mo>,</mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>s</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>s</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo fence="false" stretchy="false">}</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle D=\\{(\\mathbf {x} _{1},d_{1}),\\dots ,(\\mathbf {x} _{s},d_{s})\\}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle D=\\{(\\mathbf {x} _{1},d_{1}),\\dots ,(\\mathbf {x} _{s},d_{s})\\}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0cc14983f3260966730b1bdbe8c68f197706f4b0" style="vertical-align: -0.838ex; width:27.567ex; height:2.843ex;"/></span> is the <i>training set</i> of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>s</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s}</annotation>\n</semantics>\n</math></span><img alt="s" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;"/></span> samples, where:\n<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {x} _{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} _{j}}</annotation>\n</semantics>\n</math></span><img alt="\\mathbf {x} _{j}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/da7e57d3f8c537992b45488f9586aec0c35a85f0" style="vertical-align: -1.005ex; width:2.321ex; height:2.343ex;"/></span> is the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle n}</annotation>\n</semantics>\n</math></span><img alt="n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span>-dimensional input vector.</li>\n<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle d_{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle d_{j}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle d_{j}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3fa3426b07cfa37c76382ddbecfb4c880889657f" style="vertical-align: -1.005ex; width:2.119ex; height:2.843ex;"/></span> is the desired output value of the perceptron for that input.</li></ul></li></ul>\n<p>We show the values of the features as follows:\n</p>\n<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x_{j,i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>,</mo>\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x_{j,i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle x_{j,i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f552a62dddcb50f846dcd67d522d59d28f5e56d2" style="vertical-align: -1.005ex; width:3.264ex; height:2.343ex;"/></span> is the value of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>th feature of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>j</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle j}</annotation>\n</semantics>\n</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span>th training <i>input vector</i>.</li>\n<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x_{j,0}=1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>,</mo>\n<mn>0</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x_{j,0}=1}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle x_{j,0}=1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cda1bc48502a2676e9a67b6b5fad8c59e08ce571" style="vertical-align: -1.005ex; width:7.78ex; height:2.843ex;"/></span>.</li></ul>\n<p>To represent the weights: \n</p>\n<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fe22f0329d3ecb2e1880d44d191aba0e5475db68" style="vertical-align: -0.671ex; width:2.464ex; height:2.009ex;"/></span> is the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>th value in the <i>weight vector</i>, to be multiplied by the value of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>th input feature.</li>\n<li>Because <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x_{j,0}=1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>,</mo>\n<mn>0</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x_{j,0}=1}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle x_{j,0}=1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cda1bc48502a2676e9a67b6b5fad8c59e08ce571" style="vertical-align: -1.005ex; width:7.78ex; height:2.843ex;"/></span>, the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{0}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>0</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{0}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{0}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7aa052386ec49846179aa8bbe2b279b57a675e00" style="vertical-align: -0.671ex; width:2.718ex; height:2.009ex;"/></span> is effectively a bias that we use instead of the bias constant <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle b}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>b</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle b}</annotation>\n</semantics>\n</math></span><img alt="b" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f11423fbb2e967f986e36804a8ae4271734917c3" style="vertical-align: -0.338ex; width:0.998ex; height:2.176ex;"/></span>.</li></ul>\n<p>To show the time-dependence of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {w} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {w} }</annotation>\n</semantics>\n</math></span><img alt="\\mathbf {w} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/20795664b5b048744a2fd88977851104cc5816f8" style="vertical-align: -0.338ex; width:1.931ex; height:1.676ex;"/></span>, we use:\n</p>\n<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{i}(t)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{i}(t)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{i}(t)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a41e8d0d6f38f875bc34ae9b05e200c0d53ef0c3" style="vertical-align: -0.838ex; width:5.113ex; height:2.843ex;"/></span> is the weight <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> at time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>t</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle t}</annotation>\n</semantics>\n</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span>.</li></ul>\n<div class="thumb tright"><div class="thumbinner" style="width:502px;"><a class="image" href="/wiki/File:Perceptron.svg"><img alt="" class="thumbimage" data-file-height="744" data-file-width="1052" decoding="async" height="354" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/31/Perceptron.svg/500px-Perceptron.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/31/Perceptron.svg/750px-Perceptron.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/31/Perceptron.svg/1000px-Perceptron.svg.png 2x" width="500"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Perceptron.svg" title="Enlarge"></a></div>The appropriate weights are applied to the inputs, and the resulting weighted sum passed to a function that produces the output o.</div></div></div>\n<h3><span class="mw-headline" id="Steps">Steps</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=5" title="Edit section: Steps">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<div><ol><li>Initialize the weights and the threshold. Weights may be initialized to 0 or to a small random value. In the example below, we use 0.</li><li>For each example <span class="texhtml mvar" style="font-style:italic;">j</span> in our training set <span class="texhtml mvar" style="font-style:italic;">D</span>, perform the following steps over the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {x} _{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} _{j}}</annotation>\n</semantics>\n</math></span><img alt="{\\mathbf  {x}}_{j}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/da7e57d3f8c537992b45488f9586aec0c35a85f0" style="vertical-align: -1.005ex; width:2.321ex; height:2.343ex;"/></span> and desired output <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle d_{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle d_{j}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle d_{j}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3fa3426b07cfa37c76382ddbecfb4c880889657f" style="vertical-align: -1.005ex; width:2.119ex; height:2.843ex;"/></span>:\n<div><ol style="list-style-type:lower-alpha"><li>Calculate the actual output:\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\begin{aligned}y_{j}(t)&amp;=f[\\mathbf {w} (t)\\cdot \\mathbf {x} _{j}]\\\\&amp;=f[w_{0}(t)x_{j,0}+w_{1}(t)x_{j,1}+w_{2}(t)x_{j,2}+\\dotsb +w_{n}(t)x_{j,n}]\\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">\n<mtr>\n<mtd>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n</mtd>\n<mtd>\n<mi></mi>\n<mo>=</mo>\n<mi>f</mi>\n<mo stretchy="false">[</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">]</mo>\n</mtd>\n</mtr>\n<mtr>\n<mtd></mtd>\n<mtd>\n<mi></mi>\n<mo>=</mo>\n<mi>f</mi>\n<mo stretchy="false">[</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>0</mn>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>,</mo>\n<mn>0</mn>\n</mrow>\n</msub>\n<mo>+</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>1</mn>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>,</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>+</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>,</mo>\n<mn>2</mn>\n</mrow>\n</msub>\n<mo>+</mo>\n<mo>\xe2\x8b\xaf<!-- \xe2\x8b\xaf --></mo>\n<mo>+</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>,</mo>\n<mi>n</mi>\n</mrow>\n</msub>\n<mo stretchy="false">]</mo>\n</mtd>\n</mtr>\n</mtable>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\begin{aligned}y_{j}(t)&amp;=f[\\mathbf {w} (t)\\cdot \\mathbf {x} _{j}]\\\\&amp;=f[w_{0}(t)x_{j,0}+w_{1}(t)x_{j,1}+w_{2}(t)x_{j,2}+\\dotsb +w_{n}(t)x_{j,n}]\\end{aligned}}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle {\\begin{aligned}y_{j}(t)&amp;=f[\\mathbf {w} (t)\\cdot \\mathbf {x} _{j}]\\\\&amp;=f[w_{0}(t)x_{j,0}+w_{1}(t)x_{j,1}+w_{2}(t)x_{j,2}+\\dotsb +w_{n}(t)x_{j,n}]\\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8e2650d5fbcec4f1b38ada11b50a95014aefbd6b" style="vertical-align: -2.311ex; margin-bottom: -0.194ex; width:61.078ex; height:6.176ex;"/></span></dd></dl></li><li>Update the weights:\n<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{i}(t+1)=w_{i}(t)+r\\cdot (d_{j}-y_{j}(t))x_{j,i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n<mo>+</mo>\n<mi>r</mi>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n<mo stretchy="false">)</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>,</mo>\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{i}(t+1)=w_{i}(t)+r\\cdot (d_{j}-y_{j}(t))x_{j,i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle w_{i}(t+1)=w_{i}(t)+r\\cdot (d_{j}-y_{j}(t))x_{j,i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/57cd4d46c1a546c97ed106d62df828a0cdb91242" style="vertical-align: -1.005ex; width:37.625ex; height:3.009ex;"/></span>, for all features <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle 0\\leq i\\leq n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mn>0</mn>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mi>i</mi>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle 0\\leq i\\leq n}</annotation>\n</semantics>\n</math></span><img alt="0\\leq i\\leq n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/db879b8b15adbedaf379f6f5c5bceab41e47052b" style="vertical-align: -0.505ex; width:9.557ex; height:2.343ex;"/></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle r}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>r</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle r}</annotation>\n</semantics>\n</math></span><img alt="r" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d1ecb613aa2984f0576f70f86650b7c2a132538" style="vertical-align: -0.338ex; width:1.049ex; height:1.676ex;"/></span> is the <a href="/wiki/Learning_rate" title="Learning rate">learning rate</a>.</dd></li></ol></div></li><li>For <a href="/wiki/Offline_learning" title="Offline learning">offline learning</a>, the second step may be repeated until the iteration error <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\frac {1}{s}}\\sum _{j=1}^{s}|d_{j}-y_{j}(t)|}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mn>1</mn>\n<mi>s</mi>\n</mfrac>\n</mrow>\n<munderover>\n<mo>\xe2\x88\x91<!-- \xe2\x88\x91 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n<mo>=</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>s</mi>\n</mrow>\n</munderover>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msub>\n<mi>y</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<mi>t</mi>\n<mo stretchy="false">)</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\frac {1}{s}}\\sum _{j=1}^{s}|d_{j}-y_{j}(t)|}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle {\\frac {1}{s}}\\sum _{j=1}^{s}|d_{j}-y_{j}(t)|}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/52d6809b0682721f6f29485e14003f97dccf0e46" style="vertical-align: -3.338ex; width:17.078ex; height:7.176ex;"/></span> is less than a user-specified error threshold <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma }</annotation>\n</semantics>\n</math></span><img alt="\\gamma " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a223c880b0ce3da8f64ee33c4f0010beee400b1a" style="vertical-align: -0.838ex; width:1.262ex; height:2.176ex;"/></span>, or a predetermined number of iterations have been completed, where <i>s</i> is again the size of the sample set.</li></ol></div>\n<p>The algorithm updates the weights after steps 2a and 2b. These weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps.\n</p>\n<h3><span class="mw-headline" id="Convergence">Convergence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=6" title="Edit section: Convergence">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>The perceptron is a <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>, therefore it will never get to the state with all the input vectors classified correctly if the training set <span class="texhtml mvar" style="font-style:italic;">D</span> is not <a class="mw-redirect" href="/wiki/Linearly_separable" title="Linearly separable">linearly separable</a>, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane. In this case, no "approximate" solution will be gradually approached under the standard learning algorithm, but instead, learning will fail completely. Hence, if linear separability of the training set is not known a priori, one of the training variants below should be used.\n</p><p>If the training set <i>is</i> linearly separable, then the perceptron is guaranteed to converge.<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Furthermore, there is an upper bound on the number of times the perceptron will adjust its weights during the training.\n</p><p>Suppose that the input vectors from the two classes can be separated by a hyperplane with a margin <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma }</annotation>\n</semantics>\n</math></span><img alt="\\gamma " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a223c880b0ce3da8f64ee33c4f0010beee400b1a" style="vertical-align: -0.838ex; width:1.262ex; height:2.176ex;"/></span>, i.e. there exists a weight vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {w} ,||\\mathbf {w} ||=1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n<mo>,</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mo stretchy="false">|</mo>\n</mrow>\n<mo>=</mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {w} ,||\\mathbf {w} ||=1}</annotation>\n</semantics>\n</math></span><img alt="\\mathbf {w} ,||\\mathbf {w} ||=1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4a781bdb14ed8cb1f8602ffaed0e435e8c4972b" style="vertical-align: -0.838ex; width:11.745ex; height:2.843ex;"/></span>, and a bias term <span class="texhtml mvar" style="font-style:italic;">b</span> such that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{j}&gt;\\gamma }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>&gt;</mo>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{j}&gt;\\gamma }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{j}&gt;\\gamma }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ac2c20ebc8cb445c80afb46b93539f72f0d5ec1" style="vertical-align: -1.005ex; width:10.292ex; height:2.509ex;"/></span> for all <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>j</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle j}</annotation>\n</semantics>\n</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span> with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle d_{j}=1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle d_{j}=1}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle d_{j}=1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/76ad7eb7e947b7f18b4792c50e41f7a1510c905f" style="vertical-align: -1.005ex; width:6.38ex; height:2.843ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{j}&lt;-\\gamma }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">w</mi>\n</mrow>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="bold">x</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>&lt;</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{j}&lt;-\\gamma }</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{j}&lt;-\\gamma }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c60aefd969d8b541f30c7c6f33a7b55c929a80b8" style="vertical-align: -1.005ex; width:12.1ex; height:2.676ex;"/></span> for all <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>j</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle j}</annotation>\n</semantics>\n</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span> with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle d_{j}=0}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>0</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle d_{j}=0}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle d_{j}=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/afa9299c9262e5da1af4b67d501a7089fc0bcdca" style="vertical-align: -1.005ex; width:6.38ex; height:2.843ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle d_{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle d_{j}}</annotation>\n</semantics>\n</math></span><img alt="d_{j}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3fa3426b07cfa37c76382ddbecfb4c880889657f" style="vertical-align: -1.005ex; width:2.119ex; height:2.843ex;"/></span> is the desired output value of the perceptron for input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>j</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle j}</annotation>\n</semantics>\n</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span>. Also, let <span class="texhtml mvar" style="font-style:italic;">R</span> denote the maximum norm of an input vector. Novikoff (1962) proved that in this case the perceptron algorithm converges after making <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(R^{2}/\\gamma ^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>R</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mrow class="MJX-TeXAtom-ORD">\n<mo>/</mo>\n</mrow>\n<msup>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(R^{2}/\\gamma ^{2})}</annotation>\n</semantics>\n</math></span><img alt="O(R^{2}/\\gamma ^{2})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d174cb837dc3b404e96ac5fbf3051cfa10e4e377" style="vertical-align: -0.838ex; width:9.897ex; height:3.176ex;"/></span> updates. The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction with which it has a negative <a href="/wiki/Dot_product" title="Dot product">dot product</a>, and thus can be bounded above by <span class="texhtml"><i>O</i>(<span class="nowrap">\xe2\x88\x9a<span style="border-top:1px solid; padding:0 0.1em;"><i>t</i></span></span>)</span>, where <span class="texhtml mvar" style="font-style:italic;">t</span> is the number of changes to the weight vector. However, it can also be bounded below by <span class="texhtml"><i>O</i>(<i>t</i>)</span> because if there exists an (unknown) satisfactory weight vector, then every change makes progress in this (unknown) direction by a positive amount that depends only on the input vector.\n</p>\n<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a class="image" href="/wiki/File:Perceptron_cant_choose.svg"><img alt="" class="thumbimage" data-file-height="540" data-file-width="720" decoding="async" height="225" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Perceptron_cant_choose.svg/300px-Perceptron_cant_choose.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Perceptron_cant_choose.svg/450px-Perceptron_cant_choose.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Perceptron_cant_choose.svg/600px-Perceptron_cant_choose.svg.png 2x" width="300"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Perceptron_cant_choose.svg" title="Enlarge"></a></div>Two classes of points, and two of the infinitely many linear boundaries that separate them. Even though the boundaries are at nearly right angles to one another, the perceptron algorithm has no way of choosing between them.</div></div></div>\n<p>While the perceptron algorithm is guaranteed to converge on <i>some</i> solution in the case of a linearly separable training set, it may still pick <i>any</i> solution and problems may admit many solutions of varying quality.<sup class="reference" id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup> The <i>perceptron of optimal stability</i>, nowadays better known as the linear <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machine</a>, was designed to solve this problem (Krauth and Mezard, 1987).<sup class="reference" id="cite_ref-KrauthMezard87_11-0"><a href="#cite_note-KrauthMezard87-11">[11]</a></sup>\n</p>\n<h2><span class="mw-headline" id="Variants">Variants</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=7" title="Edit section: Variants">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far "in its pocket". The pocket algorithm then returns the solution in the pocket, rather than the last solution. It can be used also for non-separable data sets, where the aim is to find a perceptron with a small number of misclassifications. However, these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning, nor are they guaranteed to show up within a given number of learning steps.\n</p><p>The Maxover algorithm (Wendemuth, 1995) is <a href="/wiki/Robustness_(computer_science)" title="Robustness (computer science)">"robust"</a> in the sense that it will converge regardless of (prior) knowledge of linear separability of the data set.<sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> In the linearly separable case, it will solve the training problem \xe2\x80\x93 if desired, even with optimal stability (<a href="/wiki/Hyperplane_separation_theorem" title="Hyperplane separation theorem">maximum margin</a> between the classes). For non-separable data sets, it will return a solution with a small number of misclassifications. In all cases, the algorithm gradually approaches the solution in the course of learning, without memorizing previous states and without stochastic jumps. Convergence is to global optimality for separable data sets and to local optimality for non-separable data sets.\n</p><p>The Voted Perceptron (Freund and Schapire, 1999), is a variant using multiple weighted perceptrons. The algorithm starts a new perceptron every time an example is wrongly classified, initializing the weights vector with the final weights of the last perceptron. Each perceptron will also be given another weight corresponding to how many examples do they correctly classify before wrongly classifying one, and at the end the output will be a weighted vote on all perceptrons.\n</p><p>In separable problems, perceptron training can also aim at finding the largest separating margin between the classes. The so-called perceptron of optimal stability can be determined by means of iterative training and optimization schemes, such as the Min-Over algorithm (Krauth and Mezard, 1987)<sup class="reference" id="cite_ref-KrauthMezard87_11-1"><a href="#cite_note-KrauthMezard87-11">[11]</a></sup>  or the AdaTron (Anlauf and Biehl, 1989)).<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> AdaTron uses the fact that the corresponding quadratic optimization problem is convex. The perceptron of optimal stability, together with the <a class="mw-redirect" href="/wiki/Kernel_trick" title="Kernel trick">kernel trick</a>, are the conceptual foundations of the <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machine</a>.\n</p><p>The <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\alpha }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\alpha }</annotation>\n</semantics>\n</math></span><img alt="\\alpha " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3" style="vertical-align: -0.338ex; width:1.488ex; height:1.676ex;"/></span>-perceptron further used a pre-processing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify <a class="extiw" href="https://en.wiktionary.org/wiki/analogue" title="wiktionary:analogue">analogue</a> patterns, by projecting them into a <a class="mw-redirect" href="/wiki/Binary_Space_Partition" title="Binary Space Partition">binary space</a>. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.\n</p><p>Another way to solve nonlinear problems without using multiple layers is to use higher order networks (sigma-pi unit). In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an <i>n</i>-order network.\n</p><p>It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal, and the nonlinear solution is <a href="/wiki/Overfitting" title="Overfitting">overfitted</a>.\n</p><p>Other linear classification algorithms include <a href="/wiki/Winnow_(algorithm)" title="Winnow (algorithm)">Winnow</a>, <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machine</a> and <a href="/wiki/Logistic_regression" title="Logistic regression">logistic regression</a>.\n</p>\n<h2><span class="mw-headline" id="Multiclass_perceptron">Multiclass perceptron</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=8" title="Edit section: Multiclass perceptron">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>Like most other techniques for training linear classifiers, the perceptron generalizes naturally to <a href="/wiki/Multiclass_classification" title="Multiclass classification">multiclass classification</a>.  Here, the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>x</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x}</annotation>\n</semantics>\n</math></span><img alt="x" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;"/></span> and the output <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle y}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>y</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle y}</annotation>\n</semantics>\n</math></span><img alt="y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;"/></span> are drawn from arbitrary sets. A feature representation function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f(x,y)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f(x,y)}</annotation>\n</semantics>\n</math></span><img alt="f(x,y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/29473ed0c4e838ac9dbe074535e507166c0e9101" style="vertical-align: -0.838ex; width:6.607ex; height:2.843ex;"/></span> maps each possible input/output pair to a finite-dimensional real-valued feature vector.  As before, the feature vector is multiplied by a weight vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>w</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w}</annotation>\n</semantics>\n</math></span><img alt="w" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/88b1e0c8e1be5ebe69d18a8010676fa42d7961e6" style="vertical-align: -0.338ex; width:1.664ex; height:1.676ex;"/></span>, but now the resulting score is used to choose among many possible outputs:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\hat {y}}=\\operatorname {argmax} _{y}f(x,y)\\cdot w.}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mover>\n<mi>y</mi>\n<mo stretchy="false">^<!-- ^ --></mo>\n</mover>\n</mrow>\n</mrow>\n<mo>=</mo>\n<msub>\n<mi>argmax</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>y</mi>\n</mrow>\n</msub>\n<mo>\xe2\x81\xa1<!-- \xe2\x81\xa1 --></mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mi>w</mi>\n<mo>.</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\hat {y}}=\\operatorname {argmax} _{y}f(x,y)\\cdot w.}</annotation>\n</semantics>\n</math></span><img alt="{\\hat {y}}=\\operatorname {argmax} _{y}f(x,y)\\cdot w." aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dc5b83e57ad0de11c737317732783fcf03b8cf1b" style="vertical-align: -1.171ex; width:23.997ex; height:3.176ex;"/></span></dd></dl>\n<p>Learning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not.  The update becomes:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle w_{t+1}=w_{t}+f(x,y)-f(x,{\\hat {y}}).}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>w</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>+</mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mover>\n<mi>y</mi>\n<mo stretchy="false">^<!-- ^ --></mo>\n</mover>\n</mrow>\n</mrow>\n<mo stretchy="false">)</mo>\n<mo>.</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle w_{t+1}=w_{t}+f(x,y)-f(x,{\\hat {y}}).}</annotation>\n</semantics>\n</math></span><img alt="w_{t+1}=w_{t}+f(x,y)-f(x,{\\hat {y}})." aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3654bee4f0c3850e6eaad046bc25f513273c4a5" style="vertical-align: -0.838ex; width:29.868ex; height:2.843ex;"/></span></dd></dl>\n<p>This multiclass feedback formulation reduces to the original perceptron when <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>x</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x}</annotation>\n</semantics>\n</math></span><img alt="x" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;"/></span> is a real-valued vector, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle y}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>y</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle y}</annotation>\n</semantics>\n</math></span><img alt="y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;"/></span> is chosen from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\{0,1\\}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo fence="false" stretchy="false">{</mo>\n<mn>0</mn>\n<mo>,</mo>\n<mn>1</mn>\n<mo fence="false" stretchy="false">}</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\{0,1\\}}</annotation>\n</semantics>\n</math></span><img alt="\\{0,1\\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/28de5781698336d21c9c560fb1cbb3fb406923eb" style="vertical-align: -0.838ex; width:5.684ex; height:2.843ex;"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle f(x,y)=yx}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mi>y</mi>\n<mi>x</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle f(x,y)=yx}</annotation>\n</semantics>\n</math></span><img alt="f(x,y)=yx" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0f765f7d01d77ad20fbda78e82878af2fd6c99cb" style="vertical-align: -0.838ex; width:12.191ex; height:2.843ex;"/></span>.\n</p><p>For certain problems, input/output representations and features can be chosen so that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\mathrm {argmax} _{y}f(x,y)\\cdot w}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="normal">a</mi>\n<mi mathvariant="normal">r</mi>\n<mi mathvariant="normal">g</mi>\n<mi mathvariant="normal">m</mi>\n<mi mathvariant="normal">a</mi>\n<mi mathvariant="normal">x</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>y</mi>\n</mrow>\n</msub>\n<mi>f</mi>\n<mo stretchy="false">(</mo>\n<mi>x</mi>\n<mo>,</mo>\n<mi>y</mi>\n<mo stretchy="false">)</mo>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mi>w</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\mathrm {argmax} _{y}f(x,y)\\cdot w}</annotation>\n</semantics>\n</math></span><img alt="\\mathrm {argmax} _{y}f(x,y)\\cdot w" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/493974bd293b9465408430aba5596b4171a5dde4" style="vertical-align: -1.171ex; width:18.562ex; height:3.176ex;"/></span> can be found efficiently even though <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle y}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>y</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle y}</annotation>\n</semantics>\n</math></span><img alt="y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;"/></span> is chosen from a very large or even infinite set.\n</p><p>Since 2002, perceptron training has become popular in the field of <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> for such tasks as <a href="/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">part-of-speech tagging</a> and <a class="mw-redirect" href="/wiki/Syntactic_parsing" title="Syntactic parsing">syntactic parsing</a> (Collins, 2002). It has also been applied to large-scale machine learning problems in a <a href="/wiki/Distributed_computing" title="Distributed computing">distributed computing</a> setting.<sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>\n</p>\n<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=9" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="reflist" style="list-style-type: decimal;">\n<div class="mw-references-wrap mw-references-columns"><ol class="references">\n<li id="cite_note-largemargin-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-largemargin_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-largemargin_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFFreundSchapire1999"><a href="/wiki/Yoav_Freund" title="Yoav Freund">Freund, Y.</a>; <a href="/wiki/Robert_Schapire" title="Robert Schapire">Schapire, R. E.</a> (1999). <a class="external text" href="http://cseweb.ucsd.edu/~yfreund/papers/LargeMarginsUsingPerceptron.pdf" rel="nofollow">"Large margin classification using the perceptron algorithm"</a> <span class="cs1-format">(PDF)</span>. <i><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">Machine Learning</a></i>. <b>37</b> (3): 277\xe2\x80\x93296. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1023%2FA%3A1007662407062" rel="nofollow">10.1023/A:1007662407062</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>\xc2\xa0<a class="external text" href="https://api.semanticscholar.org/CorpusID:5885617" rel="nofollow">5885617</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=Large+margin+classification+using+the+perceptron+algorithm&amp;rft.volume=37&amp;rft.issue=3&amp;rft.pages=277-296&amp;rft.date=1999&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1007662407062&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A5885617&amp;rft.aulast=Freund&amp;rft.aufirst=Y.&amp;rft.au=Schapire%2C+R.+E.&amp;rft_id=http%3A%2F%2Fcseweb.ucsd.edu%2F~yfreund%2Fpapers%2FLargeMarginsUsingPerceptron.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><style data-mw-deduplicate="TemplateStyles:r982806391">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>\n</li>\n<li id="cite_note-bishop-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-bishop_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bishop_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFBishop2006">Bishop, Christopher M. (2006). <i>Pattern Recognition and Machine Learning</i>. Springer. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/0-387-31073-8" title="Special:BookSources/0-387-31073-8"><bdi>0-387-31073-8</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Pattern+Recognition+and+Machine+Learning&amp;rft.pub=Springer&amp;rft.date=2006&amp;rft.isbn=0-387-31073-8&amp;rft.aulast=Bishop&amp;rft.aufirst=Christopher+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFRosenblatt1957">Rosenblatt, Frank (1957). "The Perceptron\xe2\x80\x94a perceiving and recognizing automaton". <i>Report 85-460-1</i>. Cornell Aeronautical Laboratory.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Report+85-460-1&amp;rft.atitle=The+Perceptron%E2%80%94a+perceiving+and+recognizing+automaton&amp;rft.date=1957&amp;rft.aulast=Rosenblatt&amp;rft.aufirst=Frank&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-Olazaran-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-Olazaran_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Olazaran_4-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFOlazaran1996">Olazaran, Mikel (1996). "A Sociological Study of the Official History of the Perceptrons Controversy". <i>Social Studies of Science</i>. <b>26</b> (3): 611\xe2\x80\x93659. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1177%2F030631296026003005" rel="nofollow">10.1177/030631296026003005</a>. <a class="mw-redirect" href="/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>\xc2\xa0<a class="external text" href="//www.jstor.org/stable/285702" rel="nofollow">285702</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>\xc2\xa0<a class="external text" href="https://api.semanticscholar.org/CorpusID:16786738" rel="nofollow">16786738</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Social+Studies+of+Science&amp;rft.atitle=A+Sociological+Study+of+the+Official+History+of+the+Perceptrons+Controversy&amp;rft.volume=26&amp;rft.issue=3&amp;rft.pages=611-659&amp;rft.date=1996&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A16786738&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F285702&amp;rft_id=info%3Adoi%2F10.1177%2F030631296026003005&amp;rft.aulast=Olazaran&amp;rft.aufirst=Mikel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFAizermanBravermanRozonoer1964">Aizerman, M. A.; Braverman, E. M.; Rozonoer, L. I. (1964). "Theoretical foundations of the potential function method in pattern recognition learning". <i>Automation and Remote Control</i>. <b>25</b>: 821\xe2\x80\x93837.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Automation+and+Remote+Control&amp;rft.atitle=Theoretical+foundations+of+the+potential+function+method+in+pattern+recognition+learning&amp;rft.volume=25&amp;rft.pages=821-837&amp;rft.date=1964&amp;rft.aulast=Aizerman&amp;rft.aufirst=M.+A.&amp;rft.au=Braverman%2C+E.+M.&amp;rft.au=Rozonoer%2C+L.+I.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFMohriRostamizadeh2013">Mohri, Mehryar; Rostamizadeh, Afshin (2013). "Perceptron Mistake Bounds". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1305.0208" rel="nofollow">1305.0208</a></span> [<a class="external text" href="//arxiv.org/archive/cs.LG" rel="nofollow">cs.LG</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Perceptron+Mistake+Bounds&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1305.0208&amp;rft.aulast=Mohri&amp;rft.aufirst=Mehryar&amp;rft.au=Rostamizadeh%2C+Afshin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFCashYuste1999">Cash, Sydney; Yuste, Rafael (1999). <a class="external text" href="https://doi.org/10.1016%2FS0896-6273%2800%2981098-3" rel="nofollow">"Linear Summation of Excitatory Inputs by CA1 Pyramidal Neurons"</a>. <i><a href="/wiki/Neuron_(journal)" title="Neuron (journal)">Neuron</a></i>. <b>22</b> (2): 383\xe2\x80\x93394. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1016%2FS0896-6273%2800%2981098-3" rel="nofollow">10.1016/S0896-6273(00)81098-3</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>\xc2\xa0<a class="external text" href="//pubmed.ncbi.nlm.nih.gov/10069343" rel="nofollow">10069343</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neuron&amp;rft.atitle=Linear+Summation+of+Excitatory+Inputs+by+CA1+Pyramidal+Neurons&amp;rft.volume=22&amp;rft.issue=2&amp;rft.pages=383-394&amp;rft.date=1999&amp;rft_id=info%3Adoi%2F10.1016%2FS0896-6273%2800%2981098-3&amp;rft_id=info%3Apmid%2F10069343&amp;rft.aulast=Cash&amp;rft.aufirst=Sydney&amp;rft.au=Yuste%2C+Rafael&amp;rft_id=%2F%2Fdoi.org%2F10.1016%252FS0896-6273%252800%252981098-3&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFLiouLiouLiou2013">Liou, D.-R.; Liou, J.-W.; Liou, C.-Y. (2013). <i>Learning Behaviors of Perceptron</i>. iConcept Press. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-1-477554-73-9" title="Special:BookSources/978-1-477554-73-9"><bdi>978-1-477554-73-9</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Learning+Behaviors+of+Perceptron&amp;rft.pub=iConcept+Press&amp;rft.date=2013&amp;rft.isbn=978-1-477554-73-9&amp;rft.aulast=Liou&amp;rft.aufirst=D.-R.&amp;rft.au=Liou%2C+J.-W.&amp;rft.au=Liou%2C+C.-Y.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFNovikoff1963">Novikoff, Albert J. (1963). "On convergence proofs for perceptrons". <i>Office of Naval Research</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Office+of+Naval+Research&amp;rft.atitle=On+convergence+proofs+for+perceptrons&amp;rft.date=1963&amp;rft.aulast=Novikoff&amp;rft.aufirst=Albert+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFBishop2006">Bishop, Christopher M (2006-08-17). "Chapter 4. Linear Models for Classification". <i>Pattern Recognition and Machine Learning</i>. Springer Science+Business Media, LLC. p.\xc2\xa0194. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0387-31073-2" title="Special:BookSources/978-0387-31073-2"><bdi>978-0387-31073-2</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+4.+Linear+Models+for+Classification&amp;rft.btitle=Pattern+Recognition+and+Machine+Learning&amp;rft.pages=194&amp;rft.pub=Springer+Science%2BBusiness+Media%2C+LLC&amp;rft.date=2006-08-17&amp;rft.isbn=978-0387-31073-2&amp;rft.aulast=Bishop&amp;rft.aufirst=Christopher+M&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-KrauthMezard87-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-KrauthMezard87_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-KrauthMezard87_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFKrauthMezard1987">Krauth, W.; Mezard, M. (1987). "Learning algorithms with optimal stability in neural networks". <i>J. Of Physics A: Math. Gen</i>. <b>20</b> (11): L745\xe2\x80\x93L752. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/1987JPhA...20L.745K" rel="nofollow">1987JPhA...20L.745K</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1088%2F0305-4470%2F20%2F11%2F013" rel="nofollow">10.1088/0305-4470/20/11/013</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J.+Of+Physics+A%3A+Math.+Gen.&amp;rft.atitle=Learning+algorithms+with+optimal+stability+in+neural+networks&amp;rft.volume=20&amp;rft.issue=11&amp;rft.pages=L745-L752&amp;rft.date=1987&amp;rft_id=info%3Adoi%2F10.1088%2F0305-4470%2F20%2F11%2F013&amp;rft_id=info%3Abibcode%2F1987JPhA...20L.745K&amp;rft.aulast=Krauth&amp;rft.aufirst=W.&amp;rft.au=Mezard%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFWendemuth1995">Wendemuth, A. (1995). "Learning the Unlearnable". <i>J. Of Physics A: Math. Gen</i>. <b>28</b> (18): 5423\xe2\x80\x935436. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/1995JPhA...28.5423W" rel="nofollow">1995JPhA...28.5423W</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1088%2F0305-4470%2F28%2F18%2F030" rel="nofollow">10.1088/0305-4470/28/18/030</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J.+Of+Physics+A%3A+Math.+Gen.&amp;rft.atitle=Learning+the+Unlearnable&amp;rft.volume=28&amp;rft.issue=18&amp;rft.pages=5423-5436&amp;rft.date=1995&amp;rft_id=info%3Adoi%2F10.1088%2F0305-4470%2F28%2F18%2F030&amp;rft_id=info%3Abibcode%2F1995JPhA...28.5423W&amp;rft.aulast=Wendemuth&amp;rft.aufirst=A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFAnlaufBiehl1989">Anlauf, J. K.; Biehl, M. (1989). "The AdaTron: an Adaptive Perceptron algorithm". <i>Europhysics Letters</i>. <b>10</b> (7): 687\xe2\x80\x93692. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/1989EL.....10..687A" rel="nofollow">1989EL.....10..687A</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1209%2F0295-5075%2F10%2F7%2F014" rel="nofollow">10.1209/0295-5075/10/7/014</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Europhysics+Letters&amp;rft.atitle=The+AdaTron%3A+an+Adaptive+Perceptron+algorithm&amp;rft.volume=10&amp;rft.issue=7&amp;rft.pages=687-692&amp;rft.date=1989&amp;rft_id=info%3Adoi%2F10.1209%2F0295-5075%2F10%2F7%2F014&amp;rft_id=info%3Abibcode%2F1989EL.....10..687A&amp;rft.aulast=Anlauf&amp;rft.aufirst=J.+K.&amp;rft.au=Biehl%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFMcDonaldHallMann2010">McDonald, R.; Hall, K.; Mann, G. (2010). <a class="external text" href="https://www.aclweb.org/anthology/N10-1069.pdf" rel="nofollow">"Distributed Training Strategies for the Structured Perceptron"</a> <span class="cs1-format">(PDF)</span>. <i>Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL</i>. Association for Computational Linguistics. pp.\xc2\xa0456\xe2\x80\x93464.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Distributed+Training+Strategies+for+the+Structured+Perceptron&amp;rft.btitle=Human+Language+Technologies%3A+The+2010+Annual+Conference+of+the+North+American+Chapter+of+the+ACL&amp;rft.pages=456-464&amp;rft.pub=Association+for+Computational+Linguistics&amp;rft.date=2010&amp;rft.aulast=McDonald&amp;rft.aufirst=R.&amp;rft.au=Hall%2C+K.&amp;rft.au=Mann%2C+G.&amp;rft_id=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN10-1069.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APerceptron"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n</ol></div></div>\n<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=10" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li>Aizerman, M. A. and Braverman, E. M. and Lev I. Rozonoer. Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control, 25:821\xe2\x80\x93837, 1964.</li>\n<li>Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Cornell Aeronautical Laboratory, Psychological Review, v65, No. 6, pp.\xc2\xa0386\xe2\x80\x93408. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1037%2Fh0042519" rel="nofollow">10.1037/h0042519</a>.</li>\n<li>Rosenblatt, Frank (1962), Principles of Neurodynamics. Washington, DC:Spartan Books.</li>\n<li>Minsky M. L. and Papert S. A. 1969. <i>Perceptrons</i>. Cambridge, MA: MIT Press.</li>\n<li>Gallant, S. I. (1990). <a class="external text" href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=80230" rel="nofollow">Perceptron-based learning algorithms.</a> IEEE Transactions on Neural Networks, vol. 1, no. 2, pp.\xc2\xa0179\xe2\x80\x93191.</li>\n<li>Mohri, Mehryar and Rostamizadeh, Afshin (2013). <a class="external text" href="https://arxiv.org/abs/1305.0208" rel="nofollow">Perceptron Mistake Bounds</a> arXiv:1305.0208, 2013.</li>\n<li>Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615\xe2\x80\x93622. Polytechnic Institute of Brooklyn.</li>\n<li><a href="/wiki/Bernard_Widrow" title="Bernard Widrow">Widrow, B.</a>, Lehr, M.A., "<a class="external text" href="http://www.inf.ufrgs.br/~engel/data/media/file/cmp121/widrow.pdf" rel="nofollow">30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation</a>," <i>Proc. IEEE</i>, vol 78, no 9, pp.\xc2\xa01415\xe2\x80\x931442, (1990).</li>\n<li><a href="/wiki/Michael_Collins_(computational_linguist)" title="Michael Collins (computational linguist)">Collins, M.</a> 2002. <a class="external text" href="https://www.aclweb.org/anthology/W02-1001" rel="nofollow">Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm</a> in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \'02).</li>\n<li>Yin, Hongfeng (1996), Perceptron-Based Algorithms and Analysis, Spectrum Library, Concordia University, Canada</li></ul>\n<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=11" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><a class="external text" href="http://www.mathworks.com/matlabcentral/fileexchange/32949-a-perceptron-learns-to-perform-a-binary-nand-function/content/PerceptronImpl.m" rel="nofollow">A Perceptron implemented in MATLAB to learn binary NAND function</a></li>\n<li>Chapter 3 <a class="external text" href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K3.pdf" rel="nofollow">Weighted networks - the perceptron</a> and chapter 4 <a class="external text" href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf" rel="nofollow">Perceptron learning</a> of <a class="external text" href="http://page.mi.fu-berlin.de/rojas/neural/index.html.html" rel="nofollow"><i>Neural Networks - A Systematic Introduction</i></a> by <a href="/wiki/Ra%C3%BAl_Rojas" title="Ra\xc3\xbal Rojas">Ra\xc3\xbal Rojas</a> (<link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/><a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-3-540-60505-8" title="Special:BookSources/978-3-540-60505-8">978-3-540-60505-8</a>)</li>\n<li><a class="external text" href="http://www.csulb.edu/~cwallis/artificialn/History.htm" rel="nofollow">History of perceptrons</a></li>\n<li><a class="external text" href="http://www.cis.hut.fi/ahonkela/dippa/node41.html" rel="nofollow">Mathematics of multilayer perceptrons</a></li>\n<li><a class="external text" href="https://owenshen24.github.io/perceptron/" rel="nofollow">Visualize several perceptron variants learning in browser</a></li></ul>\n<div aria-labelledby="Authority_control_frameless_&amp;#124;text-top_&amp;#124;10px_&amp;#124;alt=Edit_this_at_Wikidata_&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q690207#identifiers&amp;#124;Edit_this_at_Wikidata" class="navbox authority-control" role="navigation" style="padding:3px"><table class="nowraplinks hlist navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th class="navbox-group" id="Authority_control_frameless_&amp;#124;text-top_&amp;#124;10px_&amp;#124;alt=Edit_this_at_Wikidata_&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q690207#identifiers&amp;#124;Edit_this_at_Wikidata" scope="row" style="width:1%"><a href="/wiki/Help:Authority_control" title="Help:Authority control">Authority control</a> <a href="https://www.wikidata.org/wiki/Q690207#identifiers" title="Edit this at Wikidata"><img alt="Edit this at Wikidata" data-file-height="20" data-file-width="20" decoding="async" height="10" src="//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x" style="vertical-align: text-top" width="10"/></a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><span class="nowrap"><a href="/wiki/National_Diet_Library" title="National Diet Library">NDL</a>: <span class="uid"><a class="external text" href="https://id.ndl.go.jp/auth/ndlna/00569067" rel="nofollow">00569067</a></span></span></li></ul>\n</div></td></tr></tbody></table></div>\n<!-- \nNewPP limit report\nParsed by mw1266\nCached time: 20201029214253\nCache expiry: 2592000\nDynamic content: false\nComplications: [vary\xe2\x80\x90revision\xe2\x80\x90sha1]\nCPU time usage: 0.476 seconds\nReal time usage: 0.719 seconds\nPreprocessor visited node count: 1891/1000000\nPost\xe2\x80\x90expand include size: 60953/2097152 bytes\nTemplate argument size: 1734/2097152 bytes\nHighest expansion depth: 14/40\nExpensive parser function count: 2/500\nUnstrip recursion depth: 1/20\nUnstrip post\xe2\x80\x90expand size: 50924/5000000 bytes\nLua time usage: 0.173/10.000 seconds\nLua memory usage: 4.67 MB/50 MB\nNumber of Wikibase entities loaded: 1/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  433.743      1 -total\n 38.00%  164.834      1 Template:Reflist\n 24.15%  104.750      8 Template:Cite_journal\n 13.73%   59.552      1 Template:Authority_control\n 12.85%   55.735      1 Template:ISBN\n  9.49%   41.145      1 Template:Machine_learning_bar\n  8.50%   36.881      1 Template:Redirect\n  8.40%   36.415      1 Template:Sidebar_with_collapsible_lists\n  5.51%   23.903      1 Template:Catalog_lookup_link\n  4.55%   19.740      1 Template:Longitem\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:172777-0!canonical!math=5 and timestamp 20201029214253 and revision id 984516045\n -->\n</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>\n<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Perceptron&amp;oldid=984516045">https://en.wikipedia.org/w/index.php?title=Perceptron&amp;oldid=984516045</a>"</div></div>\n<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></li><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:Wikipedia_articles_with_NDL_identifiers" title="Category:Wikipedia articles with NDL identifiers">Wikipedia articles with NDL identifiers</a></li><li><a href="/wiki/Category:Articles_with_example_Python_(programming_language)_code" title="Category:Articles with example Python (programming language) code">Articles with example Python (programming language) code</a></li></ul></div></div>\n</div>\n</div>\n<div id="mw-data-after-content">\n<div class="read-more-container"></div>\n</div>\n<div id="mw-navigation">\n<h2>Navigation menu</h2>\n<div id="mw-head">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-personal-label" class="mw-portlet mw-portlet-personal vector-menu" id="p-personal" role="navigation">\n<h3 id="p-personal-label">\n<span>Personal tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Perceptron" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Perceptron" title="You\'re encouraged to log in; however, it\'s not mandatory. [o]">Log in</a></li></ul>\n</div>\n</nav>\n<div id="left-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-namespaces-label" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" id="p-namespaces" role="navigation">\n<h3 id="p-namespaces-label">\n<span>Namespaces</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Perceptron" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Perceptron" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-variants-label" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu vector-menu-dropdown" id="p-variants" role="navigation">\n<input aria-labelledby="p-variants-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-variants-label">\n<span>Variants</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n</div>\n<div id="right-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-views-label" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" id="p-views" role="navigation">\n<h3 id="p-views-label">\n<span>Views</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/Perceptron">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=Perceptron&amp;action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=Perceptron&amp;action=history" title="Past revisions of this page [h]">View history</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-cactions-label" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu vector-menu-dropdown" id="p-cactions" role="navigation">\n<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-cactions-label">\n<span>More</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n<div id="p-search" role="search">\n<h3>\n<label for="searchInput">Search</label>\n</h3>\n<form action="/w/index.php" id="searchform">\n<div data-search-loc="header-navigation" id="simpleSearch">\n<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>\n<input name="title" type="hidden" value="Special:Search"/>\n<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">\n<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>\n</input></div>\n</form>\n</div>\n</div>\n</div>\n<div id="mw-panel">\n<div id="p-logo" role="banner">\n<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>\n</div>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-navigation-label" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">\n<h3 id="p-navigation-label">\n<span>Navigation</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Articles related to current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-interaction-label" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">\n<h3 id="p-interaction-label">\n<span>Contribute</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-introduction"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia">Learn to edit</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-tb-label" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" id="p-tb" role="navigation">\n<h3 id="p-tb-label">\n<span>Tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Perceptron" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Perceptron" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Perceptron&amp;oldid=984516045" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Perceptron&amp;action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Perceptron&amp;id=984516045&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q690207" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-coll-print_export-label" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">\n<h3 id="p-coll-print_export-label">\n<span>Print/export</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Perceptron&amp;action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Perceptron&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-lang-label" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" id="p-lang" role="navigation">\n<h3 id="p-lang-label">\n<span>Languages</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-ar"><a class="interlanguage-link-target" href="https://ar.wikipedia.org/wiki/%D8%A8%D9%8A%D8%B1%D8%B3%D9%8A%D8%A8%D8%AA%D8%B1%D9%88%D9%86" hreflang="ar" lang="ar" title="\xd8\xa8\xd9\x8a\xd8\xb1\xd8\xb3\xd9\x8a\xd8\xa8\xd8\xaa\xd8\xb1\xd9\x88\xd9\x86 \xe2\x80\x93 Arabic">\xd8\xa7\xd9\x84\xd8\xb9\xd8\xb1\xd8\xa8\xd9\x8a\xd8\xa9</a></li><li class="interlanguage-link interwiki-az"><a class="interlanguage-link-target" href="https://az.wikipedia.org/wiki/Perseptron" hreflang="az" lang="az" title="Perseptron \xe2\x80\x93 Azerbaijani">Az\xc9\x99rbaycanca</a></li><li class="interlanguage-link interwiki-ca"><a class="interlanguage-link-target" href="https://ca.wikipedia.org/wiki/Perceptr%C3%B3" hreflang="ca" lang="ca" title="Perceptr\xc3\xb3 \xe2\x80\x93 Catalan">Catal\xc3\xa0</a></li><li class="interlanguage-link interwiki-cs"><a class="interlanguage-link-target" href="https://cs.wikipedia.org/wiki/Perceptron" hreflang="cs" lang="cs" title="Perceptron \xe2\x80\x93 Czech">\xc4\x8ce\xc5\xa1tina</a></li><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/Perzeptron" hreflang="de" lang="de" title="Perzeptron \xe2\x80\x93 German">Deutsch</a></li><li class="interlanguage-link interwiki-el"><a class="interlanguage-link-target" href="https://el.wikipedia.org/wiki/Perceptron" hreflang="el" lang="el" title="Perceptron \xe2\x80\x93 Greek">\xce\x95\xce\xbb\xce\xbb\xce\xb7\xce\xbd\xce\xb9\xce\xba\xce\xac</a></li><li class="interlanguage-link interwiki-es"><a class="interlanguage-link-target" href="https://es.wikipedia.org/wiki/Perceptr%C3%B3n" hreflang="es" lang="es" title="Perceptr\xc3\xb3n \xe2\x80\x93 Spanish">Espa\xc3\xb1ol</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%D9%BE%D8%B1%D8%B3%D9%BE%D8%AA%D8%B1%D9%88%D9%86" hreflang="fa" lang="fa" title="\xd9\xbe\xd8\xb1\xd8\xb3\xd9\xbe\xd8\xaa\xd8\xb1\xd9\x88\xd9\x86 \xe2\x80\x93 Persian">\xd9\x81\xd8\xa7\xd8\xb1\xd8\xb3\xdb\x8c</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/Perceptron" hreflang="fr" lang="fr" title="Perceptron \xe2\x80\x93 French">Fran\xc3\xa7ais</a></li><li class="interlanguage-link interwiki-ko"><a class="interlanguage-link-target" href="https://ko.wikipedia.org/wiki/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0" hreflang="ko" lang="ko" title="\xed\x8d\xbc\xec\x85\x89\xed\x8a\xb8\xeb\xa1\xa0 \xe2\x80\x93 Korean">\xed\x95\x9c\xea\xb5\xad\xec\x96\xb4</a></li><li class="interlanguage-link interwiki-id"><a class="interlanguage-link-target" href="https://id.wikipedia.org/wiki/Perceptron" hreflang="id" lang="id" title="Perceptron \xe2\x80\x93 Indonesian">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it"><a class="interlanguage-link-target" href="https://it.wikipedia.org/wiki/Percettrone" hreflang="it" lang="it" title="Percettrone \xe2\x80\x93 Italian">Italiano</a></li><li class="interlanguage-link interwiki-he"><a class="interlanguage-link-target" href="https://he.wikipedia.org/wiki/%D7%A4%D7%A8%D7%A1%D7%A4%D7%98%D7%A8%D7%95%D7%9F" hreflang="he" lang="he" title="\xd7\xa4\xd7\xa8\xd7\xa1\xd7\xa4\xd7\x98\xd7\xa8\xd7\x95\xd7\x9f \xe2\x80\x93 Hebrew">\xd7\xa2\xd7\x91\xd7\xa8\xd7\x99\xd7\xaa</a></li><li class="interlanguage-link interwiki-mk"><a class="interlanguage-link-target" href="https://mk.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD" hreflang="mk" lang="mk" title="\xd0\x9f\xd0\xb5\xd1\x80\xd1\x86\xd0\xb5\xd0\xbf\xd1\x82\xd1\x80\xd0\xbe\xd0\xbd \xe2\x80\x93 Macedonian">\xd0\x9c\xd0\xb0\xd0\xba\xd0\xb5\xd0\xb4\xd0\xbe\xd0\xbd\xd1\x81\xd0\xba\xd0\xb8</a></li><li class="interlanguage-link interwiki-nl"><a class="interlanguage-link-target" href="https://nl.wikipedia.org/wiki/Perceptron" hreflang="nl" lang="nl" title="Perceptron \xe2\x80\x93 Dutch">Nederlands</a></li><li class="interlanguage-link interwiki-ne"><a class="interlanguage-link-target" href="https://ne.wikipedia.org/wiki/%E0%A4%AA%E0%A5%8D%E0%A4%B0%E0%A4%B8%E0%A5%87%E0%A4%AA%E0%A5%8D%E0%A4%9F%E0%A5%8D%E0%A4%B0%E0%A5%8B%E0%A4%A8" hreflang="ne" lang="ne" title="\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xb8\xe0\xa5\x87\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\x9f\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa5\x8b\xe0\xa4\xa8 \xe2\x80\x93 Nepali">\xe0\xa4\xa8\xe0\xa5\x87\xe0\xa4\xaa\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa5\x80</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3" hreflang="ja" lang="ja" title="\xe3\x83\x91\xe3\x83\xbc\xe3\x82\xbb\xe3\x83\x97\xe3\x83\x88\xe3\x83\xad\xe3\x83\xb3 \xe2\x80\x93 Japanese">\xe6\x97\xa5\xe6\x9c\xac\xe8\xaa\x9e</a></li><li class="interlanguage-link interwiki-pl"><a class="interlanguage-link-target" href="https://pl.wikipedia.org/wiki/Perceptron" hreflang="pl" lang="pl" title="Perceptron \xe2\x80\x93 Polish">Polski</a></li><li class="interlanguage-link interwiki-pt"><a class="interlanguage-link-target" href="https://pt.wikipedia.org/wiki/Perceptron" hreflang="pt" lang="pt" title="Perceptron \xe2\x80\x93 Portuguese">Portugu\xc3\xaas</a></li><li class="interlanguage-link interwiki-ru badge-Q17437796 badge-featuredarticle" title="featured article badge"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD" hreflang="ru" lang="ru" title="\xd0\x9f\xd0\xb5\xd1\x80\xd1\x86\xd0\xb5\xd0\xbf\xd1\x82\xd1\x80\xd0\xbe\xd0\xbd \xe2\x80\x93 Russian">\xd0\xa0\xd1\x83\xd1\x81\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9</a></li><li class="interlanguage-link interwiki-sk"><a class="interlanguage-link-target" href="https://sk.wikipedia.org/wiki/Perceptr%C3%B3n" hreflang="sk" lang="sk" title="Perceptr\xc3\xb3n \xe2\x80\x93 Slovak">Sloven\xc4\x8dina</a></li><li class="interlanguage-link interwiki-sl"><a class="interlanguage-link-target" href="https://sl.wikipedia.org/wiki/Perceptron" hreflang="sl" lang="sl" title="Perceptron \xe2\x80\x93 Slovenian">Sloven\xc5\xa1\xc4\x8dina</a></li><li class="interlanguage-link interwiki-sr"><a class="interlanguage-link-target" href="https://sr.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD" hreflang="sr" lang="sr" title="\xd0\x9f\xd0\xb5\xd1\x80\xd1\x86\xd0\xb5\xd0\xbf\xd1\x82\xd1\x80\xd0\xbe\xd0\xbd \xe2\x80\x93 Serbian">\xd0\xa1\xd1\x80\xd0\xbf\xd1\x81\xd0\xba\xd0\xb8 / srpski</a></li><li class="interlanguage-link interwiki-sv"><a class="interlanguage-link-target" href="https://sv.wikipedia.org/wiki/Perceptron" hreflang="sv" lang="sv" title="Perceptron \xe2\x80\x93 Swedish">Svenska</a></li><li class="interlanguage-link interwiki-th"><a class="interlanguage-link-target" href="https://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%9E%E0%B8%AD%E0%B8%A3%E0%B9%8C%E0%B9%80%E0%B8%8B%E0%B8%9B%E0%B8%95%E0%B8%A3%E0%B8%AD%E0%B8%99" hreflang="th" lang="th" title="\xe0\xb9\x80\xe0\xb8\x9e\xe0\xb8\xad\xe0\xb8\xa3\xe0\xb9\x8c\xe0\xb9\x80\xe0\xb8\x8b\xe0\xb8\x9b\xe0\xb8\x95\xe0\xb8\xa3\xe0\xb8\xad\xe0\xb8\x99 \xe2\x80\x93 Thai">\xe0\xb9\x84\xe0\xb8\x97\xe0\xb8\xa2</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD" hreflang="uk" lang="uk" title="\xd0\x9f\xd0\xb5\xd1\x80\xd1\x86\xd0\xb5\xd0\xbf\xd1\x82\xd1\x80\xd0\xbe\xd0\xbd \xe2\x80\x93 Ukrainian">\xd0\xa3\xd0\xba\xd1\x80\xd0\xb0\xd1\x97\xd0\xbd\xd1\x81\xd1\x8c\xd0\xba\xd0\xb0</a></li><li class="interlanguage-link interwiki-zh-yue"><a class="interlanguage-link-target" href="https://zh-yue.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E6%A9%9F" hreflang="yue" lang="yue" title="\xe6\x84\x9f\xe7\x9f\xa5\xe6\xa9\x9f \xe2\x80\x93 Cantonese">\xe7\xb2\xb5\xe8\xaa\x9e</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8" hreflang="zh" lang="zh" title="\xe6\x84\x9f\xe7\x9f\xa5\xe5\x99\xa8 \xe2\x80\x93 Chinese">\xe4\xb8\xad\xe6\x96\x87</a></li></ul>\n<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q690207#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class="mw-footer" id="footer" role="contentinfo">\n<ul id="footer-info">\n<li id="footer-info-lastmod"> This page was last edited on 20 October 2020, at 14:09<span class="anonymous-show">\xc2\xa0(UTC)</span>.</li>\n<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia\xc2\xae is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id="footer-places">\n<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>\n<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>\n<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>\n<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>\n<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Perceptron&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>\n<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>\n<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>\n<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>\n</ul>\n<ul class="noprint" id="footer-icons">\n<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>\n<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>\n</ul>\n<div style="clear: both;"></div>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.476","walltime":"0.719","ppvisitednodes":{"value":1891,"limit":1000000},"postexpandincludesize":{"value":60953,"limit":2097152},"templateargumentsize":{"value":1734,"limit":2097152},"expansiondepth":{"value":14,"limit":40},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":50924,"limit":5000000},"entityaccesscount":{"value":1,"limit":400},"timingprofile":["100.00%  433.743      1 -total"," 38.00%  164.834      1 Template:Reflist"," 24.15%  104.750      8 Template:Cite_journal"," 13.73%   59.552      1 Template:Authority_control"," 12.85%   55.735      1 Template:ISBN","  9.49%   41.145      1 Template:Machine_learning_bar","  8.50%   36.881      1 Template:Redirect","  8.40%   36.415      1 Template:Sidebar_with_collapsible_lists","  5.51%   23.903      1 Template:Catalog_lookup_link","  4.55%   19.740      1 Template:Longitem"]},"scribunto":{"limitreport-timeusage":{"value":"0.173","limit":"10.000"},"limitreport-memusage":{"value":4892464,"limit":52428800}},"cachereport":{"origin":"mw1266","timestamp":"20201029214253","ttl":2592000,"transientcontent":false}}});});</script>\n<script type="application/ld+json">{"@context":"https:\\/\\/schema.org","@type":"Article","name":"Perceptron","url":"https:\\/\\/en.wikipedia.org\\/wiki\\/Perceptron","sameAs":"http:\\/\\/www.wikidata.org\\/entity\\/Q690207","mainEntity":"http:\\/\\/www.wikidata.org\\/entity\\/Q690207","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png"}},"datePublished":"2003-01-22T18:20:27Z","dateModified":"2020-10-20T14:09:50Z","headline":"algorithm for supervised learning of binary classifiers"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":166,"wgHostname":"mw1366"});});</script>\n</body></html>'