b'<!DOCTYPE html>\n\n<html class="client-nojs" dir="ltr" lang="en">\n<head>\n<meta charset="utf8"/>\n<title>Q-learning - Wikipedia</title>\n<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"417fbf2f-70f5-4503-8771-900f50130ec0","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Q-learning","wgTitle":"Q-learning","wgCurRevisionId":984486286,"wgRevisionId":984486286,"wgArticleId":1281850,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: missing periodical","All articles with unsourced statements","Articles with unsourced statements from December 2017","Wikipedia articles needing clarification from January 2020","Machine learning algorithms","Reinforcement learning"],"wgPageContentLanguage":"en","wgPageContentModel":\n"wikitext","wgRelevantPageName":"Q-learning","wgRelevantArticleId":1281850,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q2664563"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready",\n"skins.vector.styles.legacy":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\\\","watchToken":"+\\\\","csrfToken":"+\\\\"});\n});});</script>\n<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>\n<meta content="" name="ResourceLoaderDynamicStyles"/>\n<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<meta content="MediaWiki 1.36.0-wmf.14" name="generator"/>\n<meta content="origin" name="referrer"/>\n<meta content="origin-when-crossorigin" name="referrer"/>\n<meta content="origin-when-cross-origin" name="referrer"/>\n<link href="//en.m.wikipedia.org/wiki/Q-learning" media="only screen and (max-width: 720px)" rel="alternate"/>\n<link href="/w/index.php?title=Q-learning&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>\n<link href="/w/index.php?title=Q-learning&amp;action=edit" rel="edit" title="Edit this page"/>\n<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>\n<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>\n<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>\n<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>\n<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>\n<link href="https://en.wikipedia.org/wiki/Q-learning" rel="canonical"/>\n<link href="//login.wikimedia.org" rel="dns-prefetch"/>\n<link href="//meta.wikimedia.org" rel="dns-prefetch"/>\n</head>\n<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Q-learning rootpage-Q-learning skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>\n<div class="noprint" id="mw-head-base"></div>\n<div class="mw-body" id="content" role="main">\n<a id="top"></a>\n<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>\n<div class="mw-indicators mw-body-content">\n</div>\n<h1 class="firstHeading" id="firstHeading" lang="en">Q-learning</h1>\n<div class="mw-body-content" id="bodyContent">\n<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>\n<div id="contentSub"></div>\n<div id="contentSub2"></div>\n<div id="jump-to-nav"></div>\n<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>\n<a class="mw-jump-link" href="#searchInput">Jump to search</a>\n<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em">Part of a series on</td></tr><tr><th style="padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br/>and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>\n<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>\n<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>\n<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>\n<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>\n<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>\n<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>\n<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>\n<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>\n<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>\n<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>\n<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>\xc2\xa0\xe2\x80\xa2 <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>\n<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>\n<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>\n<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>\n<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>\n<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>\n<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>\n<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>\n<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>\n<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>\n<li><a class="mw-redirect" href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>\n<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>\n<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>\n<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>\n<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation\xe2\x80\x93maximization algorithm">Expectation\xe2\x80\x93maximization (EM)</a></li>\n<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>\n<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>\n<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>\n<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>\n<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>\n<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>\n<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>\n<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>\n<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>\n<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>\n<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>\n<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>\n<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>\n<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>\n<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>\n<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>\n<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>\n<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>\n<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>\n<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li></ul></li>\n<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>\n<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>\n<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>\n<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>\n<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>\n<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-selflink selflink">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias\xe2\x80\x93variance dilemma">Bias\xe2\x80\x93variance dilemma</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>\n<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>\n<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>\n<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>\n<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>\n<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik\xe2\x80\x93Chervonenkis theory">VC theory</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>\n<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>\n<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>\n<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>\n<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>\n<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p><b><i>Q</i>-learning</b> is a <a href="/wiki/Model-free_(reinforcement_learning)" title="Model-free (reinforcement learning)">model-free</a> <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a> algorithm to learn quality of actions telling an agent what action to take under what circumstances. It does not require a model (hence the connotation "model-free") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\n</p><p>For any finite <a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process</a> (FMDP), <i>Q</i>-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state.<sup class="reference" id="cite_ref-auto_1-0"><a href="#cite_note-auto-1">[1]</a></sup> <i>Q</i>-learning can identify an optimal <a href="/wiki/Action_selection" title="Action selection">action-selection</a> policy for any given FMDP, given infinite exploration time and a partly-random policy.<sup class="reference" id="cite_ref-auto_1-1"><a href="#cite_note-auto-1">[1]</a></sup> "Q" names the function that the algorithm computes with the maximum expected rewards for an action taken in a given state.<sup class="reference" id="cite_ref-:0_2-0"><a href="#cite_note-:0-2">[2]</a></sup>\n</p>\n<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>\n<ul>\n<li class="toclevel-1 tocsection-1"><a href="#Reinforcement_learning"><span class="tocnumber">1</span> <span class="toctext">Reinforcement learning</span></a></li>\n<li class="toclevel-1 tocsection-2"><a href="#Algorithm"><span class="tocnumber">2</span> <span class="toctext">Algorithm</span></a></li>\n<li class="toclevel-1 tocsection-3"><a href="#Influence_of_variables"><span class="tocnumber">3</span> <span class="toctext">Influence of variables</span></a>\n<ul>\n<li class="toclevel-2 tocsection-4"><a href="#Learning_Rate"><span class="tocnumber">3.1</span> <span class="toctext">Learning Rate</span></a></li>\n<li class="toclevel-2 tocsection-5"><a href="#Discount_factor"><span class="tocnumber">3.2</span> <span class="toctext">Discount factor</span></a></li>\n<li class="toclevel-2 tocsection-6"><a href="#Initial_conditions_(Q0)"><span class="tocnumber">3.3</span> <span class="toctext">Initial conditions (<i>Q</i><sub>0</sub>)</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-7"><a href="#Implementation"><span class="tocnumber">4</span> <span class="toctext">Implementation</span></a>\n<ul>\n<li class="toclevel-2 tocsection-8"><a href="#Function_approximation"><span class="tocnumber">4.1</span> <span class="toctext">Function approximation</span></a></li>\n<li class="toclevel-2 tocsection-9"><a href="#Quantization"><span class="tocnumber">4.2</span> <span class="toctext">Quantization</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-10"><a href="#History"><span class="tocnumber">5</span> <span class="toctext">History</span></a></li>\n<li class="toclevel-1 tocsection-11"><a href="#Variants"><span class="tocnumber">6</span> <span class="toctext">Variants</span></a>\n<ul>\n<li class="toclevel-2 tocsection-12"><a href="#Deep_Q-learning"><span class="tocnumber">6.1</span> <span class="toctext">Deep Q-learning</span></a></li>\n<li class="toclevel-2 tocsection-13"><a href="#Double_Q-learning"><span class="tocnumber">6.2</span> <span class="toctext">Double Q-learning</span></a></li>\n<li class="toclevel-2 tocsection-14"><a href="#Others"><span class="tocnumber">6.3</span> <span class="toctext">Others</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-15"><a href="#Limitations"><span class="tocnumber">7</span> <span class="toctext">Limitations</span></a></li>\n<li class="toclevel-1 tocsection-16"><a href="#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>\n<li class="toclevel-1 tocsection-17"><a href="#References"><span class="tocnumber">9</span> <span class="toctext">References</span></a></li>\n<li class="toclevel-1 tocsection-18"><a href="#External_links"><span class="tocnumber">10</span> <span class="toctext">External links</span></a></li>\n</ul>\n</div>\n<h2><span class="mw-headline" id="Reinforcement_learning">Reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=1" title="Edit section: Reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div>\n<p>Reinforcement learning involves an <a href="/wiki/Intelligent_agent" title="Intelligent agent">agent</a>, a set of <i>states</i> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle S}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>S</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle S}</annotation>\n</semantics>\n</math></span><img alt="S" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2" style="vertical-align: -0.338ex; width:1.499ex; height:2.176ex;"/></span>, and a set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle A}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>A</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle A}</annotation>\n</semantics>\n</math></span><img alt="A" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" style="vertical-align: -0.338ex; width:1.743ex; height:2.176ex;"/></span> of <i>actions</i> per state. By performing an action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle a\\in A}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>a</mi>\n<mo>\xe2\x88\x88<!-- \xe2\x88\x88 --></mo>\n<mi>A</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle a\\in A}</annotation>\n</semantics>\n</math></span><img alt="a\\in A" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a97387981adb5d65f74518e20b6785a284d7abd5" style="vertical-align: -0.338ex; width:5.814ex; height:2.176ex;"/></span>, the agent transitions from state to state. Executing an action in a specific state provides the agent with a <i>reward</i> (a numerical score).\n</p><p>The goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the <a href="/wiki/Expected_value" title="Expected value">expected values</a> of the rewards of all future steps starting from the current state.\n</p><p>As an example, consider the process of boarding a train, in which the reward is measured by the negative of the total time spent boarding (alternatively, the cost of boarding the train is equal to the boarding time). One strategy is to enter the train door as soon as they open, minimizing the initial wait time for yourself. If the train is crowded, however, then you will have a slow entry after the initial action of entering the door as people are fighting you to depart the train as you attempt to board. The total boarding time, or cost, is then:\n</p>\n<ul><li>0 seconds wait time + 15 seconds fight time</li></ul>\n<p>On the next day, by random chance (exploration), you decide to wait and let other people depart first. This initially results in a longer wait time. However, time fighting other passengers is less. Overall, this path has a higher reward than that of the previous day, since the total boarding time is now:\n</p>\n<ul><li>5 second wait time + 0 second fight time.</li></ul>\n<p>Through exploration, despite the initial (patient) action resulting in a larger cost (or negative reward) than in the forceful strategy, the overall cost is lower, thus revealing a more rewarding strategy.\n</p>\n<h2><span class="mw-headline" id="Algorithm">Algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=2" title="Edit section: Algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="thumb tright"><div class="thumbinner" style="width:442px;"><a class="image" href="/wiki/File:Q-Learning_Matrix_Initialized_and_After_Training.png"><img alt="" class="thumbimage" data-file-height="1016" data-file-width="1000" decoding="async" height="447" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Q-Learning_Matrix_Initialized_and_After_Training.png/440px-Q-Learning_Matrix_Initialized_and_After_Training.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Q-Learning_Matrix_Initialized_and_After_Training.png/660px-Q-Learning_Matrix_Initialized_and_After_Training.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Q-Learning_Matrix_Initialized_and_After_Training.png/880px-Q-Learning_Matrix_Initialized_and_After_Training.png 2x" width="440"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Q-Learning_Matrix_Initialized_and_After_Training.png" title="Enlarge"></a></div>Q-Learning table of states by actions that is initialized to zero, then each cell is updated through training.</div></div></div>\n<p>After <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Delta t}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi mathvariant="normal">\xce\x94<!-- \xce\x94 --></mi>\n<mi>t</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Delta t}</annotation>\n</semantics>\n</math></span><img alt="\\Delta t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8c28867ecd34e2caed12cf38feadf6a81a7ee542" style="vertical-align: -0.338ex; width:2.775ex; height:2.176ex;"/></span> steps into the future the agent will decide some next step. The weight for this step is calculated as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma ^{\\Delta t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="normal">\xce\x94<!-- \xce\x94 --></mi>\n<mi>t</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma ^{\\Delta t}}</annotation>\n</semantics>\n</math></span><img alt="\\gamma ^{{\\Delta t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b03cb6de5fe01243b53d0b622b4755f83fcc535" style="vertical-align: -0.838ex; width:3.475ex; height:3.176ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma }</annotation>\n</semantics>\n</math></span><img alt="\\gamma " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a223c880b0ce3da8f64ee33c4f0010beee400b1a" style="vertical-align: -0.838ex; width:1.262ex; height:2.176ex;"/></span> (the <i>discount factor</i>) is a number between 0 and 1 (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle 0\\leq \\gamma \\leq 1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mn>0</mn>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle 0\\leq \\gamma \\leq 1}</annotation>\n</semantics>\n</math></span><img alt="0\\leq \\gamma \\leq 1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/005a7c9599a70c20959e64abf585f73bdd474570" style="vertical-align: -0.838ex; width:9.784ex; height:2.676ex;"/></span>) and has the effect of valuing rewards received earlier higher than those received later (reflecting the value of a "good start"). <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma }</annotation>\n</semantics>\n</math></span><img alt="\\gamma " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a223c880b0ce3da8f64ee33c4f0010beee400b1a" style="vertical-align: -0.838ex; width:1.262ex; height:2.176ex;"/></span> may also be interpreted as the probability to succeed (or survive) at every step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\Delta t}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi mathvariant="normal">\xce\x94<!-- \xce\x94 --></mi>\n<mi>t</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\Delta t}</annotation>\n</semantics>\n</math></span><img alt="\\Delta t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8c28867ecd34e2caed12cf38feadf6a81a7ee542" style="vertical-align: -0.338ex; width:2.775ex; height:2.176ex;"/></span>.\n</p><p>The algorithm, therefore, has a function that calculates the quality of a state-action combination:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q:S\\times A\\to \\mathbb {R} }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Q</mi>\n<mo>:</mo>\n<mi>S</mi>\n<mo>\xc3\x97<!-- \xc3\x97 --></mo>\n<mi>A</mi>\n<mo stretchy="false">\xe2\x86\x92<!-- \xe2\x86\x92 --></mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="double-struck">R</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q:S\\times A\\to \\mathbb {R} }</annotation>\n</semantics>\n</math></span><img alt="Q:S\\times A\\to {\\mathbb  {R}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3c9001dc0d1aadc8841f816ac2261c3c59cd4c98" style="vertical-align: -0.671ex; width:15.15ex; height:2.509ex;"/></span> .</dd></dl>\n<p>Before learning begins, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Q</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q}</annotation>\n</semantics>\n</math></span><img alt="Q" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;"/></span> is initialized to a possibly arbitrary fixed value (chosen by the programmer). Then, at each time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>t</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle t}</annotation>\n</semantics>\n</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span> the agent selects an action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle a_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle a_{t}}</annotation>\n</semantics>\n</math></span><img alt="a_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/77fce84b535e9e195e3d30ce5ae09b372d87e2e9" style="vertical-align: -0.671ex; width:2.056ex; height:2.009ex;"/></span>, observes a reward <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle r_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>r</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle r_{t}}</annotation>\n</semantics>\n</math></span><img alt="r_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb555a4a6332d0b3c8f786c87eccda2e940936d5" style="vertical-align: -0.671ex; width:1.875ex; height:2.009ex;"/></span>, enters a new state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{t+1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{t+1}}</annotation>\n</semantics>\n</math></span><img alt="s_{t+1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04abef475db835d93c80da6dcbc6bf5acaac1329" style="vertical-align: -0.671ex; width:4.017ex; height:2.009ex;"/></span> (that may depend on both the previous state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{t}}</annotation>\n</semantics>\n</math></span><img alt="s_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/92a402d151a0173378ee252a634c77898ebe4b06" style="vertical-align: -0.671ex; width:1.916ex; height:2.009ex;"/></span> and the selected action), and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Q</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q}</annotation>\n</semantics>\n</math></span><img alt="Q" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;"/></span> is updated. The core of the algorithm is a <a href="/wiki/Bellman_equation" title="Bellman equation">Bellman equation</a> as a simple <a href="/wiki/Markov_decision_process#Value_iteration" title="Markov decision process">value iteration update</a>, using the weighted average of the old value and the new information:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q^{new}(s_{t},a_{t})\\leftarrow \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {\\underbrace {r_{t}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}} _{\\text{new value (temporal difference target)}}-\\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}{\\bigg )}} ^{\\text{temporal difference}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>e</mi>\n<mi>w</mi>\n</mrow>\n</msup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo stretchy="false">\xe2\x86\x90<!-- \xe2\x86\x90 --></mo>\n<munder>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<munder>\n<mrow>\n<mi>Q</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mrow>\n<mo>\xe2\x8f\x9f<!-- \xe2\x8f\x9f --></mo>\n</munder>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>old value</mtext>\n</mrow>\n</munder>\n<mo>+</mo>\n<munder>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<munder>\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mo>\xe2\x8f\x9f<!-- \xe2\x8f\x9f --></mo>\n</munder>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>learning rate</mtext>\n</mrow>\n</munder>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<mover>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<mover>\n<mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mo maxsize="2.047em" minsize="2.047em">(</mo>\n</mrow>\n</mrow>\n<munder>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<munder>\n<mrow>\n<munder>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<munder>\n<msub>\n<mi>r</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>\xe2\x8f\x9f<!-- \xe2\x8f\x9f --></mo>\n</munder>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>reward</mtext>\n</mrow>\n</munder>\n<mo>+</mo>\n<munder>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<munder>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mo>\xe2\x8f\x9f<!-- \xe2\x8f\x9f --></mo>\n</munder>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>discount factor</mtext>\n</mrow>\n</munder>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<munder>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<munder>\n<mrow>\n<munder>\n<mo form="prefix" movablelimits="true">max</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>a</mi>\n</mrow>\n</munder>\n<mi>Q</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>a</mi>\n<mo stretchy="false">)</mo>\n</mrow>\n<mo>\xe2\x8f\x9f<!-- \xe2\x8f\x9f --></mo>\n</munder>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>estimate of optimal future value</mtext>\n</mrow>\n</munder>\n</mrow>\n<mo>\xe2\x8f\x9f<!-- \xe2\x8f\x9f --></mo>\n</munder>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>new value (temporal difference target)</mtext>\n</mrow>\n</munder>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<munder>\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<munder>\n<mrow>\n<mi>Q</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mrow>\n<mo>\xe2\x8f\x9f<!-- \xe2\x8f\x9f --></mo>\n</munder>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>old value</mtext>\n</mrow>\n</munder>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mo maxsize="2.047em" minsize="2.047em">)</mo>\n</mrow>\n</mrow>\n</mrow>\n<mo>\xe2\x8f\x9e<!-- \xe2\x8f\x9e --></mo>\n</mover>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>temporal difference</mtext>\n</mrow>\n</mover>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q^{new}(s_{t},a_{t})\\leftarrow \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {\\underbrace {r_{t}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}} _{\\text{new value (temporal difference target)}}-\\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}{\\bigg )}} ^{\\text{temporal difference}}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle Q^{new}(s_{t},a_{t})\\leftarrow \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {\\underbrace {r_{t}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}} _{\\text{new value (temporal difference target)}}-\\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}{\\bigg )}} ^{\\text{temporal difference}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686" style="vertical-align: -9.671ex; margin-right: -0.028ex; width:96.268ex; height:16.843ex;"/></span></dd></dl>\n<p>where <i><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle r_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>r</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle r_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle r_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb555a4a6332d0b3c8f786c87eccda2e940936d5" style="vertical-align: -0.671ex; width:1.875ex; height:2.009ex;"/></span></i> is the reward received when moving from the state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{t}}</annotation>\n</semantics>\n</math></span><img alt="s_{{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/92a402d151a0173378ee252a634c77898ebe4b06" style="vertical-align: -0.671ex; width:1.916ex; height:2.009ex;"/></span> to the state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{t+1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{t+1}}</annotation>\n</semantics>\n</math></span><img alt="s_{t+1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04abef475db835d93c80da6dcbc6bf5acaac1329" style="vertical-align: -0.671ex; width:4.017ex; height:2.009ex;"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\alpha }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\alpha }</annotation>\n</semantics>\n</math></span><img alt="\\alpha " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3" style="vertical-align: -0.338ex; width:1.488ex; height:1.676ex;"/></span> is the <a href="/wiki/Learning_rate" title="Learning rate">learning rate</a> (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle 0&lt;\\alpha \\leq 1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mn>0</mn>\n<mo>&lt;</mo>\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mo>\xe2\x89\xa4<!-- \xe2\x89\xa4 --></mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle 0&lt;\\alpha \\leq 1}</annotation>\n</semantics>\n</math></span><img alt="0&lt;\\alpha \\leq 1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/46f3744f24aac421ebcecd035fe6d84f7d152740" style="vertical-align: -0.505ex; width:10.009ex; height:2.343ex;"/></span>).\n</p><p>Note that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q^{new}(s_{t},a_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>n</mi>\n<mi>e</mi>\n<mi>w</mi>\n</mrow>\n</msup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q^{new}(s_{t},a_{t})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle Q^{new}(s_{t},a_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1ad897b998d9633877adc8b631a5c04950d7acad" style="vertical-align: -0.838ex; width:11.815ex; height:2.843ex;"/></span> is the addition of three factors:\n</p>\n<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle (1-\\alpha )Q(s_{t},a_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mo stretchy="false">(</mo>\n<mn>1</mn>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mo stretchy="false">)</mo>\n<mi>Q</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle (1-\\alpha )Q(s_{t},a_{t})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle (1-\\alpha )Q(s_{t},a_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6a879f5413bd1cb72317c68b3da9466efeba62b4" style="vertical-align: -0.838ex; width:15.954ex; height:2.843ex;"/></span>: the current value weighted by the learning rate. Values of the learning rate near to 1 made faster the changes in Q.</li>\n<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\alpha \\,r_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mspace width="thinmathspace"></mspace>\n<msub>\n<mi>r</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\alpha \\,r_{t}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\alpha \\,r_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9208c4ff4917bbd74375ec8a93c74d782c045449" style="vertical-align: -0.671ex; width:3.749ex; height:2.009ex;"/></span>: the reward <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle r_{t}=r(s_{t},a_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>r</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mi>r</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle r_{t}=r(s_{t},a_{t})}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle r_{t}=r(s_{t},a_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ec4b05f24dc2bd92d866cdd7d948b7c8bbd0b670" style="vertical-align: -0.838ex; width:12.837ex; height:2.843ex;"/></span> to obtain if action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle a_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle a_{t}}</annotation>\n</semantics>\n</math></span><img alt="a_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/77fce84b535e9e195e3d30ce5ae09b372d87e2e9" style="vertical-align: -0.671ex; width:2.056ex; height:2.009ex;"/></span> is taken when in state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{t}}</annotation>\n</semantics>\n</math></span><img alt="s_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/92a402d151a0173378ee252a634c77898ebe4b06" style="vertical-align: -0.671ex; width:1.916ex; height:2.009ex;"/></span> (weighted by learning rate)</li>\n<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\alpha \\gamma \\max _{a}Q(s_{t+1},a)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<munder>\n<mo form="prefix" movablelimits="true">max</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>a</mi>\n</mrow>\n</munder>\n<mi>Q</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>a</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\alpha \\gamma \\max _{a}Q(s_{t+1},a)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\alpha \\gamma \\max _{a}Q(s_{t+1},a)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af4e3ab934ee81a1d6993259c4f4e43f608273a5" style="vertical-align: -2.005ex; width:17.778ex; height:4.009ex;"/></span>: the maximum reward that can be obtained from state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{t+1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{t+1}}</annotation>\n</semantics>\n</math></span><img alt="s_{t+1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04abef475db835d93c80da6dcbc6bf5acaac1329" style="vertical-align: -0.671ex; width:4.017ex; height:2.009ex;"/></span>(weighted by learning rate and discount factor)</li></ul>\n<p>An episode of the algorithm ends when state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{t+1}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{t+1}}</annotation>\n</semantics>\n</math></span><img alt="s_{t+1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04abef475db835d93c80da6dcbc6bf5acaac1329" style="vertical-align: -0.671ex; width:4.017ex; height:2.009ex;"/></span> is a final or <i>terminal state</i>. However, <i>Q</i>-learning can also learn in non-episodic tasks.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2017)">citation needed</span></a></i>]</sup> If the discount factor is lower than 1, the action values are finite even if the problem can contain infinite loops.\n</p><p>For all final states <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{f}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>f</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{f}}</annotation>\n</semantics>\n</math></span><img alt="s_{f}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/861eb1d6654456882863ba0b53ca9ab551ad903c" style="vertical-align: -1.005ex; width:2.227ex; height:2.343ex;"/></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q(s_{f},a)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Q</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>f</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>a</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q(s_{f},a)}</annotation>\n</semantics>\n</math></span><img alt="Q(s_{f},a)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/92e2c5c3189c29875e4ab2a1b41a54479f2ebe74" style="vertical-align: -1.005ex; width:8.138ex; height:3.009ex;"/></span> is never updated, but is set to the reward value <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle r}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>r</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle r}</annotation>\n</semantics>\n</math></span><img alt="r" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d1ecb613aa2984f0576f70f86650b7c2a132538" style="vertical-align: -0.338ex; width:1.049ex; height:1.676ex;"/></span> observed for state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle s_{f}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>f</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle s_{f}}</annotation>\n</semantics>\n</math></span><img alt="s_{f}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/861eb1d6654456882863ba0b53ca9ab551ad903c" style="vertical-align: -1.005ex; width:2.227ex; height:2.343ex;"/></span>. In most cases, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q(s_{f},a)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Q</mi>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>f</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>a</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q(s_{f},a)}</annotation>\n</semantics>\n</math></span><img alt="Q(s_{f},a)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/92e2c5c3189c29875e4ab2a1b41a54479f2ebe74" style="vertical-align: -1.005ex; width:8.138ex; height:3.009ex;"/></span> can be taken to equal zero.\n</p>\n<h2><span class="mw-headline" id="Influence_of_variables">Influence of variables</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=3" title="Edit section: Influence of variables">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<h3><span class="mw-headline" id="Learning_Rate">Learning Rate</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=4" title="Edit section: Learning Rate">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>The <a href="/wiki/Learning_rate" title="Learning rate">learning rate</a> or <i>step size</i> determines to what extent newly acquired information overrides old information. A factor of 0 makes the agent learn nothing (exclusively exploiting prior knowledge), while a factor of 1 makes the agent consider only the most recent information (ignoring prior knowledge to explore possibilities). In fully <a href="/wiki/Deterministic_system" title="Deterministic system">deterministic</a> environments, a learning rate of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\alpha _{t}=1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\alpha _{t}=1}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\alpha _{t}=1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b6be8b0004058b07e1a81a9d4840948844ccddc9" style="vertical-align: -0.671ex; width:6.574ex; height:2.509ex;"/></span> is optimal. When the problem is <a class="mw-redirect" href="/wiki/Stochastic_systems" title="Stochastic systems">stochastic</a>, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. In practice, often a constant learning rate is used, such as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\alpha _{t}=0.1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<mn>0.1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\alpha _{t}=0.1}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\alpha _{t}=0.1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2eebc9fd3f841f6766ade351b95aaee0d00df927" style="vertical-align: -0.671ex; width:8.384ex; height:2.509ex;"/></span> for all <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>t</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle t}</annotation>\n</semantics>\n</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span>.<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Discount_factor">Discount factor</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=5" title="Edit section: Discount factor">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>The discount factor <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma }" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma }</annotation>\n</semantics>\n</math></span><img alt="\\gamma " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a223c880b0ce3da8f64ee33c4f0010beee400b1a" style="vertical-align: -0.838ex; width:1.262ex; height:2.176ex;"/></span> determines the importance of future rewards. A factor of 0 will make the agent "myopic" (or short-sighted) by only considering current rewards, i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle r_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>r</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle r_{t}}</annotation>\n</semantics>\n</math></span><img alt="r_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb555a4a6332d0b3c8f786c87eccda2e940936d5" style="vertical-align: -0.671ex; width:1.875ex; height:2.009ex;"/></span> (in the update rule above), while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\gamma =1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<mo>=</mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\gamma =1}</annotation>\n</semantics>\n</math></span><img alt="\\gamma =1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5682ebb86d6f024a15f4a2c1c7cb08412720bcaf" style="vertical-align: -0.838ex; width:5.523ex; height:2.676ex;"/></span>, without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite.<sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup> Even with a discount factor only slightly lower than 1, <i>Q</i>-function learning leads to propagation of errors and instabilities when the value function is approximated with an <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a>.<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> In that case, starting with a lower discount factor and increasing it towards its final value accelerates learning.<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup>\n</p>\n<h3><span id="Initial_conditions_.28Q0.29"></span><span class="mw-headline" id="Initial_conditions_(Q0)">Initial conditions (<i>Q</i><sub>0</sub>)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=6" title="Edit section: Initial conditions (Q0)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Since <i>Q</i>-learning is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. High initial values, also known as "optimistic initial conditions",<sup class="reference" id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup> can encourage exploration: no matter what action is selected, the update rule will cause it to have lower values than the other alternative, thus increasing their choice probability. The first reward <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle r}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>r</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle r}</annotation>\n</semantics>\n</math></span><img alt="r" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d1ecb613aa2984f0576f70f86650b7c2a132538" style="vertical-align: -0.338ex; width:1.049ex; height:1.676ex;"/></span> can be used to reset the initial conditions.<sup class="reference" id="cite_ref-hshteingart_8-0"><a href="#cite_note-hshteingart-8">[8]</a></sup> According to this idea, the first time an action is taken the reward is used to set the value of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Q</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q}</annotation>\n</semantics>\n</math></span><img alt="Q" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;"/></span>. This allows immediate learning in case of fixed deterministic rewards. A model that incorporates <i>reset of initial conditions</i> (RIC) is expected to predict participants\' behavior better than a model that assumes any <i>arbitrary initial condition</i> (AIC).<sup class="reference" id="cite_ref-hshteingart_8-1"><a href="#cite_note-hshteingart-8">[8]</a></sup> RIC seems to be consistent with human behaviour in repeated binary choice experiments.<sup class="reference" id="cite_ref-hshteingart_8-2"><a href="#cite_note-hshteingart-8">[8]</a></sup>\n</p>\n<h2><span class="mw-headline" id="Implementation">Implementation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=7" title="Edit section: Implementation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p><i>Q</i>-learning at its simplest stores data in tables. This approach falters with increasing numbers of states/actions since the likelihood of the agent visiting a particular state and performing a particular action is increasingly small.\n</p>\n<h3><span class="mw-headline" id="Function_approximation">Function approximation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=8" title="Edit section: Function approximation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p><i>Q</i>-learning can be combined with <a href="/wiki/Function_approximation" title="Function approximation">function approximation</a>.<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> This makes it possible to apply the algorithm to larger problems, even when the state space is continuous.\n</p><p>One solution is to use an (adapted) <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> as a function approximator.<sup class="reference" id="cite_ref-CACM_10-0"><a href="#cite_note-CACM-10">[10]</a></sup> Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states.\n</p>\n<h3><span class="mw-headline" id="Quantization">Quantization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=9" title="Edit section: Quantization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Another technique to decrease the state/action space quantizes possible values. Consider the example of learning to balance a stick on a finger. To describe a state at a certain point in time involves the position of the finger in space, its velocity, the angle of the stick and the <a href="/wiki/Angular_velocity" title="Angular velocity">angular velocity</a> of the stick. This yields a four-element vector that describes one state, i.e. a snapshot of one state encoded into four values. The problem is that infinitely many possible states are present. To shrink the possible space of valid actions multiple values can be assigned to a bucket. The exact distance of the finger from its starting position (-Infinity to Infinity) is not known, but rather whether it is far away or not (Near, Far).\n</p>\n<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=10" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p><i>Q</i>-learning was introduced by <a class="new" href="/w/index.php?title=Chris_Watkins&amp;action=edit&amp;redlink=1" title="Chris Watkins (page does not exist)">Chris Watkins</a><sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup> in 1989. A convergence proof was presented by Watkins and Dayan<sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> in 1992.\n</p><p>Watkins was addressing \xe2\x80\x9cLearning from delayed rewards\xe2\x80\x9d, the title of his PhD Thesis. Eight years earlier in 1981 the same problem, under the name of \xe2\x80\x9cDelayed reinforcement learning\xe2\x80\x9d, was solved by Bozinovski\'s Crossbar Adaptive Array (CAA).<sup class="reference" id="cite_ref-DobnikarSteele1999_13-0"><a href="#cite_note-DobnikarSteele1999-13">[13]</a></sup><sup class="reference" id="cite_ref-Trappl1982_14-0"><a href="#cite_note-Trappl1982-14">[14]</a></sup> The memory matrix W =||w(a,s)|| was the same as the eight years later Q-table of Q-learning. The architecture introduced the term \xe2\x80\x9cstate evaluation\xe2\x80\x9d in reinforcement learning. The crossbar learning algorithm, written in mathematical <a href="/wiki/Pseudocode" title="Pseudocode">pseudocode</a> in the paper, in each iteration performs the following computation:\n</p>\n<ul><li>In state s perform action a;</li>\n<li>Receive consequence state s\xe2\x80\x99;</li>\n<li>Compute state evaluation v(s\xe2\x80\x99);</li>\n<li>Update crossbar value w\xe2\x80\x99(a,s) = w(a,s) + v(s\xe2\x80\x99).</li></ul>\n<p>The term \xe2\x80\x9csecondary reinforcement\xe2\x80\x9d is borrowed from animal learning theory, to model state values via <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>: the state value v(s\xe2\x80\x99) of the consequence situation is backpropagated to the previously encountered situations. CAA computes state values vertically and actions horizontally (the "crossbar"). Demonstration graphs showing delayed reinforcement learning contained states (desirable, undesirable, and neutral states), which were computed by the state evaluation function. This learning system was a forerunner of the Q-learning algorithm.<sup class="reference" id="cite_ref-OmidvarElliott1997_15-0"><a href="#cite_note-OmidvarElliott1997-15">[15]</a></sup>\n</p><p>In 2014 <a class="mw-redirect" href="/wiki/Google_DeepMind" title="Google DeepMind">Google DeepMind</a> patented<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup> an application of Q-learning to <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>, titled "deep reinforcement learning" or "deep Q-learning" that can play <a href="/wiki/Atari_2600" title="Atari 2600">Atari 2600</a> games at expert human levels.\n</p>\n<h2><span class="mw-headline" id="Variants">Variants</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=11" title="Edit section: Variants">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<h3><span class="mw-headline" id="Deep_Q-learning">Deep Q-learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=12" title="Edit section: Deep Q-learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>The DeepMind system used a deep <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural network</a>, with layers of tiled <a href="/wiki/Convolution" title="Convolution">convolutional</a> filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values.\n</p><p>The technique used <i>experience replay,</i> a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.<sup class="reference" id="cite_ref-:0_2-1"><a href="#cite_note-:0-2">[2]</a></sup> This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative updates adjust Q towards target values that are only periodically updated, further reducing correlations with the target.<sup class="reference" id="cite_ref-DQN_17-0"><a href="#cite_note-DQN-17">[17]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Double_Q-learning">Double Q-learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=13" title="Edit section: Double Q-learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Because the future maximum approximated action value in Q-learning is evaluated using the same Q function as in current action selection policy, in noisy environments Q-learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this. Double Q-learning<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> is an <a class="new" href="/w/index.php?title=Off-policy&amp;action=edit&amp;redlink=1" title="Off-policy (page does not exist)">off-policy</a> reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.\n</p><p>In practice, two separate value functions are trained in a mutually symmetric fashion using separate experiences, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q^{A}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>A</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q^{A}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle Q^{A}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/587c22643cd3bddd90ed5f1f05a9b1aa51ba6a81" style="vertical-align: -0.671ex; width:3.303ex; height:3.009ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q^{B}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>B</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q^{B}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle Q^{B}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/255994667943473a103d1113c29fc299392b73ec" style="vertical-align: -0.671ex; width:3.318ex; height:3.009ex;"/></span>. The double Q-learning update step is then as follows:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q_{t+1}^{A}(s_{t},a_{t})=Q_{t}^{A}(s_{t},a_{t})+\\alpha _{t}(s_{t},a_{t})\\left(r_{t}+\\gamma Q_{t}^{B}\\left(s_{t+1},\\mathop {\\operatorname {arg~max} } _{a}Q_{t}^{A}(s_{t+1},a)\\right)-Q_{t}^{A}(s_{t},a_{t})\\right)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>A</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>A</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>+</mo>\n<msub>\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mrow>\n<mo>(</mo>\n<mrow>\n<msub>\n<mi>r</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>+</mo>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>B</mi>\n</mrow>\n</msubsup>\n<mrow>\n<mo>(</mo>\n<mrow>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<munder>\n<mrow class="MJX-TeXAtom-OP">\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<mi mathvariant="normal">a</mi>\n<mi mathvariant="normal">r</mi>\n<mi mathvariant="normal">g</mi>\n<mtext>\xc2\xa0</mtext>\n<mi mathvariant="normal">m</mi>\n<mi mathvariant="normal">a</mi>\n<mi mathvariant="normal">x</mi>\n</mrow>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>a</mi>\n</mrow>\n</munder>\n<mo>\xe2\x81\xa1<!-- \xe2\x81\xa1 --></mo>\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>A</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>a</mi>\n<mo stretchy="false">)</mo>\n</mrow>\n<mo>)</mo>\n</mrow>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>A</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mrow>\n<mo>)</mo>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q_{t+1}^{A}(s_{t},a_{t})=Q_{t}^{A}(s_{t},a_{t})+\\alpha _{t}(s_{t},a_{t})\\left(r_{t}+\\gamma Q_{t}^{B}\\left(s_{t+1},\\mathop {\\operatorname {arg~max} } _{a}Q_{t}^{A}(s_{t+1},a)\\right)-Q_{t}^{A}(s_{t},a_{t})\\right)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle Q_{t+1}^{A}(s_{t},a_{t})=Q_{t}^{A}(s_{t},a_{t})+\\alpha _{t}(s_{t},a_{t})\\left(r_{t}+\\gamma Q_{t}^{B}\\left(s_{t+1},\\mathop {\\operatorname {arg~max} } _{a}Q_{t}^{A}(s_{t+1},a)\\right)-Q_{t}^{A}(s_{t},a_{t})\\right)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4941acabf5144d1b3e9c271606011abdc0df444d" style="vertical-align: -2.505ex; width:91.612ex; height:6.176ex;"/></span>, and</dd>\n<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q_{t+1}^{B}(s_{t},a_{t})=Q_{t}^{B}(s_{t},a_{t})+\\alpha _{t}(s_{t},a_{t})\\left(r_{t}+\\gamma Q_{t}^{A}\\left(s_{t+1},\\mathop {\\operatorname {arg~max} } _{a}Q_{t}^{B}(s_{t+1},a)\\right)-Q_{t}^{B}(s_{t},a_{t})\\right).}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>B</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>B</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mo>+</mo>\n<msub>\n<mi>\xce\xb1<!-- \xce\xb1 --></mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n<mrow>\n<mo>(</mo>\n<mrow>\n<msub>\n<mi>r</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>+</mo>\n<mi>\xce\xb3<!-- \xce\xb3 --></mi>\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>A</mi>\n</mrow>\n</msubsup>\n<mrow>\n<mo>(</mo>\n<mrow>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<munder>\n<mrow class="MJX-TeXAtom-OP">\n<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n<mi mathvariant="normal">a</mi>\n<mi mathvariant="normal">r</mi>\n<mi mathvariant="normal">g</mi>\n<mtext>\xc2\xa0</mtext>\n<mi mathvariant="normal">m</mi>\n<mi mathvariant="normal">a</mi>\n<mi mathvariant="normal">x</mi>\n</mrow>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>a</mi>\n</mrow>\n</munder>\n<mo>\xe2\x81\xa1<!-- \xe2\x81\xa1 --></mo>\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>B</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mo>+</mo>\n<mn>1</mn>\n</mrow>\n</msub>\n<mo>,</mo>\n<mi>a</mi>\n<mo stretchy="false">)</mo>\n</mrow>\n<mo>)</mo>\n</mrow>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<msubsup>\n<mi>Q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>B</mi>\n</mrow>\n</msubsup>\n<mo stretchy="false">(</mo>\n<msub>\n<mi>s</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n</mrow>\n</msub>\n<mo stretchy="false">)</mo>\n</mrow>\n<mo>)</mo>\n</mrow>\n<mo>.</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q_{t+1}^{B}(s_{t},a_{t})=Q_{t}^{B}(s_{t},a_{t})+\\alpha _{t}(s_{t},a_{t})\\left(r_{t}+\\gamma Q_{t}^{A}\\left(s_{t+1},\\mathop {\\operatorname {arg~max} } _{a}Q_{t}^{B}(s_{t+1},a)\\right)-Q_{t}^{B}(s_{t},a_{t})\\right).}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle Q_{t+1}^{B}(s_{t},a_{t})=Q_{t}^{B}(s_{t},a_{t})+\\alpha _{t}(s_{t},a_{t})\\left(r_{t}+\\gamma Q_{t}^{A}\\left(s_{t+1},\\mathop {\\operatorname {arg~max} } _{a}Q_{t}^{B}(s_{t+1},a)\\right)-Q_{t}^{B}(s_{t},a_{t})\\right).}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3e37476013126ddd4afdba69ef7b03767f4c4b75" style="vertical-align: -2.505ex; width:92.675ex; height:6.176ex;"/></span></dd></dl>\n<p>Now the estimated value of the discounted future is evaluated using a different policy, which solves the overestimation issue.\n</p><p>This algorithm was later modified<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (January 2020)">clarification needed</span></a></i>]</sup> in 2015 and combined with <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>, as in the DQN algorithm, resulting in Double DQN, which outperforms the original DQN algorithm.<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Others">Others</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=14" title="Edit section: Others">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Delayed Q-learning is an alternative implementation of the online <i>Q</i>-learning algorithm, with <a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">probably approximately correct (PAC) learning</a>.<sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup>\n</p><p>Greedy GQ is a variant of <i>Q</i>-learning to use in combination with (linear) function approximation.<sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> The advantage of Greedy GQ is that convergence is guaranteed even when function approximation is used to estimate the action values.\n</p>\n<h2><span class="mw-headline" id="Limitations">Limitations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=15" title="Edit section: Limitations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The standard Q-learning algorithm (using a <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Q</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q}</annotation>\n</semantics>\n</math></span><img alt="Q" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;"/></span> table) applies only to discrete action and state spaces. <a href="/wiki/Discretization" title="Discretization">Discretization</a> of these values leads to inefficient learning, largely due to the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>. However, there are adaptations of Q-learning that attempt solve this problem such as Wire-fitted Neural Network Q-Learning.<sup class="reference" id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>\n</p>\n<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=16" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference learning</a></li>\n<li><a class="mw-redirect" href="/wiki/State-Action-Reward-State-Action" title="State-Action-Reward-State-Action">SARSA</a></li>\n<li><a href="/wiki/Prisoner%27s_dilemma#The_iterated_prisoner.27s_dilemma" title="Prisoner\'s dilemma">Iterated prisoner\'s dilemma</a></li>\n<li><a href="/wiki/Game_theory" title="Game theory">Game theory</a></li></ul>\n<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=17" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">\n<ol class="references">\n<li id="cite_note-auto-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-auto_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-auto_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFMelo">Melo, Francisco S. <a class="external text" href="http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf" rel="nofollow">"Convergence of Q-learning: a simple proof"</a> <span class="cs1-format">(PDF)</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Convergence+of+Q-learning%3A+a+simple+proof&amp;rft.aulast=Melo&amp;rft.aufirst=Francisco+S.&amp;rft_id=http%3A%2F%2Fusers.isr.ist.utl.pt%2F~mtjspaan%2FreadingGroup%2FProofQlearning.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span> <span class="cs1-hidden-error error citation-comment">Cite journal requires <code class="cs1-code">|journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span><style data-mw-deduplicate="TemplateStyles:r982806391">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>\n</li>\n<li id="cite_note-:0-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFMatiisen2015">Matiisen, Tambet (December 19, 2015). <a class="external text" href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" rel="nofollow">"Demystifying Deep Reinforcement Learning"</a>. <i>neuro.cs.ut.ee</i>. Computational Neuroscience Lab<span class="reference-accessdate">. Retrieved <span class="nowrap">2018-04-06</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=neuro.cs.ut.ee&amp;rft.atitle=Demystifying+Deep+Reinforcement+Learning&amp;rft.date=2015-12-19&amp;rft.aulast=Matiisen&amp;rft.aufirst=Tambet&amp;rft_id=http%3A%2F%2Fneuro.cs.ut.ee%2Fdemystifying-deep-reinforcement-learning%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFSuttonBarto1998">Sutton, Richard; Barto, Andrew (1998). <a class="external text" href="http://incompleteideas.net/sutton/book/ebook/the-book.html" rel="nofollow"><i>Reinforcement Learning: An Introduction</i></a>. MIT Press.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Reinforcement+Learning%3A+An+Introduction&amp;rft.pub=MIT+Press&amp;rft.date=1998&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard&amp;rft.au=Barto%2C+Andrew&amp;rft_id=http%3A%2F%2Fincompleteideas.net%2Fsutton%2Fbook%2Febook%2Fthe-book.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFRussellNorvig2010"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart J.</a>; <a href="/wiki/Peter_Norvig" title="Peter Norvig">Norvig, Peter</a> (2010). <i>Artificial Intelligence: A Modern Approach</i> (Third ed.). <a href="/wiki/Prentice_Hall" title="Prentice Hall">Prentice Hall</a>. p.\xc2\xa0649. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0136042594" title="Special:BookSources/978-0136042594"><bdi>978-0136042594</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.pages=649&amp;rft.edition=Third&amp;rft.pub=Prentice+Hall&amp;rft.date=2010&amp;rft.isbn=978-0136042594&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBaird1995">Baird, Leemon (1995). <a class="external text" href="http://www.leemon.com/papers/1995b.pdf" rel="nofollow">"Residual algorithms: Reinforcement learning with function approximation"</a> <span class="cs1-format">(PDF)</span>. <i>ICML</i>: 30\xe2\x80\x9337.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICML&amp;rft.atitle=Residual+algorithms%3A+Reinforcement+learning+with+function+approximation&amp;rft.pages=30-37&amp;rft.date=1995&amp;rft.aulast=Baird&amp;rft.aufirst=Leemon&amp;rft_id=http%3A%2F%2Fwww.leemon.com%2Fpapers%2F1995b.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFFran\xc3\xa7ois-LavetFonteneauErnst2015">Fran\xc3\xa7ois-Lavet, Vincent; Fonteneau, Raphael; Ernst, Damien (2015-12-07). "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1512.02011" rel="nofollow">1512.02011</a></span> [<a class="external text" href="//arxiv.org/archive/cs.LG" rel="nofollow">cs.LG</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=How+to+Discount+Deep+Reinforcement+Learning%3A+Towards+New+Dynamic+Strategies&amp;rft.date=2015-12-07&amp;rft_id=info%3Aarxiv%2F1512.02011&amp;rft.aulast=Fran%C3%A7ois-Lavet&amp;rft.aufirst=Vincent&amp;rft.au=Fonteneau%2C+Raphael&amp;rft.au=Ernst%2C+Damien&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFSuttonBarto">Sutton, Richard S.; Barto, Andrew G. <a class="external text" href="https://web.archive.org/web/20130908031737/http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node21.html" rel="nofollow">"2.7 Optimistic Initial Values"</a>. <i>Reinforcement Learning: An Introduction</i>. Archived from <a class="external text" href="http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node21.html" rel="nofollow">the original</a> on 2013-09-08<span class="reference-accessdate">. Retrieved <span class="nowrap">2013-07-18</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=2.7+Optimistic+Initial+Values&amp;rft.btitle=Reinforcement+Learning%3A+An+Introduction&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard+S.&amp;rft.au=Barto%2C+Andrew+G.&amp;rft_id=http%3A%2F%2Fwebdocs.cs.ualberta.ca%2F~sutton%2Fbook%2Febook%2Fnode21.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-hshteingart-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-hshteingart_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-hshteingart_8-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-hshteingart_8-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFShteingartNeimanLoewenstein2013">Shteingart, Hanan; Neiman, Tal; Loewenstein, Yonatan (May 2013). <a class="external text" href="http://ratio.huji.ac.il/sites/default/files/publications/dp626.pdf" rel="nofollow">"The role of first impression in operant learning"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Experimental Psychology: General</i>. <b>142</b> (2): 476\xe2\x80\x93488. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1037%2Fa0029550" rel="nofollow">10.1037/a0029550</a>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>\xc2\xa0<a class="external text" href="//www.worldcat.org/issn/1939-2222" rel="nofollow">1939-2222</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>\xc2\xa0<a class="external text" href="//pubmed.ncbi.nlm.nih.gov/22924882" rel="nofollow">22924882</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+Psychology%3A+General&amp;rft.atitle=The+role+of+first+impression+in+operant+learning.&amp;rft.volume=142&amp;rft.issue=2&amp;rft.pages=476-488&amp;rft.date=2013-05&amp;rft.issn=1939-2222&amp;rft_id=info%3Apmid%2F22924882&amp;rft_id=info%3Adoi%2F10.1037%2Fa0029550&amp;rft.aulast=Shteingart&amp;rft.aufirst=Hanan&amp;rft.au=Neiman%2C+Tal&amp;rft.au=Loewenstein%2C+Yonatan&amp;rft_id=http%3A%2F%2Fratio.huji.ac.il%2Fsites%2Fdefault%2Ffiles%2Fpublications%2Fdp626.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFHasselt2012">Hasselt, Hado van (5 March 2012). <a class="external text" href="https://books.google.com/books?id=YPjNuvrJR0MC" rel="nofollow">"Reinforcement Learning in Continuous State and Action Spaces"</a>.  In Wiering, Marco; Otterlo, Martijn van (eds.). <i>Reinforcement Learning: State-of-the-Art</i>. Springer Science &amp; Business Media. pp.\xc2\xa0207\xe2\x80\x93251. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-3-642-27645-3" title="Special:BookSources/978-3-642-27645-3"><bdi>978-3-642-27645-3</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Reinforcement+Learning+in+Continuous+State+and+Action+Spaces&amp;rft.btitle=Reinforcement+Learning%3A+State-of-the-Art&amp;rft.pages=207-251&amp;rft.pub=Springer+Science+%26+Business+Media&amp;rft.date=2012-03-05&amp;rft.isbn=978-3-642-27645-3&amp;rft.aulast=Hasselt&amp;rft.aufirst=Hado+van&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DYPjNuvrJR0MC&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-CACM-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-CACM_10-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFTesauro1995">Tesauro, Gerald (March 1995). <a class="external text" href="http://www.bkgm.com/articles/tesauro/tdl.html" rel="nofollow">"Temporal Difference Learning and TD-Gammon"</a>. <i>Communications of the ACM</i>. <b>38</b> (3): 58\xe2\x80\x9368. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1145%2F203330.203343" rel="nofollow">10.1145/203330.203343</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>\xc2\xa0<a class="external text" href="https://api.semanticscholar.org/CorpusID:8763243" rel="nofollow">8763243</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2010-02-08</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.atitle=Temporal+Difference+Learning+and+TD-Gammon&amp;rft.volume=38&amp;rft.issue=3&amp;rft.pages=58-68&amp;rft.date=1995-03&amp;rft_id=info%3Adoi%2F10.1145%2F203330.203343&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A8763243&amp;rft.aulast=Tesauro&amp;rft.aufirst=Gerald&amp;rft_id=http%3A%2F%2Fwww.bkgm.com%2Farticles%2Ftesauro%2Ftdl.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation cs2" id="CITEREFWatkins1989">Watkins, C.J.C.H. (1989), <a class="external text" href="http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf" rel="nofollow"><i>Learning from Delayed Rewards</i></a> <span class="cs1-format">(PDF)</span> (Ph.D. thesis), Cambridge University</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Learning+from+Delayed+Rewards&amp;rft.pub=Cambridge+University&amp;rft.date=1989&amp;rft.aulast=Watkins&amp;rft.aufirst=C.J.C.H.&amp;rft_id=http%3A%2F%2Fwww.cs.rhul.ac.uk%2F~chrisw%2Fnew_thesis.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text">Watkins and Dayan, C.J.C.H., (1992), \'Q-learning.Machine Learning\'</span>\n</li>\n<li id="cite_note-DobnikarSteele1999-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-DobnikarSteele1999_13-0">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFBozinovski1999">Bozinovski, S. (15 July 1999). <a class="external text" href="https://books.google.com/books?id=clKwynlfZYkC&amp;pg=PA320-325" rel="nofollow">"Crossbar Adaptive Array: The first connectionist network that solved the delayed reinforcement learning problem"</a>.  In Dobnikar, Andrej; Steele, Nigel C.; Pearson, David W.; Albrecht, Rudolf F. (eds.). <i>Artificial Neural Nets and Genetic Algorithms: Proceedings of the International Conference in Portoro\xc5\xbe, Slovenia, 1999</i>. Springer Science &amp; Business Media. pp.\xc2\xa0320\xe2\x80\x93325. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-3-211-83364-3" title="Special:BookSources/978-3-211-83364-3"><bdi>978-3-211-83364-3</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Crossbar+Adaptive+Array%3A+The+first+connectionist+network+that+solved+the+delayed+reinforcement+learning+problem&amp;rft.btitle=Artificial+Neural+Nets+and+Genetic+Algorithms%3A+Proceedings+of+the+International+Conference+in+Portoro%C5%BE%2C+Slovenia%2C+1999&amp;rft.pages=320-325&amp;rft.pub=Springer+Science+%26+Business+Media&amp;rft.date=1999-07-15&amp;rft.isbn=978-3-211-83364-3&amp;rft.aulast=Bozinovski&amp;rft.aufirst=S.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DclKwynlfZYkC%26pg%3DPA320-325&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-Trappl1982-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-Trappl1982_14-0">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFBozinovski1982">Bozinovski, S. (1982). <a class="external text" href="https://books.google.com/books?id=mGtQAAAAMAAJ&amp;pg=PA397" rel="nofollow">"A self learning system using secondary reinforcement"</a>.  In Trappl, Robert (ed.). <i>Cybernetics and Systems Research: Proceedings of the Sixth European Meeting on Cybernetics and Systems Research</i>. North Holland. pp.\xc2\xa0397\xe2\x80\x93402. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0-444-86488-8" title="Special:BookSources/978-0-444-86488-8"><bdi>978-0-444-86488-8</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+self+learning+system+using+secondary+reinforcement&amp;rft.btitle=Cybernetics+and+Systems+Research%3A+Proceedings+of+the+Sixth+European+Meeting+on+Cybernetics+and+Systems+Research&amp;rft.pages=397-402&amp;rft.pub=North+Holland&amp;rft.date=1982&amp;rft.isbn=978-0-444-86488-8&amp;rft.aulast=Bozinovski&amp;rft.aufirst=S.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DmGtQAAAAMAAJ%26pg%3DPA397&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-OmidvarElliott1997-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-OmidvarElliott1997_15-0">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFBarto1997">Barto, A. (24 February 1997). <a class="external text" href="https://books.google.com/books?id=oLcAiySCow0C" rel="nofollow">"Reinforcement learning"</a>.  In Omidvar, Omid; Elliott, David L. (eds.). <i>Neural Systems for Control</i>. Elsevier. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>\xc2\xa0<a href="/wiki/Special:BookSources/978-0-08-053739-9" title="Special:BookSources/978-0-08-053739-9"><bdi>978-0-08-053739-9</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Reinforcement+learning&amp;rft.btitle=Neural+Systems+for+Control&amp;rft.pub=Elsevier&amp;rft.date=1997-02-24&amp;rft.isbn=978-0-08-053739-9&amp;rft.aulast=Barto&amp;rft.aufirst=A.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DoLcAiySCow0C&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="https://patentimages.storage.googleapis.com/71/91/4a/c5cf4ffa56f705/US20150100530A1.pdf" rel="nofollow">"Methods and Apparatus for Reinforcement Learning, US Patent #20150100530A1"</a> <span class="cs1-format">(PDF)</span>. US Patent Office. 9 April 2015<span class="reference-accessdate">. Retrieved <span class="nowrap">28 July</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Methods+and+Apparatus+for+Reinforcement+Learning%2C+US+Patent+%2320150100530A1&amp;rft.pub=US+Patent+Office&amp;rft.date=2015-04-09&amp;rft_id=https%3A%2F%2Fpatentimages.storage.googleapis.com%2F71%2F91%2F4a%2Fc5cf4ffa56f705%2FUS20150100530A1.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-DQN-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-DQN_17-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFMnihKavukcuogluSilverRusu2015">Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Rusu, Andrei A.; Veness, Joel; Bellemare, Marc G.; Graves, Alex; Riedmiller, Martin; Fidjeland, Andreas K. (Feb 2015). "Human-level control through deep reinforcement learning". <i>Nature</i>. <b>518</b> (7540): 529\xe2\x80\x93533. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2015Natur.518..529M" rel="nofollow">2015Natur.518..529M</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1038%2Fnature14236" rel="nofollow">10.1038/nature14236</a>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>\xc2\xa0<a class="external text" href="//www.worldcat.org/issn/0028-0836" rel="nofollow">0028-0836</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>\xc2\xa0<a class="external text" href="//pubmed.ncbi.nlm.nih.gov/25719670" rel="nofollow">25719670</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>\xc2\xa0<a class="external text" href="https://api.semanticscholar.org/CorpusID:205242740" rel="nofollow">205242740</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Human-level+control+through+deep+reinforcement+learning&amp;rft.volume=518&amp;rft.issue=7540&amp;rft.pages=529-533&amp;rft.date=2015-02&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A205242740&amp;rft_id=info%3Abibcode%2F2015Natur.518..529M&amp;rft.issn=0028-0836&amp;rft_id=info%3Adoi%2F10.1038%2Fnature14236&amp;rft_id=info%3Apmid%2F25719670&amp;rft.aulast=Mnih&amp;rft.aufirst=Volodymyr&amp;rft.au=Kavukcuoglu%2C+Koray&amp;rft.au=Silver%2C+David&amp;rft.au=Rusu%2C+Andrei+A.&amp;rft.au=Veness%2C+Joel&amp;rft.au=Bellemare%2C+Marc+G.&amp;rft.au=Graves%2C+Alex&amp;rft.au=Riedmiller%2C+Martin&amp;rft.au=Fidjeland%2C+Andreas+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFvan_Hasselt2011">van Hasselt, Hado (2011). <a class="external text" href="http://papers.nips.cc/paper/3964-double-q-learning" rel="nofollow">"Double Q-learning"</a> <span class="cs1-format">(PDF)</span>. <i>Advances in Neural Information Processing Systems</i>. <b>23</b>: 2613\xe2\x80\x932622.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Double+Q-learning&amp;rft.volume=23&amp;rft.pages=2613-2622&amp;rft.date=2011&amp;rft.aulast=van+Hasselt&amp;rft.aufirst=Hado&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F3964-double-q-learning&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFvan_HasseltGuezSilver2015">van Hasselt, Hado; Guez, Arthur; Silver, David (2015). <a class="external text" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847" rel="nofollow">"Deep reinforcement learning with double Q-learning"</a> <span class="cs1-format">(PDF)</span>. <i>AAAI Conference on Artificial Intelligence</i>: 2094\xe2\x80\x932100. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1509.06461" rel="nofollow">1509.06461</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AAAI+Conference+on+Artificial+Intelligence&amp;rft.atitle=Deep+reinforcement+learning+with+double+Q-learning&amp;rft.pages=2094-2100&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1509.06461&amp;rft.aulast=van+Hasselt&amp;rft.aufirst=Hado&amp;rft.au=Guez%2C+Arthur&amp;rft.au=Silver%2C+David&amp;rft_id=https%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI16%2Fpaper%2Fdownload%2F12389%2F11847&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFStrehlLiWiewioraLangford2006">Strehl, Alexander L.; Li, Lihong; Wiewiora, Eric; Langford, John; Littman, Michael L. (2006). <a class="external text" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/published-14.pdf" rel="nofollow">"Pac model-free reinforcement learning"</a> <span class="cs1-format">(PDF)</span>. <i>Proc. 22nd ICML</i>: 881\xe2\x80\x93888.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proc.+22nd+ICML&amp;rft.atitle=Pac+model-free+reinforcement+learning&amp;rft.pages=881-888&amp;rft.date=2006&amp;rft.aulast=Strehl&amp;rft.aufirst=Alexander+L.&amp;rft.au=Li%2C+Lihong&amp;rft.au=Wiewiora%2C+Eric&amp;rft.au=Langford%2C+John&amp;rft.au=Littman%2C+Michael+L.&amp;rft_id=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fwp-content%2Fuploads%2F2016%2F02%2Fpublished-14.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFMaeiSzepesv\xc3\xa1riBhatnagarSutton2010">Maei, Hamid; Szepesv\xc3\xa1ri, Csaba; Bhatnagar, Shalabh; Sutton, Richard (2010). <a class="external text" href="https://web.archive.org/web/20120908050052/http://webdocs.cs.ualberta.ca/~sutton/papers/MSBS-10.pdf" rel="nofollow">"Toward off-policy learning control with function approximation in Proceedings of the 27th International Conference on Machine Learning"</a> <span class="cs1-format">(PDF)</span>. pp.\xc2\xa0719\xe2\x80\x93726. Archived from <a class="external text" href="https://webdocs.cs.ualberta.ca/~sutton/papers/MSBS-10.pdf" rel="nofollow">the original</a> <span class="cs1-format">(PDF)</span> on 2012-09-08<span class="reference-accessdate">. Retrieved <span class="nowrap">2016-01-25</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Toward+off-policy+learning+control+with+function+approximation+in+Proceedings+of+the+27th+International+Conference+on+Machine+Learning&amp;rft.pages=719-726&amp;rft.date=2010&amp;rft.aulast=Maei&amp;rft.aufirst=Hamid&amp;rft.au=Szepesv%C3%A1ri%2C+Csaba&amp;rft.au=Bhatnagar%2C+Shalabh&amp;rft.au=Sutton%2C+Richard&amp;rft_id=https%3A%2F%2Fwebdocs.cs.ualberta.ca%2F~sutton%2Fpapers%2FMSBS-10.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFGaskettWettergreenZelinsky1999">Gaskett, Chris; Wettergreen, David; Zelinsky, Alexander (1999). <a class="external text" href="http://users.cecs.anu.edu.au/~rsl/rsl_papers/99ai.kambara.pdf" rel="nofollow">"Q-Learning in Continuous State and Action Spaces"</a> <span class="cs1-format">(PDF)</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Q-Learning+in+Continuous+State+and+Action+Spaces&amp;rft.date=1999&amp;rft.aulast=Gaskett&amp;rft.aufirst=Chris&amp;rft.au=Wettergreen%2C+David&amp;rft.au=Zelinsky%2C+Alexander&amp;rft_id=http%3A%2F%2Fusers.cecs.anu.edu.au%2F~rsl%2Frsl_papers%2F99ai.kambara.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AQ-learning"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n</ol></div>\n<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Q-learning&amp;action=edit&amp;section=18" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><a class="external text" href="http://www.cs.rhul.ac.uk/~chrisw/thesis.html" rel="nofollow">Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge University, Cambridge, England.</a></li>\n<li><a class="external text" href="http://portal.acm.org/citation.cfm?id=1143955" rel="nofollow">Strehl, Li, Wiewiora, Langford, Littman (2006). PAC model-free reinforcement learning</a></li>\n<li><a class="external text" href="https://web.archive.org/web/20050806080008/http://www.cs.ualberta.ca/~sutton/book/the-book.html" rel="nofollow"><i>Reinforcement Learning: An Introduction</i></a> by Richard Sutton and Andrew S. Barto, an online textbook. See <a class="external text" href="https://web.archive.org/web/20081202105235/http://www.cs.ualberta.ca/~sutton/book/ebook/node65.html" rel="nofollow">"6.5 Q-Learning: Off-Policy TD Control"</a>.</li>\n<li><a class="external text" href="http://sourceforge.net/projects/piqle/" rel="nofollow">Piqle: a Generic Java Platform for Reinforcement Learning</a></li>\n<li><a class="external text" href="http://ccl.northwestern.edu/netlogo/models/community/Reinforcement%20Learning%20Maze" rel="nofollow">Reinforcement Learning Maze</a>, a demonstration of guiding an ant through a maze using <i>Q</i>-learning.</li>\n<li><a class="external text" href="http://www.research.ibm.com/infoecon/paps/html/ijcai99_qnn/node4.html" rel="nofollow"><i>Q</i>-learning work by Gerald Tesauro</a></li></ul>\n<div aria-labelledby="Differentiable_computing" class="navbox" role="navigation" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th class="navbox-title" colspan="3" scope="col"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Differentiable_computing" title="Template:Differentiable computing"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Differentiable_computing" title="Template talk:Differentiable computing"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Differentiable_computing&amp;action=edit"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="Edit this template">e</abbr></a></li></ul></div><div id="Differentiable_computing" style="font-size:114%;margin:0 4em">Differentiable computing</div></th></tr><tr><th class="navbox-group" scope="row" style="width:1%">General</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Differentiable_programming" title="Differentiable programming">Differentiable programming</a></li>\n<li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>\n<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>\n<li><a href="/wiki/Automatic_differentiation" title="Automatic differentiation">Automatic differentiation</a></li>\n<li><a href="/wiki/Neuromorphic_engineering" title="Neuromorphic engineering">Neuromorphic engineering</a></li></ul>\n</div></td><td class="noviewer navbox-image" rowspan="8" style="width:1px;padding:0px 0px 0px 2px"><div><div class="floatright"><a class="image" href="/wiki/File:DNC_training_recall_task.gif"><img alt="DNC training recall task.gif" data-file-height="459" data-file-width="919" decoding="async" height="100" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/200px-DNC_training_recall_task.gif" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/300px-DNC_training_recall_task.gif 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/400px-DNC_training_recall_task.gif 2x" width="200"/></a></div></div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Concepts</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a></li>\n<li><a href="/wiki/Cable_theory" title="Cable theory">Cable theory</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression analysis</a></li>\n<li><a href="/wiki/Pattern_recognition" title="Pattern recognition">Pattern recognition</a></li>\n<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversarial machine learning</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Programming languages</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a></li>\n<li><a href="/wiki/Julia_(programming_language)" title="Julia (programming language)">Julia</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Application</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></li>\n<li><a href="/wiki/Computational_science" title="Computational science">Scientific computing</a></li>\n<li><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial Intelligence</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Hardware</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a class="mw-redirect" href="/wiki/Tensor_processing_unit" title="Tensor processing unit">TPU</a></li>\n<li><a href="/wiki/Vision_processing_unit" title="Vision processing unit">VPU</a></li>\n<li><a href="/wiki/Memristor" title="Memristor">Memristor</a></li>\n<li><a href="/wiki/SpiNNaker" title="SpiNNaker">SpiNNaker</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Software library</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a></li>\n<li><a href="/wiki/PyTorch" title="PyTorch">PyTorch</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Implementation</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th class="navbox-group" scope="row" style="width:1%">Audio-visual</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>\n<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>\n<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>\n<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>\n<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>\n<li><a href="/wiki/Speech_synthesis" title="Speech synthesis">Speech synthesis</a></li>\n<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>\n<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition system</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Verbal</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>\n<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li>\n<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>\n<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>\n<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>\n<li><a href="/wiki/Watson_(computer)" title="Watson (computer)">Watson</a></li>\n<li><a href="/wiki/GPT-3" title="GPT-3">GPT-3</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Decisional</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>\n<li><a class="mw-selflink selflink">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li></ul>\n</div></td></tr></tbody></table><div></div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">People</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>\n<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>\n<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>\n<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>\n<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>\n<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>\n<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>\n<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li></ul>\n</div></td></tr><tr><td class="navbox-abovebelow" colspan="3"><div>\n<ul><li><img alt="Portal" data-file-height="28" data-file-width="32" decoding="async" height="14" src="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" title="Portal" width="16"/> Portals\n<ul><li><a href="/wiki/Portal:Computer_programming" title="Portal:Computer programming">Computer programming</a></li>\n<li><a href="/wiki/Portal:Technology" title="Portal:Technology">Technology</a></li></ul></li>\n<li><img alt="Category" data-file-height="31" data-file-width="36" decoding="async" height="14" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" title="Category" width="16"/> Category\n<ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li>\n<li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></li></ul>\n</div></td></tr></tbody></table></div>\n<!-- \nNewPP limit report\nParsed by mw2222\nCached time: 20201020094507\nCache expiry: 2592000\nDynamic content: false\nComplications: [vary\xe2\x80\x90revision\xe2\x80\x90sha1]\nCPU time usage: 0.516 seconds\nReal time usage: 0.821 seconds\nPreprocessor visited node count: 2162/1000000\nPost\xe2\x80\x90expand include size: 94314/2097152 bytes\nTemplate argument size: 2017/2097152 bytes\nHighest expansion depth: 12/40\nExpensive parser function count: 2/500\nUnstrip recursion depth: 1/20\nUnstrip post\xe2\x80\x90expand size: 74086/5000000 bytes\nLua time usage: 0.219/10.000 seconds\nLua memory usage: 5.78 MB/50 MB\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  501.122      1 -total\n 54.30%  272.109      1 Template:Reflist\n 19.84%   99.398      1 Template:Cite_document\n 14.30%   71.658      1 Template:Machine_learning_bar\n 13.49%   67.617      1 Template:Sidebar_with_collapsible_lists\n 12.20%   61.122      1 Template:Citation_needed\n  9.59%   48.077      7 Template:Cite_book\n  9.46%   47.425      1 Template:Fix\n  8.45%   42.368      7 Template:Cite_journal\n  7.78%   38.993      4 Template:Category_handler\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:1281850-0!canonical!math=5 and timestamp 20201020094507 and revision id 984486286\n -->\n</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>\n<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Q-learning&amp;oldid=984486286">https://en.wikipedia.org/w/index.php?title=Q-learning&amp;oldid=984486286</a>"</div></div>\n<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></li><li><a href="/wiki/Category:Reinforcement_learning" title="Category:Reinforcement learning">Reinforcement learning</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_December_2017" title="Category:Articles with unsourced statements from December 2017">Articles with unsourced statements from December 2017</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_January_2020" title="Category:Wikipedia articles needing clarification from January 2020">Wikipedia articles needing clarification from January 2020</a></li></ul></div></div>\n</div>\n</div>\n<div id="mw-data-after-content">\n<div class="read-more-container"></div>\n</div>\n<div id="mw-navigation">\n<h2>Navigation menu</h2>\n<div id="mw-head">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-personal-label" class="mw-portlet mw-portlet-personal vector-menu" id="p-personal" role="navigation">\n<h3 id="p-personal-label">\n<span>Personal tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Q-learning" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Q-learning" title="You\'re encouraged to log in; however, it\'s not mandatory. [o]">Log in</a></li></ul>\n</div>\n</nav>\n<div id="left-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-namespaces-label" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" id="p-namespaces" role="navigation">\n<h3 id="p-namespaces-label">\n<span>Namespaces</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Q-learning" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Q-learning" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-variants-label" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu vector-menu-dropdown" id="p-variants" role="navigation">\n<input aria-labelledby="p-variants-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-variants-label">\n<span>Variants</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n</div>\n<div id="right-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-views-label" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" id="p-views" role="navigation">\n<h3 id="p-views-label">\n<span>Views</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/Q-learning">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=Q-learning&amp;action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=Q-learning&amp;action=history" title="Past revisions of this page [h]">View history</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-cactions-label" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu vector-menu-dropdown" id="p-cactions" role="navigation">\n<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-cactions-label">\n<span>More</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n<div id="p-search" role="search">\n<h3>\n<label for="searchInput">Search</label>\n</h3>\n<form action="/w/index.php" id="searchform">\n<div data-search-loc="header-navigation" id="simpleSearch">\n<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>\n<input name="title" type="hidden" value="Special:Search"/>\n<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">\n<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>\n</input></div>\n</form>\n</div>\n</div>\n</div>\n<div id="mw-panel">\n<div id="p-logo" role="banner">\n<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>\n</div>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-navigation-label" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">\n<h3 id="p-navigation-label">\n<span>Navigation</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Articles related to current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-interaction-label" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">\n<h3 id="p-interaction-label">\n<span>Contribute</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-introduction"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia">Learn to edit</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-tb-label" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" id="p-tb" role="navigation">\n<h3 id="p-tb-label">\n<span>Tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Q-learning" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Q-learning" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Q-learning&amp;oldid=984486286" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Q-learning&amp;action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Q-learning&amp;id=984486286&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q2664563" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-coll-print_export-label" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">\n<h3 id="p-coll-print_export-label">\n<span>Print/export</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Q-learning&amp;action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Q-learning&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-lang-label" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" id="p-lang" role="navigation">\n<h3 id="p-lang-label">\n<span>Languages</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-es"><a class="interlanguage-link-target" href="https://es.wikipedia.org/wiki/Q-learning" hreflang="es" lang="es" title="Q-learning \xe2\x80\x93 Spanish">Espa\xc3\xb1ol</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%DA%A9%DB%8C%D9%88-%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C" hreflang="fa" lang="fa" title="\xda\xa9\xdb\x8c\xd9\x88-\xdb\x8c\xd8\xa7\xd8\xaf\xda\xaf\xdb\x8c\xd8\xb1\xdb\x8c \xe2\x80\x93 Persian">\xd9\x81\xd8\xa7\xd8\xb1\xd8\xb3\xdb\x8c</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/Q-learning" hreflang="fr" lang="fr" title="Q-learning \xe2\x80\x93 French">Fran\xc3\xa7ais</a></li><li class="interlanguage-link interwiki-ko"><a class="interlanguage-link-target" href="https://ko.wikipedia.org/wiki/Q_%EB%9F%AC%EB%8B%9D" hreflang="ko" lang="ko" title="Q \xeb\x9f\xac\xeb\x8b\x9d \xe2\x80\x93 Korean">\xed\x95\x9c\xea\xb5\xad\xec\x96\xb4</a></li><li class="interlanguage-link interwiki-it"><a class="interlanguage-link-target" href="https://it.wikipedia.org/wiki/Q-learning" hreflang="it" lang="it" title="Q-learning \xe2\x80\x93 Italian">Italiano</a></li><li class="interlanguage-link interwiki-he"><a class="interlanguage-link-target" href="https://he.wikipedia.org/wiki/Q-learning" hreflang="he" lang="he" title="Q-learning \xe2\x80\x93 Hebrew">\xd7\xa2\xd7\x91\xd7\xa8\xd7\x99\xd7\xaa</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/Q%E5%AD%A6%E7%BF%92" hreflang="ja" lang="ja" title="Q\xe5\xad\xa6\xe7\xbf\x92 \xe2\x80\x93 Japanese">\xe6\x97\xa5\xe6\x9c\xac\xe8\xaa\x9e</a></li><li class="interlanguage-link interwiki-no"><a class="interlanguage-link-target" href="https://no.wikipedia.org/wiki/Q-l%C3%A6ring" hreflang="nb" lang="nb" title="Q-l\xc3\xa6ring \xe2\x80\x93 Norwegian Bokm\xc3\xa5l">Norsk bokm\xc3\xa5l</a></li><li class="interlanguage-link interwiki-ro"><a class="interlanguage-link-target" href="https://ro.wikipedia.org/wiki/Q-learning" hreflang="ro" lang="ro" title="Q-learning \xe2\x80\x93 Romanian">Rom\xc3\xa2n\xc4\x83</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/Q-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5" hreflang="ru" lang="ru" title="Q-\xd0\xbe\xd0\xb1\xd1\x83\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xe2\x80\x93 Russian">\xd0\xa0\xd1\x83\xd1\x81\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/Q-%D0%BD%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F" hreflang="uk" lang="uk" title="Q-\xd0\xbd\xd0\xb0\xd0\xb2\xd1\x87\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8f \xe2\x80\x93 Ukrainian">\xd0\xa3\xd0\xba\xd1\x80\xd0\xb0\xd1\x97\xd0\xbd\xd1\x81\xd1\x8c\xd0\xba\xd0\xb0</a></li><li class="interlanguage-link interwiki-vi"><a class="interlanguage-link-target" href="https://vi.wikipedia.org/wiki/Q-learning_(h%E1%BB%8Dc_t%C4%83ng_c%C6%B0%E1%BB%9Dng)" hreflang="vi" lang="vi" title="Q-learning (h\xe1\xbb\x8dc t\xc4\x83ng c\xc6\xb0\xe1\xbb\x9dng) \xe2\x80\x93 Vietnamese">Ti\xe1\xba\xbfng Vi\xe1\xbb\x87t</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/Q%E5%AD%A6%E4%B9%A0" hreflang="zh" lang="zh" title="Q\xe5\xad\xa6\xe4\xb9\xa0 \xe2\x80\x93 Chinese">\xe4\xb8\xad\xe6\x96\x87</a></li></ul>\n<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q2664563#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class="mw-footer" id="footer" role="contentinfo">\n<ul id="footer-info">\n<li id="footer-info-lastmod"> This page was last edited on 20 October 2020, at 09:44<span class="anonymous-show">\xc2\xa0(UTC)</span>.</li>\n<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia\xc2\xae is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id="footer-places">\n<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>\n<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>\n<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>\n<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>\n<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Q-learning&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>\n<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>\n<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>\n<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>\n</ul>\n<ul class="noprint" id="footer-icons">\n<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>\n<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>\n</ul>\n<div style="clear: both;"></div>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.516","walltime":"0.821","ppvisitednodes":{"value":2162,"limit":1000000},"postexpandincludesize":{"value":94314,"limit":2097152},"templateargumentsize":{"value":2017,"limit":2097152},"expansiondepth":{"value":12,"limit":40},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":74086,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  501.122      1 -total"," 54.30%  272.109      1 Template:Reflist"," 19.84%   99.398      1 Template:Cite_document"," 14.30%   71.658      1 Template:Machine_learning_bar"," 13.49%   67.617      1 Template:Sidebar_with_collapsible_lists"," 12.20%   61.122      1 Template:Citation_needed","  9.59%   48.077      7 Template:Cite_book","  9.46%   47.425      1 Template:Fix","  8.45%   42.368      7 Template:Cite_journal","  7.78%   38.993      4 Template:Category_handler"]},"scribunto":{"limitreport-timeusage":{"value":"0.219","limit":"10.000"},"limitreport-memusage":{"value":6060989,"limit":52428800}},"cachereport":{"origin":"mw2222","timestamp":"20201020094507","ttl":2592000,"transientcontent":false}}});});</script>\n<script type="application/ld+json">{"@context":"https:\\/\\/schema.org","@type":"Article","name":"Q-learning","url":"https:\\/\\/en.wikipedia.org\\/wiki\\/Q-learning","sameAs":"http:\\/\\/www.wikidata.org\\/entity\\/Q2664563","mainEntity":"http:\\/\\/www.wikidata.org\\/entity\\/Q2664563","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png"}},"datePublished":"2004-12-15T17:38:13Z","dateModified":"2020-10-20T09:44:59Z","headline":"algorithm"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":157,"wgHostname":"mw1273"});});</script>\n</body></html>'