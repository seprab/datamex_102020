b'<!DOCTYPE html>\n\n<html class="client-nojs" dir="ltr" lang="en">\n<head>\n<meta charset="utf8"/>\n<title>Transformer (machine learning model) - Wikipedia</title>\n<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"b9eaca78-5d13-4d3a-bace-0f4042d52d0c","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Transformer_(machine_learning_model)","wgTitle":"Transformer (machine learning model)","wgCurRevisionId":985192375,"wgRevisionId":985192375,"wgArticleId":61603971,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: multiple names: authors list","CS1 errors: missing periodical","Articles with short description","Short description is different from Wikidata","Artificial neural networks"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext",\n"wgRelevantPageName":"Transformer_(machine_learning_model)","wgRelevantArticleId":61603971,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q85810444"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready",\n"ext.math.styles":"ready","ext.pygments":"ready","skins.vector.styles.legacy":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\\\","watchToken":"+\\\\","csrfToken":"+\\\\"});\n});});</script>\n<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.pygments%2CwikimediaBadges%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cjquery.makeCollapsible.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>\n<meta content="" name="ResourceLoaderDynamicStyles"/>\n<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>\n<meta content="MediaWiki 1.36.0-wmf.14" name="generator"/>\n<meta content="origin" name="referrer"/>\n<meta content="origin-when-crossorigin" name="referrer"/>\n<meta content="origin-when-cross-origin" name="referrer"/>\n<link href="//en.m.wikipedia.org/wiki/Transformer_(machine_learning_model)" media="only screen and (max-width: 720px)" rel="alternate"/>\n<link href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>\n<link href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit" rel="edit" title="Edit this page"/>\n<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>\n<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>\n<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>\n<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>\n<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>\n<link href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="canonical"/>\n<link href="//login.wikimedia.org" rel="dns-prefetch"/>\n<link href="//meta.wikimedia.org" rel="dns-prefetch"/>\n</head>\n<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Transformer_machine_learning_model rootpage-Transformer_machine_learning_model skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>\n<div class="noprint" id="mw-head-base"></div>\n<div class="mw-body" id="content" role="main">\n<a id="top"></a>\n<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>\n<div class="mw-indicators mw-body-content">\n</div>\n<h1 class="firstHeading" id="firstHeading" lang="en">Transformer (machine learning model)</h1>\n<div class="mw-body-content" id="bodyContent">\n<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>\n<div id="contentSub"></div>\n<div id="contentSub2"></div>\n<div id="jump-to-nav"></div>\n<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>\n<a class="mw-jump-link" href="#searchInput">Jump to search</a>\n<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Machine learning algorithm used for natural language processing</div>\n<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em">Part of a series on</td></tr><tr><th style="padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br/>and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>\n<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>\n<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>\n<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>\n<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>\n<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>\n<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>\n<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>\n<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>\n<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>\n<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>\n<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>\xc2\xa0\xe2\x80\xa2 <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>\n<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>\n<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>\n<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>\n<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>\n<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>\n<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>\n<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>\n<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>\n<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>\n<li><a class="mw-redirect" href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>\n<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>\n<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>\n<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>\n<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation\xe2\x80\x93maximization algorithm">Expectation\xe2\x80\x93maximization (EM)</a></li>\n<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>\n<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>\n<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>\n<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>\n<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>\n<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>\n<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>\n<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>\n<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>\n<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>\n<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>\n<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>\n<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>\n<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>\n<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>\n<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>\n<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>\n<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>\n<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>\n<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li></ul></li>\n<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>\n<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>\n<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>\n<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>\n<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>\n<li><a class="mw-selflink selflink">Transformer</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias\xe2\x80\x93variance dilemma">Bias\xe2\x80\x93variance dilemma</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>\n<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>\n<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>\n<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>\n<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>\n<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik\xe2\x80\x93Chervonenkis theory">VC theory</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>\n<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>\n<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>\n<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>\n<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>\n<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p>The <b>Transformer</b> is a <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a> model introduced in 2017, used primarily in the field of <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> (NLP).<sup class="reference" id="cite_ref-:0_1-0"><a href="#cite_note-:0-1">[1]</a></sup>\n</p><p>Like <a class="mw-redirect" href="/wiki/Recurrent_neural_networks" title="Recurrent neural networks">recurrent neural networks</a> (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as <a href="/wiki/Statistical_machine_translation" title="Statistical machine translation">translation</a> and <a href="/wiki/Automatic_summarization" title="Automatic summarization">text summarization</a>. However, unlike RNNs, Transformers do not require that the sequential data be processed in order. For example, if the input data is a natural language sentence, the Transformer does not need to process the beginning of it before the end. Due to this feature, the Transformer allows for much more <a href="/wiki/Parallel_computing" title="Parallel computing">parallelization</a> than RNNs and therefore reduced training times.<sup class="reference" id="cite_ref-:0_1-1"><a href="#cite_note-:0-1">[1]</a></sup>\n</p><p>Since their introduction, Transformers have become the model of choice for tackling many problems in NLP, replacing older recurrent neural network models such as the <a href="/wiki/Long_short-term_memory" title="Long short-term memory">long short-term memory</a> (LSTM). Since the Transformer model facilitates more parallelization during training, it has enabled training on larger datasets than was possible before it was introduced. This has led to the development of <a href="/wiki/Transfer_learning" title="Transfer learning">pretrained systems</a> such as <a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a> (Bidirectional Encoder Representations from Transformers) and <a href="/wiki/OpenAI#Generative_models" title="OpenAI">GPT</a> (Generative Pre-trained Transformer), which have been trained with huge general language datasets, and can be fine-tuned to specific language tasks.<sup class="reference" id="cite_ref-:6_2-0"><a href="#cite_note-:6-2">[2]</a></sup><sup class="reference" id="cite_ref-:7_3-0"><a href="#cite_note-:7-3">[3]</a></sup>\n</p>\n<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>\n<ul>\n<li class="toclevel-1 tocsection-1"><a href="#Background"><span class="tocnumber">1</span> <span class="toctext">Background</span></a></li>\n<li class="toclevel-1 tocsection-2"><a href="#Architecture"><span class="tocnumber">2</span> <span class="toctext">Architecture</span></a>\n<ul>\n<li class="toclevel-2 tocsection-3"><a href="#Scaled_dot-product_attention"><span class="tocnumber">2.1</span> <span class="toctext">Scaled dot-product attention</span></a>\n<ul>\n<li class="toclevel-3 tocsection-4"><a href="#Multi-head_attention"><span class="tocnumber">2.1.1</span> <span class="toctext">Multi-head attention</span></a></li>\n</ul>\n</li>\n<li class="toclevel-2 tocsection-5"><a href="#Encoder"><span class="tocnumber">2.2</span> <span class="toctext">Encoder</span></a></li>\n<li class="toclevel-2 tocsection-6"><a href="#Decoder"><span class="tocnumber">2.3</span> <span class="toctext">Decoder</span></a></li>\n<li class="toclevel-2 tocsection-7"><a href="#Alternatives"><span class="tocnumber">2.4</span> <span class="toctext">Alternatives</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-8"><a href="#Training"><span class="tocnumber">3</span> <span class="toctext">Training</span></a></li>\n<li class="toclevel-1 tocsection-9"><a href="#Implementations"><span class="tocnumber">4</span> <span class="toctext">Implementations</span></a></li>\n<li class="toclevel-1 tocsection-10"><a href="#Applications"><span class="tocnumber">5</span> <span class="toctext">Applications</span></a></li>\n<li class="toclevel-1 tocsection-11"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>\n</ul>\n</div>\n<h2><span class="mw-headline" id="Background">Background</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=1" title="Edit section: Background">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>Before the introduction of Transformers, most state-of-the-art NLP systems relied on gated <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural networks</a> (RNNs), such as <a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTMs</a> and <a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">gated recurrent units</a> (GRUs), with added attention mechanisms. The Transformer built on these attention technologies without using an RNN structure, highlighting the fact that the attention mechanisms alone, without recurrent sequential processing, are powerful enough to achieve the performance of RNNs with attention.\n</p><p>Gated RNNs process tokens sequentially, maintaining a state vector that contains a representation of the data seen after every token. To process the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle n^{th}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="false" scriptlevel="0">\n<msup>\n<mi>n</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>t</mi>\n<mi>h</mi>\n</mrow>\n</msup>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\textstyle n^{th}}</annotation>\n</semantics>\n</math></span><img alt="{\\textstyle n^{th}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fec3ca41498a8f33eec940500dd168d7990e9cee" style="vertical-align: -0.338ex; width:3.167ex; height:2.509ex;"/></span> token, the model combines the state representing the sentence up to token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle n-1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="false" scriptlevel="0">\n<mi>n</mi>\n<mo>\xe2\x88\x92<!-- \xe2\x88\x92 --></mo>\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\textstyle n-1}</annotation>\n</semantics>\n</math></span><img alt="{\\textstyle n-1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/960c88fa1831b7505d9672de66058532fa5d4053" style="vertical-align: -0.505ex; width:5.398ex; height:2.343ex;"/></span> with the information of the new token to create a new state, representing the sentence up to token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="false" scriptlevel="0">\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\textstyle n}</annotation>\n</semantics>\n</math></span><img alt="{\\textstyle n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc6e1f880981346a604257ebcacdef24c0aca2d6" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span>. Theoretically, the information from one token can propagate arbitrarily far down the sequence, if at every point the state continues to encode information about the token. But in practice this mechanism is imperfect: due in part to the <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">vanishing gradient problem</a>, the model\'s state at the end of a long sentence often does not contain precise, extractable information about early tokens.\n</p><p>This problem was addressed by the introduction of attention mechanisms. Attention mechanisms let a model directly look at, and draw from, the state at any earlier point in the sentence. The attention layer can access all previous states and weighs them according to some learned measure of relevancy to the current token, providing sharper information about far-away relevant tokens. A clear example of the utility of attention is in translation. In an English-to-French translation system, the first word of the French output most probably depends heavily on the beginning of the English input. However, in a classic encoder-decoder LSTM model, in order to produce the first word of the French output the model is only given the state vector of the <i>last</i> English word. Theoretically, this vector can encode information about the whole English sentence, giving the model all necessary knowledge, but in practice this information is often not well preserved. If an attention mechanism is introduced, the model can instead learn to attend to the states of early English tokens when producing the beginning of the French output, giving it a much better concept of what it is translating.\n</p><p>When added to RNNs, attention mechanisms led to large gains in performance. The introduction of the Transformer brought to light the fact that attention mechanisms were powerful in themselves, and that sequential recurrent processing of data was not necessary for achieving the performance gains of RNNs with attention. The Transformer uses an attention mechanism without being an RNN, processing all tokens at the same time and calculating attention weights between them. The fact that Transformers do not rely on sequential processing, and lend themselves very easily to parallelization, allows Transformers to be trained more efficiently on larger datasets.\n</p>\n<h2><span class="mw-headline" id="Architecture">Architecture</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=2" title="Edit section: Architecture">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>Like the models invented before it, the Transformer is an encoder-decoder architecture. The encoder consists of a set of encoding layers that processes the input iteratively one layer after another and the decoder consists of a set of decoding layers that does the same thing to the output of the encoder.\n</p><p>The function of each encoder layer is to process its input to generate encodings, containing information about which parts of the inputs are relevant to each other. It passes its set of encodings to the next encoder layer as inputs. Each decoder layer does the opposite, taking all the encodings and processes them, using their incorporated contextual information to generate an output sequence.<sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup> To achieve this, each encoder and decoder layer makes use of an attention mechanism, which for each input, weighs the relevance of every other input and draws information from them accordingly to produce the output.<sup class="reference" id="cite_ref-:1_5-0"><a href="#cite_note-:1-5">[5]</a></sup> Each decoder layer also has an additional attention mechanism which draws information from the outputs of previous decoders, before the decoder layer draws information from the encodings. Both the encoder and decoder layers have a <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feed-forward neural network</a> for additional processing of the outputs, and contain residual connections and layer normalization steps.<sup class="reference" id="cite_ref-:1_5-1"><a href="#cite_note-:1-5">[5]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Scaled_dot-product_attention">Scaled dot-product attention</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=3" title="Edit section: Scaled dot-product attention">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>The basic building blocks of the Transformer are scaled dot-product attention units. When a sentence is passed into a Transformer model, attention weights are calculated between every token simultaneously. The attention unit produces embeddings for every token in context that contain information not only about the token itself, but also a weighted combination of other relevant tokens weighted by the attention weights.\n</p><p>Concretely, for each attention unit the Transformer model learns three weight matrices; the query weights <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle W_{Q}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>Q</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle W_{Q}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle W_{Q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/27233f4471dd458034969c094d1ac13bff1e38d1" style="vertical-align: -1.005ex; width:3.726ex; height:2.843ex;"/></span>, the key weights <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle W_{K}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>K</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle W_{K}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle W_{K}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1476608b04371ce36cdf6625d8b9ba8a96c615b9" style="vertical-align: -0.671ex; width:3.887ex; height:2.509ex;"/></span>, and the value weights <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle W_{V}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>V</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle W_{V}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle W_{V}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2ea6ed8ea3f0c61c33d4efb054e07f58b7046597" style="vertical-align: -0.671ex; width:3.69ex; height:2.509ex;"/></span>. For each token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>, the input <a href="/wiki/Word_embedding" title="Word embedding">word embedding</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle x_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle x_{i}}</annotation>\n</semantics>\n</math></span><img alt="x_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;"/></span> is multiplied with each of the three weight matrices to produce a query vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle q_{i}=x_{i}W_{Q}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>Q</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle q_{i}=x_{i}W_{Q}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle q_{i}=x_{i}W_{Q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ebba814bf54791662c42b73973f13b95a062a835" style="vertical-align: -1.005ex; width:10.79ex; height:2.843ex;"/></span>, a key vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k_{i}=x_{i}W_{K}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>K</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k_{i}=x_{i}W_{K}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle k_{i}=x_{i}W_{K}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c55d138a4e6a0203e8bac5d4f87d7143dec961ce" style="vertical-align: -0.671ex; width:11.126ex; height:2.509ex;"/></span>, and a value vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{i}=x_{i}W_{V}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>=</mo>\n<msub>\n<mi>x</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>V</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{i}=x_{i}W_{V}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle v_{i}=x_{i}W_{V}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/237f8999bf057b953208623a1647ade7ed908c52" style="vertical-align: -0.671ex; width:10.845ex; height:2.509ex;"/></span>. Attention weights are calculated using the query and key vectors: the attention weight <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle a_{ij}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle a_{ij}}</annotation>\n</semantics>\n</math></span><img alt="a_{ij}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ebea6cd2813c330c798921a2894b358f7b643917" style="vertical-align: -1.005ex; width:2.707ex; height:2.343ex;"/></span> from token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> to token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>j</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle j}</annotation>\n</semantics>\n</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span> is the dot product between <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle q_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle q_{i}}</annotation>\n</semantics>\n</math></span><img alt="q_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2752dcbff884354069fe332b8e51eb0a70a531b6" style="vertical-align: -0.671ex; width:1.837ex; height:2.009ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k_{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k_{j}}</annotation>\n</semantics>\n</math></span><img alt="k_j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/05ddf2c6d7759ac955e001a7cfafb2abfca41b0b" style="vertical-align: -1.005ex; width:2.121ex; height:2.843ex;"/></span>. The attention weights are divided by the square root of the dimension of the key vectors, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\sqrt {d_{k}}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<msqrt>\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>k</mi>\n</mrow>\n</msub>\n</msqrt>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\sqrt {d_{k}}}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle {\\sqrt {d_{k}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0be678d1b945828faecd56b29927f5a60011be37" style="vertical-align: -1.005ex; width:4.621ex; height:3.343ex;"/></span>, which stabilizes gradients during training, and passed through a <a href="/wiki/Softmax_function" title="Softmax function">softmax</a> which normalizes the weights to sum to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle 1}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mn>1</mn>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle 1}</annotation>\n</semantics>\n</math></span><img alt="1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/92d98b82a3778f043108d4e20960a9193df57cbf" style="vertical-align: -0.338ex; width:1.162ex; height:2.176ex;"/></span>. The fact that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle W_{Q}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>Q</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle W_{Q}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle W_{Q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/27233f4471dd458034969c094d1ac13bff1e38d1" style="vertical-align: -1.005ex; width:3.726ex; height:2.843ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle W_{K}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>K</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle W_{K}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle W_{K}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1476608b04371ce36cdf6625d8b9ba8a96c615b9" style="vertical-align: -0.671ex; width:3.887ex; height:2.509ex;"/></span> are different matrices allows attention to be non-symmetric: if token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> attends to token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>j</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle j}</annotation>\n</semantics>\n</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span> (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle q_{i}\\cdot k_{j}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<msub>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle q_{i}\\cdot k_{j}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle q_{i}\\cdot k_{j}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7de4219a59ace005d92f8d0a13466dbdb5fd6d9c" style="vertical-align: -1.005ex; width:5.637ex; height:2.843ex;"/></span> is large), this does not necessarily mean that token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>j</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle j}</annotation>\n</semantics>\n</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span> will attend to token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle q_{j}\\cdot k_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>j</mi>\n</mrow>\n</msub>\n<mo>\xe2\x8b\x85<!-- \xe2\x8b\x85 --></mo>\n<msub>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle q_{j}\\cdot k_{i}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle q_{j}\\cdot k_{i}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d40445a57203e20510d7b629e0567957524700e4" style="vertical-align: -1.005ex; width:5.637ex; height:2.843ex;"/></span> is large).  The output of the attention unit for token <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> is the weighted sum of the value vectors of all tokens, weighted by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle a_{ij}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>a</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n<mi>j</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle a_{ij}}</annotation>\n</semantics>\n</math></span><img alt="a_{ij}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ebea6cd2813c330c798921a2894b358f7b643917" style="vertical-align: -1.005ex; width:2.707ex; height:2.343ex;"/></span>, the attention from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span> to each token.\n</p><p>The attention calculation for all tokens can be expressed as one large matrix calculation, which is useful for training due to computational matrix operation optimizations which make matrix operations fast to compute. The matrices <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle Q}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>Q</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle Q}</annotation>\n</semantics>\n</math></span><img alt="Q" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;"/></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle K}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>K</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle K}</annotation>\n</semantics>\n</math></span><img alt="K" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle V}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>V</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle V}</annotation>\n</semantics>\n</math></span><img alt="V" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845" style="vertical-align: -0.338ex; width:1.787ex; height:2.176ex;"/></span> are defined as the matrices where the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>i</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle i}</annotation>\n</semantics>\n</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>th rows are vectors <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle q_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>q</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle q_{i}}</annotation>\n</semantics>\n</math></span><img alt="q_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2752dcbff884354069fe332b8e51eb0a70a531b6" style="vertical-align: -0.671ex; width:1.837ex; height:2.009ex;"/></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle k_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>k</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle k_{i}}</annotation>\n</semantics>\n</math></span><img alt="k_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f29138ed3ad54ffce527daccadc49c520459b0b0" style="vertical-align: -0.671ex; width:2.011ex; height:2.509ex;"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle v_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<msub>\n<mi>v</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>i</mi>\n</mrow>\n</msub>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle v_{i}}</annotation>\n</semantics>\n</math></span><img alt="v_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7dffe5726650f6daac54829972a94f38eb8ec127" style="vertical-align: -0.671ex; width:1.927ex; height:2.009ex;"/></span> respectively.\n</p><p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow class="MJX-TeXAtom-ORD">\n<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">\n<mtr>\n<mtd>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>Attention</mtext>\n</mrow>\n<mo stretchy="false">(</mo>\n<mi>Q</mi>\n<mo>,</mo>\n<mi>K</mi>\n<mo>,</mo>\n<mi>V</mi>\n<mo stretchy="false">)</mo>\n<mo>=</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mtext>softmax</mtext>\n</mrow>\n<mrow>\n<mo>(</mo>\n<mrow class="MJX-TeXAtom-ORD">\n<mfrac>\n<mrow>\n<mi>Q</mi>\n<msup>\n<mi>K</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mrow class="MJX-TeXAtom-ORD">\n<mi mathvariant="normal">T</mi>\n</mrow>\n</mrow>\n</msup>\n</mrow>\n<msqrt>\n<msub>\n<mi>d</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>k</mi>\n</mrow>\n</msub>\n</msqrt>\n</mfrac>\n</mrow>\n<mo>)</mo>\n</mrow>\n<mi>V</mi>\n</mtd>\n</mtr>\n</mtable>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b2afc7240eb97375a384b1628c18438e3068e3f" style="vertical-align: -3.171ex; width:43.753ex; height:7.509ex;"/></span>\n</p>\n<h4><span class="mw-headline" id="Multi-head_attention">Multi-head attention</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=4" title="Edit section: Multi-head attention">edit</a><span class="mw-editsection-bracket">]</span></span></h4>\n<p>One set of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\left(W_{Q},W_{K},W_{V}\\right)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mrow>\n<mo>(</mo>\n<mrow>\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>Q</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>K</mi>\n</mrow>\n</msub>\n<mo>,</mo>\n<msub>\n<mi>W</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mi>V</mi>\n</mrow>\n</msub>\n</mrow>\n<mo>)</mo>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle \\left(W_{Q},W_{K},W_{V}\\right)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle \\left(W_{Q},W_{K},W_{V}\\right)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/021c670a99024a281521fcfdc56d59571a70e4fd" style="vertical-align: -1.005ex; width:15.18ex; height:3.009ex;"/></span> matrices is called an <i>attention head</i>, and each layer in a Transformer model has multiple attention heads. While one attention head attends to the tokens that are relevant to each token, with multiple attention heads the model can learn to do this for different definitions of "relevance". Research has shown that many attention heads in Transformers encode relevance relations that are interpretable by humans. For example there are attention heads that, for every token, attend mostly to the next word, or attention heads that mainly attend from verbs to their direct objects.<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup> Since Transformer models have multiple attention heads, they have the possibility of capturing many levels and types of relevance relations, from surface-level to semantic. The multiple outputs for the multi-head attention layer are concatenated to pass into the feed-forward neural network layers.\n</p>\n<h3><span class="mw-headline" id="Encoder">Encoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=5" title="Edit section: Encoder">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Each encoder consists of two major components: a self-attention mechanism and a feed-forward neural network. The self-attention mechanism takes in a set of input encodings from the previous encoder and weighs their relevance to each other to generate a set of output encodings. The feed-forward neural network then further processes each output encoding individually. These output encodings are finally passed to the next encoder as its input, as well as the decoders.\n</p><p>The first encoder takes positional information and <a href="/wiki/Word_embedding" title="Word embedding">embeddings</a> of the input sequence as its input, rather than encodings. The positional information is necessary for the Transformer to make use of the order of the sequence, because no other part of the Transformer makes use of this.<sup class="reference" id="cite_ref-:0_1-2"><a href="#cite_note-:0-1">[1]</a></sup>\n</p>\n<h3><span class="mw-headline" id="Decoder">Decoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=6" title="Edit section: Decoder">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Each decoder consists of three major components: a self-attention mechanism, an attention mechanism over the encodings, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders.<sup class="reference" id="cite_ref-:0_1-3"><a href="#cite_note-:0-1">[1]</a></sup><sup class="reference" id="cite_ref-:1_5-2"><a href="#cite_note-:1-5">[5]</a></sup>\n</p><p>Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. Since the transformer should not use the current or future output to predict an output though, the output sequence must be partially masked to prevent this reverse information flow.<sup class="reference" id="cite_ref-:0_1-4"><a href="#cite_note-:0-1">[1]</a></sup> The last decoder is followed by a final <a href="/wiki/Matrix_multiplication" title="Matrix multiplication">linear transformation</a> and <a href="/wiki/Softmax_function" title="Softmax function">softmax layer</a>, to produce the output probabilities over the vocabulary.\n</p>\n<h3><span class="mw-headline" id="Alternatives">Alternatives</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=7" title="Edit section: Alternatives">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Training Transformer-based architectures can be very expensive, especially for long sentences.<sup class="reference" id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup> Alternative architectures include the Reformer, which reduces the computational load from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(L^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<msup>\n<mi>L</mi>\n<mrow class="MJX-TeXAtom-ORD">\n<mn>2</mn>\n</mrow>\n</msup>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(L^{2})}</annotation>\n</semantics>\n</math></span><img alt="O(L^{2})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e3ce944f7a9ec442a793b7f3120b444634e44e33" style="vertical-align: -0.838ex; width:6.22ex; height:3.176ex;"/></span> to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(L\\ln L)}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>O</mi>\n<mo stretchy="false">(</mo>\n<mi>L</mi>\n<mi>ln</mi>\n<mo>\xe2\x81\xa1<!-- \xe2\x81\xa1 --></mo>\n<mi>L</mi>\n<mo stretchy="false">)</mo>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle O(L\\ln L)}</annotation>\n</semantics>\n</math></span><img alt="{\\displaystyle O(L\\ln L)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/eded5e1d1d15576a801a82bbdb86f84c49467777" style="vertical-align: -0.838ex; width:9.462ex; height:2.843ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle L}" xmlns="http://www.w3.org/1998/Math/MathML">\n<semantics>\n<mrow class="MJX-TeXAtom-ORD">\n<mstyle displaystyle="true" scriptlevel="0">\n<mi>L</mi>\n</mstyle>\n</mrow>\n<annotation encoding="application/x-tex">{\\displaystyle L}</annotation>\n</semantics>\n</math></span><img alt="L" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/103168b86f781fe6e9a4a87b8ea1cebe0ad4ede8" style="vertical-align: -0.338ex; width:1.583ex; height:2.176ex;"/></span> is the length of the sequence. This is done using <a href="/wiki/Locality-sensitive_hashing" title="Locality-sensitive hashing">locality-sensitive hashing</a> and reversible layers.<sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup><sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup>\n</p>\n<h2><span class="mw-headline" id="Training">Training</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=8" title="Edit section: Training">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>Transformers typically undergo <a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">semi-supervised learning</a> involving <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> pretraining followed by <a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a> fine-tuning. Pretraining is typically done on a much larger dataset than fine-tuning, due to the restricted availability of labeled training data. Tasks for pretraining and fine-tuning commonly include:\n</p>\n<ul><li>next-sentence prediction<sup class="reference" id="cite_ref-:6_2-1"><a href="#cite_note-:6-2">[2]</a></sup></li>\n<li><a href="/wiki/Question_answering" title="Question answering">question answering</a><sup class="reference" id="cite_ref-:7_3-1"><a href="#cite_note-:7-3">[3]</a></sup></li>\n<li><a href="/wiki/Natural-language_understanding" title="Natural-language understanding">reading comprehension</a></li>\n<li><a href="/wiki/Sentiment_analysis" title="Sentiment analysis">sentiment analysis</a><sup class="reference" id="cite_ref-:8_10-0"><a href="#cite_note-:8-10">[10]</a></sup></li>\n<li><a class="mw-redirect" href="/wiki/Text_Summaries" title="Text Summaries">paraphrasing</a><sup class="reference" id="cite_ref-:8_10-1"><a href="#cite_note-:8-10">[10]</a></sup></li></ul>\n<h2><span class="mw-headline" id="Implementations">Implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=9" title="Edit section: Implementations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The Transformer model has been implemented in major deep learning frameworks such as <a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a> and <a href="/wiki/PyTorch" title="PyTorch">PyTorch</a>. Below is pseudo code for an implementation of the Transformer variant known as the "vanilla" transformer:\n</p>\n<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="k">def</span> <span class="nf">vanilla_transformer</span><span class="p">(</span><span class="n">enc_inp</span><span class="p">,</span> <span class="n">dec_inp</span><span class="p">):</span>\n    <span class="sd">"""Transformer variant known as the "vanilla" transformer."""</span>\n    <span class="n">x</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">enc_inp</span><span class="p">)</span> <span class="o">*</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">d_m</span><span class="p">)</span>\n    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">pos_encoding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>\n    <span class="n">x</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>\n    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_enc_layers</span><span class="p">):</span>\n        <span class="n">attn</span> <span class="o">=</span> <span class="n">multi_head_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>\n        <span class="n">attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>\n        <span class="n">attn</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn</span><span class="p">)</span>\n\n        <span class="n">x</span> <span class="o">=</span> <span class="n">point_wise_ff</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>\n        <span class="n">x</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn</span><span class="p">)</span>\n\n    <span class="c1"># x is at this point the output of the encoder</span>\n    <span class="n">enc_out</span> <span class="o">=</span> <span class="n">x</span>\n\n    <span class="n">x</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">dec_inp</span><span class="p">)</span> <span class="o">*</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">d_m</span><span class="p">)</span>\n    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">pos_encoding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>\n    <span class="n">x</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>\n    <span class="n">mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>\n    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_dec_layers</span><span class="p">):</span>\n        <span class="n">attn1</span> <span class="o">=</span> <span class="n">multi_head_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>\n        <span class="n">attn1</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span><span class="n">attn1</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>\n\n        <span class="n">attn2</span> <span class="o">=</span> <span class="n">multi_head_attention</span><span class="p">(</span><span class="n">attn1</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>\n        <span class="n">attn2</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">attn2</span><span class="p">)</span>\n        <span class="n">attn2</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span><span class="n">attn1</span> <span class="o">+</span> <span class="n">attn2</span><span class="p">)</span>\n\n        <span class="n">x</span> <span class="o">=</span> <span class="n">point_wise_ff</span><span class="p">(</span><span class="n">attn2</span><span class="p">)</span>\n        <span class="n">x</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span><span class="n">attn2</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>\n    <span class="k">return</span> <span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>\n</pre></div>\n<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=10" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The Transformer finds most of its applications in the field of <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> (NLP), for example the tasks of <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a> and <a href="/wiki/Time_series" title="Time series">time series prediction</a>.<sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>  Many pretrained models such as <a href="/wiki/GPT-3" title="GPT-3">GPT-3</a>, GPT-2, BERT, XLNet, and RoBERTa demonstrate the ability of Transformers to perform a wide variety of such NLP-related tasks, and have the potential to find real-world applications.<sup class="reference" id="cite_ref-:6_2-2"><a href="#cite_note-:6-2">[2]</a></sup><sup class="reference" id="cite_ref-:7_3-2"><a href="#cite_note-:7-3">[3]</a></sup><sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> These may include:\n</p>\n<ul><li><a href="/wiki/Machine_translation" title="Machine translation">machine translation</a></li>\n<li><a href="/wiki/Automatic_summarization" title="Automatic summarization">document summarization</a></li>\n<li><a href="/wiki/Natural-language_generation" title="Natural-language generation">document generation</a></li>\n<li><a href="/wiki/Named-entity_recognition" title="Named-entity recognition">named entity recognition</a> (NER)<sup class="reference" id="cite_ref-:9_13-0"><a href="#cite_note-:9-13">[13]</a></sup></li>\n<li><a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a><sup class="reference" id="cite_ref-:9_13-1"><a href="#cite_note-:9-13">[13]</a></sup></li>\n<li><a href="/wiki/Sequence_analysis" title="Sequence analysis">biological sequence analysis</a><sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup></li></ul>\n<p>In 2020, it was shown that the transformer architecture, more specifically GPT-2, could be fine-tuned to play chess.<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup>\n</p>\n<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit&amp;section=11" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="reflist" style="list-style-type: decimal;">\n<div class="mw-references-wrap mw-references-columns"><ol class="references">\n<li id="cite_note-:0-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:0_1-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:0_1-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFPolosukhinKaiserGomezJones2017">Polosukhin, Illia; Kaiser, Lukasz; Gomez, Aidan N.; Jones, Llion; Uszkoreit, Jakob; Parmar, Niki; Shazeer, Noam; Vaswani, Ashish (2017-06-12). "Attention Is All You Need". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1706.03762" rel="nofollow">1706.03762</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Attention+Is+All+You+Need&amp;rft.date=2017-06-12&amp;rft_id=info%3Aarxiv%2F1706.03762&amp;rft.aulast=Polosukhin&amp;rft.aufirst=Illia&amp;rft.au=Kaiser%2C+Lukasz&amp;rft.au=Gomez%2C+Aidan+N.&amp;rft.au=Jones%2C+Llion&amp;rft.au=Uszkoreit%2C+Jakob&amp;rft.au=Parmar%2C+Niki&amp;rft.au=Shazeer%2C+Noam&amp;rft.au=Vaswani%2C+Ashish&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><style data-mw-deduplicate="TemplateStyles:r982806391">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>\n</li>\n<li id="cite_note-:6-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-:6_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:6_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:6_2-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="nofollow">"Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing"</a>. <i>Google AI Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-08-25</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Google+AI+Blog&amp;rft.atitle=Open+Sourcing+BERT%3A+State-of-the-Art+Pre-training+for+Natural+Language+Processing&amp;rft_id=http%3A%2F%2Fai.googleblog.com%2F2018%2F11%2Fopen-sourcing-bert-state-of-art-pre.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-:7-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-:7_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:7_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:7_3-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="https://openai.com/blog/better-language-models/" rel="nofollow">"Better Language Models and Their Implications"</a>. <i>OpenAI</i>. 2019-02-14<span class="reference-accessdate">. Retrieved <span class="nowrap">2019-08-25</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Better+Language+Models+and+Their+Implications&amp;rft.date=2019-02-14&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fbetter-language-models%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="https://indico.io/blog/sequence-modeling-neural-networks-part2-attention-models/" rel="nofollow">"Sequence Modeling with Neural Networks (Part 2): Attention Models"</a>. <i>Indico</i>. 2016-04-18<span class="reference-accessdate">. Retrieved <span class="nowrap">2019-10-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Indico&amp;rft.atitle=Sequence+Modeling+with+Neural+Networks+%28Part+2%29%3A+Attention+Models&amp;rft.date=2016-04-18&amp;rft_id=https%3A%2F%2Findico.io%2Fblog%2Fsequence-modeling-neural-networks-part2-attention-models%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-:1-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:1_5-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFAlammar">Alammar, Jay. <a class="external text" href="http://jalammar.github.io/illustrated-transformer/" rel="nofollow">"The Illustrated Transformer"</a>. <i>jalammar.github.io</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-10-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=jalammar.github.io&amp;rft.atitle=The+Illustrated+Transformer&amp;rft.aulast=Alammar&amp;rft.aufirst=Jay&amp;rft_id=http%3A%2F%2Fjalammar.github.io%2Fillustrated-transformer%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFClarkKhandelwalLevyManning2019">Clark, Kevin; Khandelwal, Urvashi; Levy, Omer; Manning, Christopher D. (August 2019). <a class="external text" href="https://www.aclweb.org/anthology/W19-4828" rel="nofollow">"What Does BERT Look at? An Analysis of BERT\'s Attention"</a>. <i>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</i>. Florence, Italy: Association for Computational Linguistics: 276\xe2\x80\x93286. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.18653%2Fv1%2FW19-4828" rel="nofollow">10.18653/v1/W19-4828</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+2019+ACL+Workshop+BlackboxNLP%3A+Analyzing+and+Interpreting+Neural+Networks+for+NLP&amp;rft.atitle=What+Does+BERT+Look+at%3F+An+Analysis+of+BERT%27s+Attention&amp;rft.pages=276-286&amp;rft.date=2019-08&amp;rft_id=info%3Adoi%2F10.18653%2Fv1%2FW19-4828&amp;rft.aulast=Clark&amp;rft.aufirst=Kevin&amp;rft.au=Khandelwal%2C+Urvashi&amp;rft.au=Levy%2C+Omer&amp;rft.au=Manning%2C+Christopher+D.&amp;rft_id=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FW19-4828&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1"><a class="external text" href="https://arxiv.org/pdf/2001.04451.pdf" rel="nofollow">"Reformer: The Efficient Transformer"</a> <span class="cs1-format">(PDF)</span>. <i>ICLR 2020</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICLR+2020&amp;rft.atitle=Reformer%3A+The+Efficient+Transformer&amp;rft_id=https%3A%2F%2Farxiv.org%2Fpdf%2F2001.04451.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="https://www.coursera.org/lecture/attention-models-in-nlp/tasks-with-long-sequences-suzNH" rel="nofollow">"Task with long sequences"</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Task+with+long+sequences&amp;rft_id=https%3A%2F%2Fwww.coursera.org%2Flecture%2Fattention-models-in-nlp%2Ftasks-with-long-sequences-suzNH&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="http://ai.googleblog.com/2020/01/reformer-efficient-transformer.html" rel="nofollow">"Reformer: The Efficient Transformer"</a>. <i>Google AI Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2020-10-22</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Google+AI+Blog&amp;rft.atitle=Reformer%3A+The+Efficient+Transformer&amp;rft_id=http%3A%2F%2Fai.googleblog.com%2F2020%2F01%2Freformer-efficient-transformer.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-:8-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-:8_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:8_10-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFWangSinghMichaelHill2018">Wang, Alex; Singh, Amanpreet; Michael, Julian; Hill, Felix; Levy, Omer; Bowman, Samuel (2018). "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding". <i>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</i>. Stroudsburg, PA, USA: Association for Computational Linguistics: 353\xe2\x80\x93355. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1804.07461" rel="nofollow">1804.07461</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2018arXiv180407461W" rel="nofollow">2018arXiv180407461W</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.18653%2Fv1%2Fw18-5446" rel="nofollow">10.18653/v1/w18-5446</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>\xc2\xa0<a class="external text" href="https://api.semanticscholar.org/CorpusID:5034059" rel="nofollow">5034059</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+2018+EMNLP+Workshop+BlackboxNLP%3A+Analyzing+and+Interpreting+Neural+Networks+for+NLP&amp;rft.atitle=GLUE%3A+A+Multi-Task+Benchmark+and+Analysis+Platform+for+Natural+Language+Understanding&amp;rft.pages=353-355&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1804.07461&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A5034059&amp;rft_id=info%3Adoi%2F10.18653%2Fv1%2Fw18-5446&amp;rft_id=info%3Abibcode%2F2018arXiv180407461W&amp;rft.aulast=Wang&amp;rft.aufirst=Alex&amp;rft.au=Singh%2C+Amanpreet&amp;rft.au=Michael%2C+Julian&amp;rft.au=Hill%2C+Felix&amp;rft.au=Levy%2C+Omer&amp;rft.au=Bowman%2C+Samuel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFAllard2019">Allard, Maxime (2019-07-01). <a class="external text" href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04" rel="nofollow">"What is a Transformer?"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-10-21</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=What+is+a+Transformer%3F&amp;rft.date=2019-07-01&amp;rft.aulast=Allard&amp;rft.aufirst=Maxime&amp;rft_id=https%3A%2F%2Fmedium.com%2Finside-machine-learning%2Fwhat-is-a-transformer-d07dd1fbec04&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFYang,_Zhilin_Dai,_Zihang_Yang,_Yiming_Carbonell,_Jaime_Salakhutdinov,_Ruslan_Le,_Quoc_V.2019">Yang, Zhilin Dai, Zihang Yang, Yiming Carbonell, Jaime Salakhutdinov, Ruslan Le, Quoc V. (2019-06-19). <i>XLNet: Generalized Autoregressive Pretraining for Language Understanding</i>. <a class="mw-redirect" href="/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>\xc2\xa0<a class="external text" href="//www.worldcat.org/oclc/1106350082" rel="nofollow">1106350082</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=XLNet%3A+Generalized+Autoregressive+Pretraining+for+Language+Understanding&amp;rft.date=2019-06-19&amp;rft_id=info%3Aoclcnum%2F1106350082&amp;rft.au=Yang%2C+Zhilin+Dai%2C+Zihang+Yang%2C+Yiming+Carbonell%2C+Jaime+Salakhutdinov%2C+Ruslan+Le%2C+Quoc+V.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><span class="cs1-maint citation-comment">CS1 maint: multiple names: authors list (<a href="/wiki/Category:CS1_maint:_multiple_names:_authors_list" title="Category:CS1 maint: multiple names: authors list">link</a>)</span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-:9-13"><span class="mw-cite-backlink">^ <a href="#cite_ref-:9_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:9_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFMonsters2017">Monsters, Data (2017-09-26). <a class="external text" href="https://medium.com/@datamonsters/artificial-neural-networks-in-natural-language-processing-bcf62aa9151a" rel="nofollow">"10 Applications of Artificial Neural Networks in Natural Language Processing"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-10-21</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=10+Applications+of+Artificial+Neural+Networks+in+Natural+Language+Processing&amp;rft.date=2017-09-26&amp;rft.aulast=Monsters&amp;rft.aufirst=Data&amp;rft_id=https%3A%2F%2Fmedium.com%2F%40datamonsters%2Fartificial-neural-networks-in-natural-language-processing-bcf62aa9151a&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFRivesGoyalMeierGuo2019">Rives, Alexander; Goyal, Siddharth; Meier, Joshua; Guo, Demi; Ott, Myle; Zitnick, C. Lawrence; Ma, Jerry; Fergus, Rob (2019). <a class="external text" href="https://doi.org/10.1101%2F622803" rel="nofollow">"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"</a>. <a class="mw-redirect" href="/wiki/BioRxiv_(identifier)" title="BioRxiv (identifier)">bioRxiv</a>\xc2\xa0<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1101%2F622803" rel="nofollow">10.1101/622803</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1101%2F622803" rel="nofollow">10.1101/622803</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Biological+structure+and+function+emerge+from+scaling+unsupervised+learning+to+250+million+protein+sequences&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.1101%2F622803&amp;rft_id=%2F%2Fdoi.org%2F10.1101%2F622803&amp;rft.aulast=Rives&amp;rft.aufirst=Alexander&amp;rft.au=Goyal%2C+Siddharth&amp;rft.au=Meier%2C+Joshua&amp;rft.au=Guo%2C+Demi&amp;rft.au=Ott%2C+Myle&amp;rft.au=Zitnick%2C+C.+Lawrence&amp;rft.au=Ma%2C+Jerry&amp;rft.au=Fergus%2C+Rob&amp;rft_id=%2F%2Fdoi.org%2F10.1101%252F622803&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span> <span class="cs1-hidden-error error citation-comment">Cite journal requires <code class="cs1-code">|journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFNoeverCiolinoKalin2020">Noever, David; Ciolino, Matt; Kalin, Josh (2020-08-21). "The Chess Transformer: Mastering Play using Generative Language Models". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/2008.04057" rel="nofollow">2008.04057</a></span> [<a class="external text" href="//arxiv.org/archive/cs.AI" rel="nofollow">cs.AI</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=The+Chess+Transformer%3A+Mastering+Play+using+Generative+Language+Models&amp;rft.date=2020-08-21&amp;rft_id=info%3Aarxiv%2F2008.04057&amp;rft.aulast=Noever&amp;rft.aufirst=David&amp;rft.au=Ciolino%2C+Matt&amp;rft.au=Kalin%2C+Josh&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATransformer+%28machine+learning+model%29"></span><link href="mw-data:TemplateStyles:r982806391" rel="mw-deduplicated-inline-style"/></span>\n</li>\n</ol></div></div>\n<p><br/>\n</p>\n<div aria-labelledby="Differentiable_computing" class="navbox" role="navigation" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th class="navbox-title" colspan="3" scope="col"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Differentiable_computing" title="Template:Differentiable computing"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Differentiable_computing" title="Template talk:Differentiable computing"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Differentiable_computing&amp;action=edit"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="Edit this template">e</abbr></a></li></ul></div><div id="Differentiable_computing" style="font-size:114%;margin:0 4em">Differentiable computing</div></th></tr><tr><th class="navbox-group" scope="row" style="width:1%">General</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Differentiable_programming" title="Differentiable programming">Differentiable programming</a></li>\n<li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>\n<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>\n<li><a href="/wiki/Automatic_differentiation" title="Automatic differentiation">Automatic differentiation</a></li>\n<li><a href="/wiki/Neuromorphic_engineering" title="Neuromorphic engineering">Neuromorphic engineering</a></li></ul>\n</div></td><td class="noviewer navbox-image" rowspan="8" style="width:1px;padding:0px 0px 0px 2px"><div><div class="floatright"><a class="image" href="/wiki/File:DNC_training_recall_task.gif"><img alt="DNC training recall task.gif" data-file-height="459" data-file-width="919" decoding="async" height="100" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/200px-DNC_training_recall_task.gif" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/300px-DNC_training_recall_task.gif 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/400px-DNC_training_recall_task.gif 2x" width="200"/></a></div></div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Concepts</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a></li>\n<li><a href="/wiki/Cable_theory" title="Cable theory">Cable theory</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression analysis</a></li>\n<li><a href="/wiki/Pattern_recognition" title="Pattern recognition">Pattern recognition</a></li>\n<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversarial machine learning</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Programming languages</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a></li>\n<li><a href="/wiki/Julia_(programming_language)" title="Julia (programming language)">Julia</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Application</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></li>\n<li><a href="/wiki/Computational_science" title="Computational science">Scientific computing</a></li>\n<li><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial Intelligence</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Hardware</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a class="mw-redirect" href="/wiki/Tensor_processing_unit" title="Tensor processing unit">TPU</a></li>\n<li><a href="/wiki/Vision_processing_unit" title="Vision processing unit">VPU</a></li>\n<li><a href="/wiki/Memristor" title="Memristor">Memristor</a></li>\n<li><a href="/wiki/SpiNNaker" title="SpiNNaker">SpiNNaker</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Software library</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a></li>\n<li><a href="/wiki/PyTorch" title="PyTorch">PyTorch</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Implementation</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th class="navbox-group" scope="row" style="width:1%">Audio-visual</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>\n<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>\n<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>\n<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>\n<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>\n<li><a href="/wiki/Speech_synthesis" title="Speech synthesis">Speech synthesis</a></li>\n<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>\n<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition system</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Verbal</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>\n<li><a class="mw-selflink selflink">Transformer</a></li>\n<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>\n<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>\n<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>\n<li><a href="/wiki/Watson_(computer)" title="Watson (computer)">Watson</a></li>\n<li><a href="/wiki/GPT-3" title="GPT-3">GPT-3</a></li></ul>\n</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Decisional</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>\n<li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li></ul>\n</div></td></tr></tbody></table><div></div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">People</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">\n<ul><li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>\n<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>\n<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>\n<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>\n<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>\n<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>\n<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>\n<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li></ul>\n</div></td></tr><tr><td class="navbox-abovebelow" colspan="3"><div>\n<ul><li><img alt="Portal" data-file-height="28" data-file-width="32" decoding="async" height="14" src="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" title="Portal" width="16"/> Portals\n<ul><li><a href="/wiki/Portal:Computer_programming" title="Portal:Computer programming">Computer programming</a></li>\n<li><a href="/wiki/Portal:Technology" title="Portal:Technology">Technology</a></li></ul></li>\n<li><img alt="Category" data-file-height="31" data-file-width="36" decoding="async" height="14" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" title="Category" width="16"/> Category\n<ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li>\n<li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></li></ul>\n</div></td></tr></tbody></table></div>\n<!-- \nNewPP limit report\nParsed by mw1323\nCached time: 20201029152340\nCache expiry: 2592000\nDynamic content: false\nComplications: [vary\xe2\x80\x90revision\xe2\x80\x90sha1]\nCPU time usage: 0.468 seconds\nReal time usage: 1.085 seconds\nPreprocessor visited node count: 1463/1000000\nPost\xe2\x80\x90expand include size: 77932/2097152 bytes\nTemplate argument size: 1305/2097152 bytes\nHighest expansion depth: 11/40\nExpensive parser function count: 0/500\nUnstrip recursion depth: 1/20\nUnstrip post\xe2\x80\x90expand size: 58692/5000000 bytes\nLua time usage: 0.205/10.000 seconds\nLua memory usage: 4.69 MB/50 MB\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  904.385      1 -total\n 26.44%  239.141      1 Template:Reflist\n 14.57%  131.742      2 Template:Cite_arxiv\n  9.00%   81.371      1 Template:Short_description\n  5.34%   48.268      8 Template:Cite_web\n  5.00%   45.209      1 Template:Machine_learning_bar\n  4.54%   41.087      1 Template:Pagetype\n  4.43%   40.069      1 Template:Sidebar_with_collapsible_lists\n  3.12%   28.213      1 Template:Differentiable_computing\n  2.89%   26.103      2 Template:Navbox\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:61603971-0!canonical!math=5 and timestamp 20201029152339 and revision id 985192375\n -->\n</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>\n<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&amp;oldid=985192375">https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&amp;oldid=985192375</a>"</div></div>\n<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_multiple_names:_authors_list" title="Category:CS1 maint: multiple names: authors list">CS1 maint: multiple names: authors list</a></li><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_is_different_from_Wikidata" title="Category:Short description is different from Wikidata">Short description is different from Wikidata</a></li></ul></div></div>\n</div>\n</div>\n<div id="mw-data-after-content">\n<div class="read-more-container"></div>\n</div>\n<div id="mw-navigation">\n<h2>Navigation menu</h2>\n<div id="mw-head">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-personal-label" class="mw-portlet mw-portlet-personal vector-menu" id="p-personal" role="navigation">\n<h3 id="p-personal-label">\n<span>Personal tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Transformer+%28machine+learning+model%29" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Transformer+%28machine+learning+model%29" title="You\'re encouraged to log in; however, it\'s not mandatory. [o]">Log in</a></li></ul>\n</div>\n</nav>\n<div id="left-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-namespaces-label" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" id="p-namespaces" role="navigation">\n<h3 id="p-namespaces-label">\n<span>Namespaces</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Transformer_(machine_learning_model)" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Transformer_(machine_learning_model)" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-variants-label" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu vector-menu-dropdown" id="p-variants" role="navigation">\n<input aria-labelledby="p-variants-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-variants-label">\n<span>Variants</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n</div>\n<div id="right-navigation">\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-views-label" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" id="p-views" role="navigation">\n<h3 id="p-views-label">\n<span>Views</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/Transformer_(machine_learning_model)">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=history" title="Past revisions of this page [h]">View history</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-cactions-label" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu vector-menu-dropdown" id="p-cactions" role="navigation">\n<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox" type="checkbox"/>\n<h3 id="p-cactions-label">\n<span>More</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"></ul>\n</div>\n</nav>\n<div id="p-search" role="search">\n<h3>\n<label for="searchInput">Search</label>\n</h3>\n<form action="/w/index.php" id="searchform">\n<div data-search-loc="header-navigation" id="simpleSearch">\n<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>\n<input name="title" type="hidden" value="Special:Search"/>\n<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">\n<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>\n</input></div>\n</form>\n</div>\n</div>\n</div>\n<div id="mw-panel">\n<div id="p-logo" role="banner">\n<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>\n</div>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-navigation-label" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">\n<h3 id="p-navigation-label">\n<span>Navigation</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Articles related to current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-interaction-label" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">\n<h3 id="p-interaction-label">\n<span>Contribute</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-introduction"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia">Learn to edit</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-tb-label" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" id="p-tb" role="navigation">\n<h3 id="p-tb-label">\n<span>Tools</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Transformer_(machine_learning_model)" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Transformer_(machine_learning_model)" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;oldid=985192375" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Transformer_(machine_learning_model)&amp;action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Transformer_%28machine_learning_model%29&amp;id=985192375&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q85810444" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-coll-print_export-label" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">\n<h3 id="p-coll-print_export-label">\n<span>Print/export</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Transformer_%28machine_learning_model%29&amp;action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Transformer_(machine_learning_model)&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>\n</div>\n</nav>\n<!-- Please do not use role attribute as CSS selector, it is deprecated. -->\n<nav aria-labelledby="p-lang-label" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" id="p-lang" role="navigation">\n<h3 id="p-lang-label">\n<span>Languages</span>\n</h3>\n<div class="vector-menu-content">\n<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/Transformer_(Maschinelles_Lernen)" hreflang="de" lang="de" title="Transformer (Maschinelles Lernen) \xe2\x80\x93 German">Deutsch</a></li><li class="interlanguage-link interwiki-eu"><a class="interlanguage-link-target" href="https://eu.wikipedia.org/wiki/Transformer_(ikasketa_automatikoko_eredua)" hreflang="eu" lang="eu" title="Transformer (ikasketa automatikoko eredua) \xe2\x80\x93 Basque">Euskara</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80_(%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)" hreflang="ru" lang="ru" title="\xd0\xa2\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd1\x84\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb5\xd1\x80 (\xd0\xbc\xd0\xbe\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8c \xd0\xbc\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbe\xd0\xb1\xd1\x83\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f) \xe2\x80\x93 Russian">\xd0\xa0\xd1\x83\xd1\x81\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80_(%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BD%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F)" hreflang="uk" lang="uk" title="\xd0\xa2\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd1\x84\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb5\xd1\x80 (\xd0\xbc\xd0\xbe\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8c \xd0\xbc\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbd\xd0\xb0\xd0\xb2\xd1\x87\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8f) \xe2\x80\x93 Ukrainian">\xd0\xa3\xd0\xba\xd1\x80\xd0\xb0\xd1\x97\xd0\xbd\xd1\x81\xd1\x8c\xd0\xba\xd0\xb0</a></li></ul>\n<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q85810444#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class="mw-footer" id="footer" role="contentinfo">\n<ul id="footer-info">\n<li id="footer-info-lastmod"> This page was last edited on 24 October 2020, at 14:37<span class="anonymous-show">\xc2\xa0(UTC)</span>.</li>\n<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia\xc2\xae is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id="footer-places">\n<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>\n<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>\n<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>\n<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>\n<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>\n<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>\n<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>\n<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>\n</ul>\n<ul class="noprint" id="footer-icons">\n<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>\n<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>\n</ul>\n<div style="clear: both;"></div>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.468","walltime":"1.085","ppvisitednodes":{"value":1463,"limit":1000000},"postexpandincludesize":{"value":77932,"limit":2097152},"templateargumentsize":{"value":1305,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":0,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":58692,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  904.385      1 -total"," 26.44%  239.141      1 Template:Reflist"," 14.57%  131.742      2 Template:Cite_arxiv","  9.00%   81.371      1 Template:Short_description","  5.34%   48.268      8 Template:Cite_web","  5.00%   45.209      1 Template:Machine_learning_bar","  4.54%   41.087      1 Template:Pagetype","  4.43%   40.069      1 Template:Sidebar_with_collapsible_lists","  3.12%   28.213      1 Template:Differentiable_computing","  2.89%   26.103      2 Template:Navbox"]},"scribunto":{"limitreport-timeusage":{"value":"0.205","limit":"10.000"},"limitreport-memusage":{"value":4919687,"limit":52428800}},"cachereport":{"origin":"mw1323","timestamp":"20201029152340","ttl":2592000,"transientcontent":false}}});});</script>\n<script type="application/ld+json">{"@context":"https:\\/\\/schema.org","@type":"Article","name":"Transformer (machine learning model)","url":"https:\\/\\/en.wikipedia.org\\/wiki\\/Transformer_(machine_learning_model)","sameAs":"http:\\/\\/www.wikidata.org\\/entity\\/Q85810444","mainEntity":"http:\\/\\/www.wikidata.org\\/entity\\/Q85810444","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png"}},"datePublished":"2019-08-25T16:32:02Z","dateModified":"2020-10-24T14:37:09Z","headline":"A machine learning model from Google Brain"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":140,"wgHostname":"mw1324"});});</script>\n</body></html>'